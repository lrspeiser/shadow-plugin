{
  "module": "src/infrastructure/fileSystem",
  "moduleType": "other",
  "capabilities": [
    "Caches file contents in memory to accelerate repeated file access operations",
    "Automatically refreshes cached content when files are modified on disk",
    "Processes multiple files in parallel for faster bulk operations",
    "Intelligently filters files to skip non-source directories and irrelevant files",
    "Provides consistent error handling across all file operations",
    "Reduces disk I/O to improve overall extension responsiveness"
  ],
  "summary": "The fileSystem module provides high-performance file access and processing capabilities for the extension. It maintains an intelligent in-memory cache that speeds up repeated file reads while automatically staying synchronized with disk changes, ensuring users always work with current file content without manual refresh actions.\n\nThe module handles bulk file operations efficiently by processing multiple files simultaneously and automatically filtering out non-source directories like node_modules, .git, dist, and build folders. This means users don't need to manually exclude common directories - the extension intelligently focuses on relevant source files.\n\nAll file operations are wrapped with consistent error handling, so users experience graceful degradation rather than crashes when file access issues occur. The combination of caching, parallel processing, and smart filtering results in noticeably faster extension performance, particularly when working with large codebases or performing operations that touch many files at once.",
  "files": [
    {
      "file": "src/infrastructure/fileSystem/fileCache.ts",
      "role": "Core Logic",
      "purpose": "Caches file contents in memory to reduce redundant file system reads and improve performance across the extension.",
      "userVisibleActions": [
        "Faster file access when the same file is read multiple times",
        "Automatic refresh of cached content when files are modified on disk",
        "Improved extension responsiveness due to reduced disk I/O operations"
      ],
      "developerVisibleActions": [
        "Retrieve file content with automatic caching via getFile() method",
        "Cache automatically invalidates when files change on disk through file system watcher",
        "LRU (Least Recently Used) eviction policy maintains cache size limits",
        "Cache statistics available for monitoring hits, misses, and evictions",
        "Cache entries expire after a TTL (time-to-live) period to ensure freshness",
        "File hash verification ensures cached content matches current disk content"
      ],
      "keyFunctions": [
        {
          "name": "getFile",
          "desc": "Retrieves file content from cache if available and valid, otherwise reads from disk and caches it",
          "inputs": "filePath: string",
          "outputs": "Promise<string> - file content"
        },
        {
          "name": "constructor",
          "desc": "Initializes the cache with configurable size limit and TTL settings",
          "inputs": "maxSize: number (default 500), ttl: number (default 5000ms)",
          "outputs": "FileCache instance"
        },
        {
          "name": "setupWatcher",
          "desc": "Sets up file system watcher to automatically invalidate cache entries when files change",
          "inputs": "none",
          "outputs": "void"
        },
        {
          "name": "isStale",
          "desc": "Checks if a cached entry has exceeded its TTL",
          "inputs": "cached: CachedFile",
          "outputs": "boolean"
        },
        {
          "name": "evictIfNeeded",
          "desc": "Removes least recently used entries when cache size exceeds maximum",
          "inputs": "none",
          "outputs": "void"
        }
      ],
      "dependencies": [
        "vscode",
        "fs",
        "path"
      ],
      "intent": "This file exists to optimize file system operations by caching frequently accessed files in memory, reducing disk I/O and improving extension performance. It solves the problem of redundant file reads across multiple components by providing a centralized, automatically invalidating cache with size management and freshness guarantees.",
      "rawContent": "```json\n{\n  \"purpose\": \"Caches file contents in memory to reduce redundant file system reads and improve performance across the extension.\",\n  \"userVisibleActions\": [\n    \"Faster file access when the same file is read multiple times\",\n    \"Automatic refresh of cached content when files are modified on disk\",\n    \"Improved extension responsiveness due to reduced disk I/O operations\"\n  ],\n  \"developerVisibleActions\": [\n    \"Retrieve file content with automatic caching via getFile() method\",\n    \"Cache automatically invalidates when files change on disk through file system watcher\",\n    \"LRU (Least Recently Used) eviction policy maintains cache size limits\",\n    \"Cache statistics available for monitoring hits, misses, and evictions\",\n    \"Cache entries expire after a TTL (time-to-live) period to ensure freshness\",\n    \"File hash verification ensures cached content matches current disk content\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"getFile\",\n      \"desc\": \"Retrieves file content from cache if available and valid, otherwise reads from disk and caches it\",\n      \"inputs\": \"filePath: string\",\n      \"outputs\": \"Promise<string> - file content\"\n    },\n    {\n      \"name\": \"constructor\",\n      \"desc\": \"Initializes the cache with configurable size limit and TTL settings\",\n      \"inputs\": \"maxSize: number (default 500), ttl: number (default 5000ms)\",\n      \"outputs\": \"FileCache instance\"\n    },\n    {\n      \"name\": \"setupWatcher\",\n      \"desc\": \"Sets up file system watcher to automatically invalidate cache entries when files change\",\n      \"inputs\": \"none\",\n      \"outputs\": \"void\"\n    },\n    {\n      \"name\": \"isStale\",\n      \"desc\": \"Checks if a cached entry has exceeded its TTL\",\n      \"inputs\": \"cached: CachedFile\",\n      \"outputs\": \"boolean\"\n    },\n    {\n      \"name\": \"evictIfNeeded\",\n      \"desc\": \"Removes least recently used entries when cache size exceeds maximum\",\n      \"inputs\": \"none\",\n      \"outputs\": \"void\"\n    }\n  ],\n  \"dependencies\": [\n    \"vscode\",\n    \"fs\",\n    \"path\"\n  ],\n  \"intent\": \"This file exists to optimize file system operations by caching frequently accessed files in memory, reducing disk I/O and improving extension performance. It solves the problem of redundant file reads across multiple components by providing a centralized, automatically invalidating cache with size management and freshness guarantees.\"\n}\n```"
    },
    {
      "file": "src/infrastructure/fileSystem/fileProcessor.ts",
      "role": "Core Logic",
      "purpose": "Provides a unified file processing system that filters, reads, and processes multiple files in parallel while handling errors consistently",
      "userVisibleActions": [
        "Files are automatically filtered to skip non-source directories (node_modules, .git, dist, build, etc.)",
        "Multiple files are processed simultaneously for faster operations",
        "Errors during file processing are caught and handled gracefully"
      ],
      "developerVisibleActions": [
        "Developer provides an array of file paths and a processing function",
        "System automatically filters out unwanted files based on patterns",
        "Files are read and processed in parallel batches",
        "Custom file filters can be provided to override default skip patterns",
        "Custom file readers can be injected for testing or alternate file sources",
        "Error context can be passed for better error tracking and debugging"
      ],
      "keyFunctions": [
        {
          "name": "shouldProcess",
          "desc": "Determines if a file should be processed based on filter patterns",
          "inputs": "filePath (string)",
          "outputs": "boolean indicating whether to process the file"
        },
        {
          "name": "readFile",
          "desc": "Reads file content from the filesystem",
          "inputs": "filePath (string)",
          "outputs": "Promise<string> containing file content"
        },
        {
          "name": "processFiles",
          "desc": "Processes multiple files in parallel with filtering, reading, and custom processing logic",
          "inputs": "files (string[]), processor function, optional error context",
          "outputs": "Promise<T[]> containing processed results for each file"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "../../utils/errorHandler"
      ],
      "intent": "This file exists to eliminate duplicate file processing patterns scattered across the codebase by providing a single, reusable system for filtering files, reading them, and processing them in parallel. It solves the problem of inconsistent file handling, reduces code duplication, and provides a standardized way to process multiple files with built-in error handling and customizable filtering.",
      "rawContent": "```json\n{\n  \"purpose\": \"Provides a unified file processing system that filters, reads, and processes multiple files in parallel while handling errors consistently\",\n  \"userVisibleActions\": [\n    \"Files are automatically filtered to skip non-source directories (node_modules, .git, dist, build, etc.)\",\n    \"Multiple files are processed simultaneously for faster operations\",\n    \"Errors during file processing are caught and handled gracefully\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer provides an array of file paths and a processing function\",\n    \"System automatically filters out unwanted files based on patterns\",\n    \"Files are read and processed in parallel batches\",\n    \"Custom file filters can be provided to override default skip patterns\",\n    \"Custom file readers can be injected for testing or alternate file sources\",\n    \"Error context can be passed for better error tracking and debugging\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"shouldProcess\",\n      \"desc\": \"Determines if a file should be processed based on filter patterns\",\n      \"inputs\": \"filePath (string)\",\n      \"outputs\": \"boolean indicating whether to process the file\"\n    },\n    {\n      \"name\": \"readFile\",\n      \"desc\": \"Reads file content from the filesystem\",\n      \"inputs\": \"filePath (string)\",\n      \"outputs\": \"Promise<string> containing file content\"\n    },\n    {\n      \"name\": \"processFiles\",\n      \"desc\": \"Processes multiple files in parallel with filtering, reading, and custom processing logic\",\n      \"inputs\": \"files (string[]), processor function, optional error context\",\n      \"outputs\": \"Promise<T[]> containing processed results for each file\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"../../utils/errorHandler\"\n  ],\n  \"intent\": \"This file exists to eliminate duplicate file processing patterns scattered across the codebase by providing a single, reusable system for filtering files, reading them, and processing them in parallel. It solves the problem of inconsistent file handling, reduces code duplication, and provides a standardized way to process multiple files with built-in error handling and customizable filtering.\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-19T21:26:54.436Z"
  }
}