{
  "module": "src/ai",
  "moduleType": "other",
  "capabilities": [
    "Automatic rate limiting for multiple LLM providers (OpenAI, Claude) to prevent quota violations",
    "Intelligent retry handling with exponential backoff for transient API failures",
    "Structured parsing of LLM responses into standardized documentation formats",
    "Graceful degradation with fallback text extraction when AI responses are malformed",
    "Provider-specific rate limit management with automatic request throttling"
  ],
  "summary": "The AI module provides a robust infrastructure layer for interacting with Large Language Model (LLM) providers like OpenAI and Claude. It ensures reliable API communication through three key mechanisms: rate limiting, retry handling, and response parsing. Users benefit from automatic protection against API quota violations, with requests being intelligently throttled or delayed based on provider-specific limits.\n\nWhen temporary failures occur (network issues, timeouts, rate limits), the module automatically retries requests with increasing wait times between attempts, maximizing the chance of success without user intervention. All LLM responses are parsed into standardized, structured formats suitable for file summaries, module summaries, and product documentation, with clear sections for purpose, user actions, and dependencies.\n\nThe module operates transparently in the background, handling all complexity of API communication while delivering clean, organized documentation to users. Even when AI responses are incomplete or malformed, the system provides partial data through fallback extraction mechanisms, ensuring users always receive useful output from their documentation generation requests.",
  "files": [
    {
      "file": "src/ai/llmRateLimiter.ts",
      "role": "Core Logic",
      "purpose": "Manages API rate limiting for LLM providers (OpenAI and Claude) to prevent exceeding their request quotas",
      "userVisibleActions": [
        "API requests are automatically throttled to stay within provider limits",
        "Requests may be delayed or rejected if rate limits are exceeded",
        "Different LLM providers (OpenAI, Claude) have different rate limit allowances"
      ],
      "developerVisibleActions": [
        "Configure custom rate limits for each LLM provider (max requests per time window)",
        "Check if a request can be made before calling the LLM API",
        "Record requests to track usage against rate limits",
        "OpenAI defaults to 60 requests per minute, Claude defaults to 50 requests per minute",
        "Rate limits are enforced using a sliding time window approach"
      ],
      "keyFunctions": [
        {
          "name": "constructor",
          "desc": "Initializes rate limiter with default limits for OpenAI (60 req/min) and Claude (50 req/min)",
          "inputs": "none",
          "outputs": "RateLimiter instance"
        },
        {
          "name": "configure",
          "desc": "Sets custom rate limit configuration for a specific LLM provider",
          "inputs": "provider (openai|claude), config (maxRequests, windowMs)",
          "outputs": "void"
        },
        {
          "name": "canMakeRequest",
          "desc": "Checks if a request can be made without exceeding the rate limit for the provider",
          "inputs": "provider (openai|claude)",
          "outputs": "boolean (true if request allowed, false if rate limit exceeded)"
        },
        {
          "name": "recordRequest",
          "desc": "Records the timestamp of a request to track usage against rate limits",
          "inputs": "provider (openai|claude)",
          "outputs": "void"
        }
      ],
      "dependencies": [],
      "intent": "Prevents the application from exceeding LLM provider API rate limits by tracking request timestamps and enforcing configurable request quotas per time window, ensuring reliable API access and avoiding service interruptions or penalties",
      "rawContent": "```json\n{\n  \"purpose\": \"Manages API rate limiting for LLM providers (OpenAI and Claude) to prevent exceeding their request quotas\",\n  \"userVisibleActions\": [\n    \"API requests are automatically throttled to stay within provider limits\",\n    \"Requests may be delayed or rejected if rate limits are exceeded\",\n    \"Different LLM providers (OpenAI, Claude) have different rate limit allowances\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure custom rate limits for each LLM provider (max requests per time window)\",\n    \"Check if a request can be made before calling the LLM API\",\n    \"Record requests to track usage against rate limits\",\n    \"OpenAI defaults to 60 requests per minute, Claude defaults to 50 requests per minute\",\n    \"Rate limits are enforced using a sliding time window approach\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"constructor\",\n      \"desc\": \"Initializes rate limiter with default limits for OpenAI (60 req/min) and Claude (50 req/min)\",\n      \"inputs\": \"none\",\n      \"outputs\": \"RateLimiter instance\"\n    },\n    {\n      \"name\": \"configure\",\n      \"desc\": \"Sets custom rate limit configuration for a specific LLM provider\",\n      \"inputs\": \"provider (openai|claude), config (maxRequests, windowMs)\",\n      \"outputs\": \"void\"\n    },\n    {\n      \"name\": \"canMakeRequest\",\n      \"desc\": \"Checks if a request can be made without exceeding the rate limit for the provider\",\n      \"inputs\": \"provider (openai|claude)\",\n      \"outputs\": \"boolean (true if request allowed, false if rate limit exceeded)\"\n    },\n    {\n      \"name\": \"recordRequest\",\n      \"desc\": \"Records the timestamp of a request to track usage against rate limits\",\n      \"inputs\": \"provider (openai|claude)\",\n      \"outputs\": \"void\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Prevents the application from exceeding LLM provider API rate limits by tracking request timestamps and enforcing configurable request quotas per time window, ensuring reliable API access and avoiding service interruptions or penalties\"\n}\n```"
    },
    {
      "file": "src/ai/llmResponseParser.ts",
      "role": "Core Logic",
      "purpose": "Parses and extracts structured data from LLM text responses into standardized formats for file summaries, module summaries, and product documentation.",
      "userVisibleActions": [
        "When AI analysis completes, user receives structured documentation with clear sections for purpose, actions, and dependencies",
        "User sees organized file and module summaries with extracted key information",
        "User receives product documentation with categorized features and use cases",
        "When AI responses are malformed, user still gets partial data with fallback text extraction"
      ],
      "developerVisibleActions": [
        "Developer calls parser methods with raw LLM response text and receives typed objects (FileSummary, ModuleSummary, EnhancedProductDocumentation)",
        "Developer provides file paths and role information to get parsed file summaries",
        "Developer passes module paths to get consolidated module analysis",
        "Developer triggers product documentation parsing to extract features, use cases, and target users",
        "Parser attempts JSON parsing first, then falls back to text extraction if JSON parsing fails",
        "Developer receives LLMInsights objects with parsed purpose, capabilities, and context",
        "Parser handles extraction of list sections, nested structures, and specific data fields from unstructured text"
      ],
      "keyFunctions": [
        {
          "name": "parseFileSummary",
          "desc": "Extracts file-level documentation from LLM response",
          "inputs": "content (string), filePath (string), role (string)",
          "outputs": "FileSummary object with purpose, actions, functions, and dependencies"
        },
        {
          "name": "parseModuleSummary",
          "desc": "Extracts module-level documentation from LLM response",
          "inputs": "content (string), modulePath (string)",
          "outputs": "ModuleSummary object with organized module information"
        },
        {
          "name": "parseEnhancedProductDocumentation",
          "desc": "Extracts comprehensive product documentation from LLM response",
          "inputs": "content (string)",
          "outputs": "EnhancedProductDocumentation object with features, use cases, and target users"
        },
        {
          "name": "parseInsights",
          "desc": "Extracts general insights and analysis from LLM response",
          "inputs": "content (string)",
          "outputs": "LLMInsights object with purpose, capabilities, and context"
        },
        {
          "name": "extractSection",
          "desc": "Extracts a specific named section from text content",
          "inputs": "content (string), sectionName (string)",
          "outputs": "Extracted text string or empty string"
        },
        {
          "name": "extractListSection",
          "desc": "Extracts a list of items from a named section",
          "inputs": "content (string), sectionName (string)",
          "outputs": "Array of strings extracted from the section"
        }
      ],
      "dependencies": [
        "../fileDocumentation (FileSummary, ModuleSummary, EnhancedProductDocumentation)",
        "../llmService (LLMInsights, ProductPurposeAnalysis, AnalysisContext)"
      ],
      "intent": "Solves the problem of converting unstructured LLM text responses into consistent, typed data structures that can be programmatically used throughout the application. Acts as the bridge between raw AI output and structured documentation objects, handling both JSON and freeform text responses with fallback strategies.",
      "rawContent": "```json\n{\n  \"purpose\": \"Parses and extracts structured data from LLM text responses into standardized formats for file summaries, module summaries, and product documentation.\",\n  \"userVisibleActions\": [\n    \"When AI analysis completes, user receives structured documentation with clear sections for purpose, actions, and dependencies\",\n    \"User sees organized file and module summaries with extracted key information\",\n    \"User receives product documentation with categorized features and use cases\",\n    \"When AI responses are malformed, user still gets partial data with fallback text extraction\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer calls parser methods with raw LLM response text and receives typed objects (FileSummary, ModuleSummary, EnhancedProductDocumentation)\",\n    \"Developer provides file paths and role information to get parsed file summaries\",\n    \"Developer passes module paths to get consolidated module analysis\",\n    \"Developer triggers product documentation parsing to extract features, use cases, and target users\",\n    \"Parser attempts JSON parsing first, then falls back to text extraction if JSON parsing fails\",\n    \"Developer receives LLMInsights objects with parsed purpose, capabilities, and context\",\n    \"Parser handles extraction of list sections, nested structures, and specific data fields from unstructured text\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"parseFileSummary\",\n      \"desc\": \"Extracts file-level documentation from LLM response\",\n      \"inputs\": \"content (string), filePath (string), role (string)\",\n      \"outputs\": \"FileSummary object with purpose, actions, functions, and dependencies\"\n    },\n    {\n      \"name\": \"parseModuleSummary\",\n      \"desc\": \"Extracts module-level documentation from LLM response\",\n      \"inputs\": \"content (string), modulePath (string)\",\n      \"outputs\": \"ModuleSummary object with organized module information\"\n    },\n    {\n      \"name\": \"parseEnhancedProductDocumentation\",\n      \"desc\": \"Extracts comprehensive product documentation from LLM response\",\n      \"inputs\": \"content (string)\",\n      \"outputs\": \"EnhancedProductDocumentation object with features, use cases, and target users\"\n    },\n    {\n      \"name\": \"parseInsights\",\n      \"desc\": \"Extracts general insights and analysis from LLM response\",\n      \"inputs\": \"content (string)\",\n      \"outputs\": \"LLMInsights object with purpose, capabilities, and context\"\n    },\n    {\n      \"name\": \"extractSection\",\n      \"desc\": \"Extracts a specific named section from text content\",\n      \"inputs\": \"content (string), sectionName (string)\",\n      \"outputs\": \"Extracted text string or empty string\"\n    },\n    {\n      \"name\": \"extractListSection\",\n      \"desc\": \"Extracts a list of items from a named section\",\n      \"inputs\": \"content (string), sectionName (string)\",\n      \"outputs\": \"Array of strings extracted from the section\"\n    }\n  ],\n  \"dependencies\": [\n    \"../fileDocumentation (FileSummary, ModuleSummary, EnhancedProductDocumentation)\",\n    \"../llmService (LLMInsights, ProductPurposeAnalysis, AnalysisContext)\"\n  ],\n  \"intent\": \"Solves the problem of converting unstructured LLM text responses into consistent, typed data structures that can be programmatically used throughout the application. Acts as the bridge between raw AI output and structured documentation objects, handling both JSON and freeform text responses with fallback strategies.\"\n}\n```"
    },
    {
      "file": "src/ai/llmRetryHandler.ts",
      "role": "Core Logic",
      "purpose": "Handles automatic retry logic for LLM API requests with exponential backoff when errors occur",
      "userVisibleActions": [
        "API requests automatically retry when temporary failures occur (rate limits, timeouts, network issues)",
        "Longer wait times occur between successive retry attempts",
        "Failed requests eventually succeed or show error after maximum retries exhausted"
      ],
      "developerVisibleActions": [
        "Wrap API calls with retry handler to get automatic retry behavior",
        "Configure retry behavior (max attempts, delays, which errors trigger retries)",
        "Receive callbacks when retries occur to track attempt progress",
        "Get final result with number of attempts made",
        "Non-retryable errors immediately throw without retry attempts"
      ],
      "keyFunctions": [
        {
          "name": "executeWithRetry",
          "desc": "Executes an async operation with automatic retry logic and exponential backoff",
          "inputs": "operation function to execute, optional retry configuration (maxRetries, delays, error types)",
          "outputs": "Promise resolving to operation result with attempt count"
        },
        {
          "name": "isRetryableError",
          "desc": "Determines if an error should trigger a retry based on error type classification",
          "inputs": "error object, list of retryable error patterns",
          "outputs": "boolean indicating if error is retryable"
        }
      ],
      "dependencies": [],
      "intent": "Provides resilient LLM API communication by automatically handling transient failures like rate limits, timeouts, and network issues without requiring manual retry logic in calling code",
      "rawContent": "```json\n{\n  \"purpose\": \"Handles automatic retry logic for LLM API requests with exponential backoff when errors occur\",\n  \"userVisibleActions\": [\n    \"API requests automatically retry when temporary failures occur (rate limits, timeouts, network issues)\",\n    \"Longer wait times occur between successive retry attempts\",\n    \"Failed requests eventually succeed or show error after maximum retries exhausted\"\n  ],\n  \"developerVisibleActions\": [\n    \"Wrap API calls with retry handler to get automatic retry behavior\",\n    \"Configure retry behavior (max attempts, delays, which errors trigger retries)\",\n    \"Receive callbacks when retries occur to track attempt progress\",\n    \"Get final result with number of attempts made\",\n    \"Non-retryable errors immediately throw without retry attempts\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"executeWithRetry\",\n      \"desc\": \"Executes an async operation with automatic retry logic and exponential backoff\",\n      \"inputs\": \"operation function to execute, optional retry configuration (maxRetries, delays, error types)\",\n      \"outputs\": \"Promise resolving to operation result with attempt count\"\n    },\n    {\n      \"name\": \"isRetryableError\",\n      \"desc\": \"Determines if an error should trigger a retry based on error type classification\",\n      \"inputs\": \"error object, list of retryable error patterns\",\n      \"outputs\": \"boolean indicating if error is retryable\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Provides resilient LLM API communication by automatically handling transient failures like rate limits, timeouts, and network issues without requiring manual retry logic in calling code\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-19T21:24:44.017Z"
  }
}