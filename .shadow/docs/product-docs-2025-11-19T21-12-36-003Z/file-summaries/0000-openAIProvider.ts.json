{
  "file": "src/ai/providers/openAIProvider.ts",
  "role": "Core Logic",
  "purpose": "Implements OpenAI API integration to send chat requests and receive responses from GPT models",
  "userVisibleActions": [
    "Enables AI-powered chat interactions using OpenAI's GPT models",
    "Provides structured JSON responses from AI when requested",
    "Returns AI-generated text content based on user prompts",
    "Shows error when OpenAI API key is not configured"
  ],
  "developerVisibleActions": [
    "Configure OpenAI API key through configuration manager",
    "Send chat messages with optional system prompts to OpenAI",
    "Request structured JSON outputs from the AI model",
    "Check if OpenAI provider is properly configured before use",
    "Handle token usage and cost tracking from API responses",
    "Set custom models (defaults to gpt-4o)",
    "Automatically extract JSON from AI responses"
  ],
  "keyFunctions": [
    {
      "name": "initialize",
      "desc": "Sets up OpenAI client with API key from configuration",
      "inputs": "None (reads from config)",
      "outputs": "Initializes client or sets to null if no key"
    },
    {
      "name": "isConfigured",
      "desc": "Checks if the OpenAI provider is ready to use",
      "inputs": "None",
      "outputs": "Boolean indicating if API key is set"
    },
    {
      "name": "getName",
      "desc": "Returns the provider identifier",
      "inputs": "None",
      "outputs": "String 'openai'"
    },
    {
      "name": "sendRequest",
      "desc": "Sends a chat completion request to OpenAI and returns the response",
      "inputs": "LLMRequestOptions (model, messages, systemPrompt, responseFormat)",
      "outputs": "LLMResponse with content, finish reason, and token usage"
    },
    {
      "name": "requestStructuredOutput",
      "desc": "Requests AI response in JSON format and parses it",
      "inputs": "LLMRequestOptions with JSON schema",
      "outputs": "StructuredOutputResponse with parsed JSON object or null"
    }
  ],
  "dependencies": [
    "openai",
    "../../config/configurationManager",
    "../../utils/jsonExtractor",
    "./ILLMProvider"
  ],
  "intent": "This file exists to provide a concrete implementation of the LLM provider interface specifically for OpenAI's API, enabling the application to communicate with GPT models for AI-powered features. It solves the problem of abstracting OpenAI-specific API calls behind a common interface while handling configuration, error cases, and response parsing including structured JSON outputs.",
  "rawContent": "```json\n{\n  \"purpose\": \"Implements OpenAI API integration to send chat requests and receive responses from GPT models\",\n  \"userVisibleActions\": [\n    \"Enables AI-powered chat interactions using OpenAI's GPT models\",\n    \"Provides structured JSON responses from AI when requested\",\n    \"Returns AI-generated text content based on user prompts\",\n    \"Shows error when OpenAI API key is not configured\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure OpenAI API key through configuration manager\",\n    \"Send chat messages with optional system prompts to OpenAI\",\n    \"Request structured JSON outputs from the AI model\",\n    \"Check if OpenAI provider is properly configured before use\",\n    \"Handle token usage and cost tracking from API responses\",\n    \"Set custom models (defaults to gpt-4o)\",\n    \"Automatically extract JSON from AI responses\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"initialize\",\n      \"desc\": \"Sets up OpenAI client with API key from configuration\",\n      \"inputs\": \"None (reads from config)\",\n      \"outputs\": \"Initializes client or sets to null if no key\"\n    },\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Checks if the OpenAI provider is ready to use\",\n      \"inputs\": \"None\",\n      \"outputs\": \"Boolean indicating if API key is set\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the provider identifier\",\n      \"inputs\": \"None\",\n      \"outputs\": \"String 'openai'\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a chat completion request to OpenAI and returns the response\",\n      \"inputs\": \"LLMRequestOptions (model, messages, systemPrompt, responseFormat)\",\n      \"outputs\": \"LLMResponse with content, finish reason, and token usage\"\n    },\n    {\n      \"name\": \"requestStructuredOutput\",\n      \"desc\": \"Requests AI response in JSON format and parses it\",\n      \"inputs\": \"LLMRequestOptions with JSON schema\",\n      \"outputs\": \"StructuredOutputResponse with parsed JSON object or null\"\n    }\n  ],\n  \"dependencies\": [\n    \"openai\",\n    \"../../config/configurationManager\",\n    \"../../utils/jsonExtractor\",\n    \"./ILLMProvider\"\n  ],\n  \"intent\": \"This file exists to provide a concrete implementation of the LLM provider interface specifically for OpenAI's API, enabling the application to communicate with GPT models for AI-powered features. It solves the problem of abstracting OpenAI-specific API calls behind a common interface while handling configuration, error cases, and response parsing including structured JSON outputs.\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-19T21:14:00.079Z"
  }
}