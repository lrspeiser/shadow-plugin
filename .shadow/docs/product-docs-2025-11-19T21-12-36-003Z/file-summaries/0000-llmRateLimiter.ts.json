{
  "file": "src/ai/llmRateLimiter.ts",
  "role": "Core Logic",
  "purpose": "Manages API rate limiting for LLM providers (OpenAI and Claude) to prevent exceeding their request quotas",
  "userVisibleActions": [
    "API requests are automatically throttled to stay within provider limits",
    "Requests may be delayed or rejected if rate limits are exceeded",
    "Different LLM providers (OpenAI, Claude) have different rate limit allowances"
  ],
  "developerVisibleActions": [
    "Configure custom rate limits for each LLM provider (max requests per time window)",
    "Check if a request can be made before calling the LLM API",
    "Record requests to track usage against rate limits",
    "OpenAI defaults to 60 requests per minute, Claude defaults to 50 requests per minute",
    "Rate limits are enforced using a sliding time window approach"
  ],
  "keyFunctions": [
    {
      "name": "constructor",
      "desc": "Initializes rate limiter with default limits for OpenAI (60 req/min) and Claude (50 req/min)",
      "inputs": "none",
      "outputs": "RateLimiter instance"
    },
    {
      "name": "configure",
      "desc": "Sets custom rate limit configuration for a specific LLM provider",
      "inputs": "provider (openai|claude), config (maxRequests, windowMs)",
      "outputs": "void"
    },
    {
      "name": "canMakeRequest",
      "desc": "Checks if a request can be made without exceeding the rate limit for the provider",
      "inputs": "provider (openai|claude)",
      "outputs": "boolean (true if request allowed, false if rate limit exceeded)"
    },
    {
      "name": "recordRequest",
      "desc": "Records the timestamp of a request to track usage against rate limits",
      "inputs": "provider (openai|claude)",
      "outputs": "void"
    }
  ],
  "dependencies": [],
  "intent": "Prevents the application from exceeding LLM provider API rate limits by tracking request timestamps and enforcing configurable request quotas per time window, ensuring reliable API access and avoiding service interruptions or penalties",
  "rawContent": "```json\n{\n  \"purpose\": \"Manages API rate limiting for LLM providers (OpenAI and Claude) to prevent exceeding their request quotas\",\n  \"userVisibleActions\": [\n    \"API requests are automatically throttled to stay within provider limits\",\n    \"Requests may be delayed or rejected if rate limits are exceeded\",\n    \"Different LLM providers (OpenAI, Claude) have different rate limit allowances\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure custom rate limits for each LLM provider (max requests per time window)\",\n    \"Check if a request can be made before calling the LLM API\",\n    \"Record requests to track usage against rate limits\",\n    \"OpenAI defaults to 60 requests per minute, Claude defaults to 50 requests per minute\",\n    \"Rate limits are enforced using a sliding time window approach\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"constructor\",\n      \"desc\": \"Initializes rate limiter with default limits for OpenAI (60 req/min) and Claude (50 req/min)\",\n      \"inputs\": \"none\",\n      \"outputs\": \"RateLimiter instance\"\n    },\n    {\n      \"name\": \"configure\",\n      \"desc\": \"Sets custom rate limit configuration for a specific LLM provider\",\n      \"inputs\": \"provider (openai|claude), config (maxRequests, windowMs)\",\n      \"outputs\": \"void\"\n    },\n    {\n      \"name\": \"canMakeRequest\",\n      \"desc\": \"Checks if a request can be made without exceeding the rate limit for the provider\",\n      \"inputs\": \"provider (openai|claude)\",\n      \"outputs\": \"boolean (true if request allowed, false if rate limit exceeded)\"\n    },\n    {\n      \"name\": \"recordRequest\",\n      \"desc\": \"Records the timestamp of a request to track usage against rate limits\",\n      \"inputs\": \"provider (openai|claude)\",\n      \"outputs\": \"void\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Prevents the application from exceeding LLM provider API rate limits by tracking request timestamps and enforcing configurable request quotas per time window, ensuring reliable API access and avoiding service interruptions or penalties\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-19T21:12:45.854Z"
  }
}