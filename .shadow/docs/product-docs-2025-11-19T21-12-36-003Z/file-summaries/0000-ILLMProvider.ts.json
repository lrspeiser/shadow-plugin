{
  "file": "src/ai/providers/ILLMProvider.ts",
  "role": "Core Logic",
  "purpose": "Defines the standard interface that all AI language model providers (like OpenAI, Claude, etc.) must implement for consistent interaction throughout the application.",
  "userVisibleActions": [
    "User receives AI-generated text responses to their queries",
    "User receives structured JSON data from AI requests when specific formats are needed",
    "User experiences consistent AI behavior regardless of which provider (OpenAI, Claude, etc.) is configured",
    "User sees AI responses that respect configured parameters like temperature and token limits"
  ],
  "developerVisibleActions": [
    "Developer implements this interface to add support for new AI providers",
    "Developer calls sendRequest() to get text responses from any configured AI provider",
    "Developer calls sendStructuredRequest() to get parsed JSON responses with optional schemas",
    "Developer checks isConfigured() to verify provider is ready before making requests",
    "Developer uses standardized message format with role and content for all providers",
    "Developer configures AI behavior using maxTokens, temperature, and responseFormat options",
    "Developer receives additional context like finish reasons and raw responses for debugging"
  ],
  "keyFunctions": [
    {
      "name": "isConfigured",
      "desc": "Verifies if the AI provider has valid credentials and is ready to accept requests",
      "inputs": "none",
      "outputs": "boolean indicating if provider is configured"
    },
    {
      "name": "sendRequest",
      "desc": "Sends a conversation to the AI provider and retrieves a text response",
      "inputs": "LLMRequestOptions containing model, messages, system prompt, and generation parameters",
      "outputs": "Promise<LLMResponse> with content string and metadata"
    },
    {
      "name": "sendStructuredRequest",
      "desc": "Sends a request expecting structured JSON output, with optional schema validation",
      "inputs": "LLMRequestOptions and optional schema definition",
      "outputs": "Promise<StructuredOutputResponse<T>> with parsed typed data and optional follow-up requests"
    },
    {
      "name": "getName",
      "desc": "Returns the human-readable name of the provider for identification",
      "inputs": "none",
      "outputs": "string with provider name"
    }
  ],
  "dependencies": [],
  "intent": "This file exists to create a unified abstraction layer over different AI providers (OpenAI, Claude, custom implementations), allowing the rest of the application to work with any AI provider through a consistent interface without needing to know provider-specific details. It solves the problem of vendor lock-in and makes the codebase provider-agnostic, enabling easy switching or addition of new AI providers.",
  "rawContent": "```json\n{\n  \"purpose\": \"Defines the standard interface that all AI language model providers (like OpenAI, Claude, etc.) must implement for consistent interaction throughout the application.\",\n  \"userVisibleActions\": [\n    \"User receives AI-generated text responses to their queries\",\n    \"User receives structured JSON data from AI requests when specific formats are needed\",\n    \"User experiences consistent AI behavior regardless of which provider (OpenAI, Claude, etc.) is configured\",\n    \"User sees AI responses that respect configured parameters like temperature and token limits\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer implements this interface to add support for new AI providers\",\n    \"Developer calls sendRequest() to get text responses from any configured AI provider\",\n    \"Developer calls sendStructuredRequest() to get parsed JSON responses with optional schemas\",\n    \"Developer checks isConfigured() to verify provider is ready before making requests\",\n    \"Developer uses standardized message format with role and content for all providers\",\n    \"Developer configures AI behavior using maxTokens, temperature, and responseFormat options\",\n    \"Developer receives additional context like finish reasons and raw responses for debugging\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Verifies if the AI provider has valid credentials and is ready to accept requests\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating if provider is configured\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a conversation to the AI provider and retrieves a text response\",\n      \"inputs\": \"LLMRequestOptions containing model, messages, system prompt, and generation parameters\",\n      \"outputs\": \"Promise<LLMResponse> with content string and metadata\"\n    },\n    {\n      \"name\": \"sendStructuredRequest\",\n      \"desc\": \"Sends a request expecting structured JSON output, with optional schema validation\",\n      \"inputs\": \"LLMRequestOptions and optional schema definition\",\n      \"outputs\": \"Promise<StructuredOutputResponse<T>> with parsed typed data and optional follow-up requests\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the human-readable name of the provider for identification\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string with provider name\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"This file exists to create a unified abstraction layer over different AI providers (OpenAI, Claude, custom implementations), allowing the rest of the application to work with any AI provider through a consistent interface without needing to know provider-specific details. It solves the problem of vendor lock-in and makes the codebase provider-agnostic, enabling easy switching or addition of new AI providers.\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-19T21:13:27.914Z"
  }
}