{
  "module": "src/ai",
  "moduleType": "other",
  "capabilities": [
    "Automatically manages AI API request rates to prevent exceeding provider limits",
    "Intelligently retries failed AI requests with exponential backoff",
    "Parses and structures AI-generated documentation into standardized formats",
    "Converts natural language AI responses into organized documentation components",
    "Provides seamless error handling and recovery for temporary API failures",
    "Ensures reliable AI interactions even under rate limiting or network issues"
  ],
  "summary": "This module provides robust AI request management infrastructure that ensures reliable and compliant interactions with LLM providers. It handles the complete lifecycle of AI requests from rate limiting to response processing, protecting users from API quota violations while maintaining a smooth experience.\n\nThe module automatically throttles outgoing requests to stay within provider rate limits, preventing errors before they occur. When temporary failures do happen (network issues, rate limits, transient errors), the retry handler automatically recovers with intelligent backoff strategies. Once responses arrive, the parser extracts and structures the AI-generated content into standardized documentation formats including file summaries, module descriptions, and product-level documentation.\n\nUsers experience seamless AI-powered documentation generation without needing to understand or manage API limitations, retry logic, or response formatting. The module handles all complexity behind the scenes, ensuring requests succeed when possible and fail gracefully when necessary, while always delivering structured, usable documentation output.",
  "files": [
    {
      "file": "src/ai/llmRateLimiter.ts",
      "role": "Core Logic",
      "purpose": "Prevents LLM API requests from exceeding provider rate limits by tracking and enforcing request quotas per time window",
      "userVisibleActions": [
        "User's AI requests are automatically throttled to prevent exceeding API limits",
        "User experiences prevented errors when too many AI requests are made too quickly",
        "User's requests are blocked when rate limits are reached within a time window"
      ],
      "developerVisibleActions": [
        "Developer checks if an AI request can be made before calling the API",
        "Developer records each successful AI request to update rate limit tracking",
        "Developer configures custom rate limits for different LLM providers (OpenAI, Claude)",
        "Developer receives boolean feedback on whether a request is allowed",
        "Developer sees default limits: OpenAI (60 req/min), Claude (50 req/min)"
      ],
      "keyFunctions": [
        {
          "name": "canMakeRequest",
          "desc": "Checks if a new request is allowed based on recent request history and configured limits",
          "inputs": "provider: LLMProvider ('openai' or 'claude')",
          "outputs": "boolean - true if request can proceed, false if rate limit reached"
        },
        {
          "name": "recordRequest",
          "desc": "Records a timestamp for a completed request to track usage against rate limits",
          "inputs": "provider: LLMProvider ('openai' or 'claude')",
          "outputs": "void - updates internal request history"
        },
        {
          "name": "configure",
          "desc": "Sets custom rate limit configuration for a specific LLM provider",
          "inputs": "provider: LLMProvider, config: RateLimitConfig (maxRequests, windowMs)",
          "outputs": "void - updates provider configuration"
        }
      ],
      "dependencies": [],
      "intent": "Protects the application from exceeding LLM API rate limits by implementing a sliding window rate limiter that tracks request timestamps per provider and enforces configured quotas, preventing API errors and service interruptions",
      "rawContent": "```json\n{\n  \"purpose\": \"Prevents LLM API requests from exceeding provider rate limits by tracking and enforcing request quotas per time window\",\n  \"userVisibleActions\": [\n    \"User's AI requests are automatically throttled to prevent exceeding API limits\",\n    \"User experiences prevented errors when too many AI requests are made too quickly\",\n    \"User's requests are blocked when rate limits are reached within a time window\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer checks if an AI request can be made before calling the API\",\n    \"Developer records each successful AI request to update rate limit tracking\",\n    \"Developer configures custom rate limits for different LLM providers (OpenAI, Claude)\",\n    \"Developer receives boolean feedback on whether a request is allowed\",\n    \"Developer sees default limits: OpenAI (60 req/min), Claude (50 req/min)\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"canMakeRequest\",\n      \"desc\": \"Checks if a new request is allowed based on recent request history and configured limits\",\n      \"inputs\": \"provider: LLMProvider ('openai' or 'claude')\",\n      \"outputs\": \"boolean - true if request can proceed, false if rate limit reached\"\n    },\n    {\n      \"name\": \"recordRequest\",\n      \"desc\": \"Records a timestamp for a completed request to track usage against rate limits\",\n      \"inputs\": \"provider: LLMProvider ('openai' or 'claude')\",\n      \"outputs\": \"void - updates internal request history\"\n    },\n    {\n      \"name\": \"configure\",\n      \"desc\": \"Sets custom rate limit configuration for a specific LLM provider\",\n      \"inputs\": \"provider: LLMProvider, config: RateLimitConfig (maxRequests, windowMs)\",\n      \"outputs\": \"void - updates provider configuration\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Protects the application from exceeding LLM API rate limits by implementing a sliding window rate limiter that tracks request timestamps per provider and enforces configured quotas, preventing API errors and service interruptions\"\n}\n```"
    },
    {
      "file": "src/ai/llmResponseParser.ts",
      "role": "Core Logic",
      "purpose": "Parses and extracts structured data from LLM text responses into standardized documentation formats",
      "userVisibleActions": [
        "Receives natural language analysis from AI and converts it into organized documentation",
        "Displays parsed file summaries showing what each code file does",
        "Shows module-level documentation describing groups of related files",
        "Presents product-level documentation explaining overall system purpose",
        "Provides fallback text parsing when AI responses aren't in expected JSON format"
      ],
      "developerVisibleActions": [
        "Call parseFileSummary() to convert LLM text into FileSummary objects",
        "Call parseModuleSummary() to extract module-level documentation",
        "Call parseProductDocumentation() to get high-level product analysis",
        "Call parseLLMInsights() to extract AI-generated insights about code",
        "Call parseProductPurpose() to understand overall product goals",
        "Get structured data with fallback text extraction when JSON parsing fails",
        "Extract sections from text using pattern matching for 'purpose', 'userVisibleActions', 'dependencies', etc.",
        "Extract list items from markdown-style bullet points in LLM responses",
        "Handle parsing errors gracefully with default values"
      ],
      "keyFunctions": [
        {
          "name": "parseFileSummary",
          "desc": "Converts LLM response text into a structured FileSummary object",
          "inputs": "content (LLM response text), filePath (file being analyzed), role (file's role in system)",
          "outputs": "FileSummary object with purpose, actions, functions, dependencies, and intent"
        },
        {
          "name": "parseModuleSummary",
          "desc": "Extracts module-level documentation from LLM response",
          "inputs": "content (LLM response text), moduleName (name of module being analyzed)",
          "outputs": "ModuleSummary object describing the module's purpose and components"
        },
        {
          "name": "parseProductDocumentation",
          "desc": "Parses high-level product documentation from LLM response",
          "inputs": "content (LLM response text)",
          "outputs": "EnhancedProductDocumentation object with product overview and architecture"
        },
        {
          "name": "parseLLMInsights",
          "desc": "Extracts AI-generated insights about code quality and patterns",
          "inputs": "content (LLM response text)",
          "outputs": "LLMInsights object with analysis findings"
        },
        {
          "name": "parseProductPurpose",
          "desc": "Extracts the overall purpose and goals of the product from LLM analysis",
          "inputs": "content (LLM response text), context (analysis context)",
          "outputs": "ProductPurposeAnalysis object describing what the product does"
        },
        {
          "name": "extractSection",
          "desc": "Finds and extracts a specific section from text response",
          "inputs": "content (text to search), sectionName (section identifier)",
          "outputs": "Extracted section text or empty string"
        },
        {
          "name": "extractListSection",
          "desc": "Extracts bullet-point lists from text responses",
          "inputs": "content (text to search), sectionName (list identifier)",
          "outputs": "Array of list items"
        }
      ],
      "dependencies": [
        "../fileDocumentation",
        "../llmService"
      ],
      "intent": "This file exists to bridge the gap between unstructured AI responses and structured documentation data. LLMs return natural language text, but the application needs consistent, typed data structures. This parser handles the messy reality of parsing AI output - trying JSON first, falling back to text extraction, and ensuring the application always gets valid documentation objects even when AI responses are malformed.",
      "rawContent": "```json\n{\n  \"purpose\": \"Parses and extracts structured data from LLM text responses into standardized documentation formats\",\n  \"userVisibleActions\": [\n    \"Receives natural language analysis from AI and converts it into organized documentation\",\n    \"Displays parsed file summaries showing what each code file does\",\n    \"Shows module-level documentation describing groups of related files\",\n    \"Presents product-level documentation explaining overall system purpose\",\n    \"Provides fallback text parsing when AI responses aren't in expected JSON format\"\n  ],\n  \"developerVisibleActions\": [\n    \"Call parseFileSummary() to convert LLM text into FileSummary objects\",\n    \"Call parseModuleSummary() to extract module-level documentation\",\n    \"Call parseProductDocumentation() to get high-level product analysis\",\n    \"Call parseLLMInsights() to extract AI-generated insights about code\",\n    \"Call parseProductPurpose() to understand overall product goals\",\n    \"Get structured data with fallback text extraction when JSON parsing fails\",\n    \"Extract sections from text using pattern matching for 'purpose', 'userVisibleActions', 'dependencies', etc.\",\n    \"Extract list items from markdown-style bullet points in LLM responses\",\n    \"Handle parsing errors gracefully with default values\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"parseFileSummary\",\n      \"desc\": \"Converts LLM response text into a structured FileSummary object\",\n      \"inputs\": \"content (LLM response text), filePath (file being analyzed), role (file's role in system)\",\n      \"outputs\": \"FileSummary object with purpose, actions, functions, dependencies, and intent\"\n    },\n    {\n      \"name\": \"parseModuleSummary\",\n      \"desc\": \"Extracts module-level documentation from LLM response\",\n      \"inputs\": \"content (LLM response text), moduleName (name of module being analyzed)\",\n      \"outputs\": \"ModuleSummary object describing the module's purpose and components\"\n    },\n    {\n      \"name\": \"parseProductDocumentation\",\n      \"desc\": \"Parses high-level product documentation from LLM response\",\n      \"inputs\": \"content (LLM response text)\",\n      \"outputs\": \"EnhancedProductDocumentation object with product overview and architecture\"\n    },\n    {\n      \"name\": \"parseLLMInsights\",\n      \"desc\": \"Extracts AI-generated insights about code quality and patterns\",\n      \"inputs\": \"content (LLM response text)\",\n      \"outputs\": \"LLMInsights object with analysis findings\"\n    },\n    {\n      \"name\": \"parseProductPurpose\",\n      \"desc\": \"Extracts the overall purpose and goals of the product from LLM analysis\",\n      \"inputs\": \"content (LLM response text), context (analysis context)\",\n      \"outputs\": \"ProductPurposeAnalysis object describing what the product does\"\n    },\n    {\n      \"name\": \"extractSection\",\n      \"desc\": \"Finds and extracts a specific section from text response\",\n      \"inputs\": \"content (text to search), sectionName (section identifier)\",\n      \"outputs\": \"Extracted section text or empty string\"\n    },\n    {\n      \"name\": \"extractListSection\",\n      \"desc\": \"Extracts bullet-point lists from text responses\",\n      \"inputs\": \"content (text to search), sectionName (list identifier)\",\n      \"outputs\": \"Array of list items\"\n    }\n  ],\n  \"dependencies\": [\n    \"../fileDocumentation\",\n    \"../llmService\"\n  ],\n  \"intent\": \"This file exists to bridge the gap between unstructured AI responses and structured documentation data. LLMs return natural language text, but the application needs consistent, typed data structures. This parser handles the messy reality of parsing AI output - trying JSON first, falling back to text extraction, and ensuring the application always gets valid documentation objects even when AI responses are malformed.\"\n}\n```"
    },
    {
      "file": "src/ai/llmRetryHandler.ts",
      "role": "Core Logic",
      "purpose": "Handles automatic retries of LLM API requests when they fail due to temporary errors like rate limits or network issues",
      "userVisibleActions": [
        "When an AI request fails temporarily, the system automatically retries it without user intervention",
        "Requests that fail due to rate limits or network issues are automatically retried with increasing delays between attempts",
        "Failed requests are retried up to a maximum number of times before finally failing",
        "Users experience seamless AI interactions even when temporary API issues occur"
      ],
      "developerVisibleActions": [
        "Configure maximum retry attempts, delay timings, and backoff multipliers for API requests",
        "Specify which error types should trigger automatic retries (rate limits, timeouts, network errors)",
        "Receive callbacks on each retry attempt with attempt number and error details",
        "Get retry metadata including total number of attempts made for successful requests",
        "Classify errors as retryable or non-retryable based on error messages and codes",
        "Apply exponential backoff delays between retry attempts to avoid overwhelming services",
        "Non-retryable errors (like authentication failures) are immediately thrown without retry"
      ],
      "keyFunctions": [
        {
          "name": "executeWithRetry",
          "desc": "Executes an async operation with automatic retry logic and exponential backoff",
          "inputs": "operation function to execute, retry options (maxRetries, delays, retryable errors, callback)",
          "outputs": "Promise resolving to operation result with attempt count metadata"
        },
        {
          "name": "isRetryableError",
          "desc": "Determines if an error should trigger a retry attempt based on error type and message",
          "inputs": "error object, list of retryable error patterns",
          "outputs": "boolean indicating if the error is retryable"
        }
      ],
      "dependencies": [],
      "intent": "Provides resilience for LLM API requests by automatically retrying temporary failures like rate limits, timeouts, and network errors with exponential backoff, ensuring a more reliable AI experience without requiring user or developer intervention for transient issues",
      "rawContent": "```json\n{\n  \"purpose\": \"Handles automatic retries of LLM API requests when they fail due to temporary errors like rate limits or network issues\",\n  \"userVisibleActions\": [\n    \"When an AI request fails temporarily, the system automatically retries it without user intervention\",\n    \"Requests that fail due to rate limits or network issues are automatically retried with increasing delays between attempts\",\n    \"Failed requests are retried up to a maximum number of times before finally failing\",\n    \"Users experience seamless AI interactions even when temporary API issues occur\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure maximum retry attempts, delay timings, and backoff multipliers for API requests\",\n    \"Specify which error types should trigger automatic retries (rate limits, timeouts, network errors)\",\n    \"Receive callbacks on each retry attempt with attempt number and error details\",\n    \"Get retry metadata including total number of attempts made for successful requests\",\n    \"Classify errors as retryable or non-retryable based on error messages and codes\",\n    \"Apply exponential backoff delays between retry attempts to avoid overwhelming services\",\n    \"Non-retryable errors (like authentication failures) are immediately thrown without retry\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"executeWithRetry\",\n      \"desc\": \"Executes an async operation with automatic retry logic and exponential backoff\",\n      \"inputs\": \"operation function to execute, retry options (maxRetries, delays, retryable errors, callback)\",\n      \"outputs\": \"Promise resolving to operation result with attempt count metadata\"\n    },\n    {\n      \"name\": \"isRetryableError\",\n      \"desc\": \"Determines if an error should trigger a retry attempt based on error type and message\",\n      \"inputs\": \"error object, list of retryable error patterns\",\n      \"outputs\": \"boolean indicating if the error is retryable\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Provides resilience for LLM API requests by automatically retrying temporary failures like rate limits, timeouts, and network errors with exponential backoff, ensuring a more reliable AI experience without requiring user or developer intervention for transient issues\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-20T00:59:17.031Z"
  }
}