{
  "module": "src/domain/services/testing",
  "moduleType": "tests",
  "capabilities": [
    "Automatically analyze codebases to identify testable functions and create prioritized test plans based on complexity and risk",
    "Detect existing test environments and generate setup plans for multiple programming languages (TypeScript, JavaScript, Python, Java, C++) and frameworks (Jest, Mocha, pytest, JUnit, Google Test)",
    "Generate unit tests for code functions using AI in small batches with real-time progress tracking",
    "Execute test suites and capture detailed results including pass/fail status, error messages, and execution statistics",
    "Automatically validate and fix failing tests through AI-powered retry attempts",
    "Provide comprehensive test reports showing overall test health and coverage"
  ],
  "summary": "This module provides an end-to-end AI-powered testing workflow that helps developers create, execute, and maintain unit tests for their codebase. It begins by analyzing the code to understand which functions are testable and creates a prioritized test plan based on complexity, dependencies, and risk factors. The module then detects the current test environment configuration and can generate setup instructions if the testing infrastructure is incomplete.\n\nOnce the environment is ready, the module generates unit tests for selected functions using AI assistance, processing them in small batches while providing real-time progress updates. After test generation, it automatically executes the test suites using the appropriate testing framework (Jest, Mocha, pytest, JUnit, or Google Test) and captures detailed results including pass/fail counts, error messages, stack traces, and execution timing.\n\nWhen tests fail, the module leverages AI to automatically analyze and fix the failures through multiple retry attempts, significantly reducing manual debugging effort. Throughout the entire workflow, users receive progress notifications, detailed statistics, and comprehensive test reports that help them understand their codebase's test coverage and health. This automated approach transforms testing from a manual, time-consuming task into a streamlined, AI-assisted process.",
  "files": [
    {
      "file": "src/domain/services/testing/llmTestGenerationService.ts",
      "role": "Core Logic",
      "purpose": "Generates unit tests for code functions using an LLM in small batches with progress tracking and execution validation.",
      "userVisibleActions": [
        "Progress updates showing which function is being tested and completion percentage",
        "Test generation results indicating success or failure for each function",
        "Notifications when test files are created or updated",
        "Error messages if test generation fails for specific functions"
      ],
      "developerVisibleActions": [
        "Developer triggers test generation for selected functions",
        "System reads source code and extracts function signatures",
        "LLM generates test code based on function context and existing mocks",
        "Generated tests are validated by executing them",
        "Test files are written to the test directory structure",
        "Progress callback provides real-time status updates during batch processing",
        "Results map shows success/failure status for each function tested"
      ],
      "keyFunctions": [
        {
          "name": "generateTestBatch",
          "desc": "Generates tests for multiple functions in a batch with progress tracking",
          "inputs": "functions array, workspace root path, LLM service, optional progress callback",
          "outputs": "Map of function names to test generation results"
        },
        {
          "name": "extractFunctionSource",
          "desc": "Retrieves the source code for a specific function from its file",
          "inputs": "TestableFunction object, workspace root path",
          "outputs": "Source code string for the function"
        },
        {
          "name": "generateTestForFunction",
          "desc": "Creates a test for a single function using LLM and validates it",
          "inputs": "Function details, source code, LLM service, workspace root, test framework type, existing mocks",
          "outputs": "TestGenerationResult with test code and execution status"
        },
        {
          "name": "buildGenerationPrompt",
          "desc": "Constructs the LLM prompt with function context and mock information",
          "inputs": "Function object, source code, test framework name, existing mocks",
          "outputs": "Formatted prompt string for LLM"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestableFunction",
        "TestGenerationState",
        "TestGenerationResult",
        "buildGenerationPrompt",
        "TestExecutionService",
        "SWLogger"
      ],
      "intent": "Automates test generation by leveraging AI to write unit tests incrementally for code functions, reducing manual test writing effort while ensuring generated tests are valid through automatic execution and validation.",
      "rawContent": "```json\n{\n  \"purpose\": \"Generates unit tests for code functions using an LLM in small batches with progress tracking and execution validation.\",\n  \"userVisibleActions\": [\n    \"Progress updates showing which function is being tested and completion percentage\",\n    \"Test generation results indicating success or failure for each function\",\n    \"Notifications when test files are created or updated\",\n    \"Error messages if test generation fails for specific functions\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer triggers test generation for selected functions\",\n    \"System reads source code and extracts function signatures\",\n    \"LLM generates test code based on function context and existing mocks\",\n    \"Generated tests are validated by executing them\",\n    \"Test files are written to the test directory structure\",\n    \"Progress callback provides real-time status updates during batch processing\",\n    \"Results map shows success/failure status for each function tested\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"generateTestBatch\",\n      \"desc\": \"Generates tests for multiple functions in a batch with progress tracking\",\n      \"inputs\": \"functions array, workspace root path, LLM service, optional progress callback\",\n      \"outputs\": \"Map of function names to test generation results\"\n    },\n    {\n      \"name\": \"extractFunctionSource\",\n      \"desc\": \"Retrieves the source code for a specific function from its file\",\n      \"inputs\": \"TestableFunction object, workspace root path\",\n      \"outputs\": \"Source code string for the function\"\n    },\n    {\n      \"name\": \"generateTestForFunction\",\n      \"desc\": \"Creates a test for a single function using LLM and validates it\",\n      \"inputs\": \"Function details, source code, LLM service, workspace root, test framework type, existing mocks\",\n      \"outputs\": \"TestGenerationResult with test code and execution status\"\n    },\n    {\n      \"name\": \"buildGenerationPrompt\",\n      \"desc\": \"Constructs the LLM prompt with function context and mock information\",\n      \"inputs\": \"Function object, source code, test framework name, existing mocks\",\n      \"outputs\": \"Formatted prompt string for LLM\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestableFunction\",\n    \"TestGenerationState\",\n    \"TestGenerationResult\",\n    \"buildGenerationPrompt\",\n    \"TestExecutionService\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"Automates test generation by leveraging AI to write unit tests incrementally for code functions, reducing manual test writing effort while ensuring generated tests are valid through automatic execution and validation.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestPlanningService.ts",
      "role": "Core Logic",
      "purpose": "Analyzes code functions and creates a prioritized test plan using LLM guidance based on complexity, dependencies, and product context.",
      "userVisibleActions": [
        "Receives analysis of which functions in their codebase are testable and which are not",
        "Gets a prioritized test plan showing which functions should be tested first based on complexity and risk",
        "Views function groups organized by testing priority",
        "Sees statistics about total functions vs testable functions in their codebase"
      ],
      "developerVisibleActions": [
        "Calls analyzeFunctions() to extract function metadata from code analysis results",
        "Calls createTestPlan() to generate an LLM-based test strategy with prioritized function groups",
        "Calls saveTestPlan() to persist the generated test plan to disk for later use",
        "Reviews log messages showing test planning progress and statistics",
        "Uses the resulting TestPlan object containing function groups, testability status, and testing priorities"
      ],
      "keyFunctions": [
        {
          "name": "analyzeFunctions",
          "desc": "Extracts and normalizes function metadata from code analysis results",
          "inputs": "codeAnalysis object containing function information",
          "outputs": "Array of function objects with name, file, lines, complexity, parameters, and return type"
        },
        {
          "name": "createTestPlan",
          "desc": "Generates a prioritized test plan by sending function data to LLM with product and architecture context",
          "inputs": "AnalysisContext, functions array, llmService, optional productDocs and architectureInsights",
          "outputs": "TestPlan object with function groups, testability counts, and testing priorities"
        },
        {
          "name": "saveTestPlan",
          "desc": "Persists the generated test plan to a JSON file on disk",
          "inputs": "TestPlan object and file path where to save",
          "outputs": "File written to disk (void return)"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "testPlanTypes",
        "testPrompts",
        "AnalysisContext",
        "SWLogger"
      ],
      "intent": "This service exists to bridge code analysis results with test generation by intelligently prioritizing which functions should be tested first. It solves the problem of determining testing priorities in large codebases by using LLM reasoning to consider complexity, dependencies, and business context rather than testing everything indiscriminately.",
      "rawContent": "```json\n{\n  \"purpose\": \"Analyzes code functions and creates a prioritized test plan using LLM guidance based on complexity, dependencies, and product context.\",\n  \"userVisibleActions\": [\n    \"Receives analysis of which functions in their codebase are testable and which are not\",\n    \"Gets a prioritized test plan showing which functions should be tested first based on complexity and risk\",\n    \"Views function groups organized by testing priority\",\n    \"Sees statistics about total functions vs testable functions in their codebase\"\n  ],\n  \"developerVisibleActions\": [\n    \"Calls analyzeFunctions() to extract function metadata from code analysis results\",\n    \"Calls createTestPlan() to generate an LLM-based test strategy with prioritized function groups\",\n    \"Calls saveTestPlan() to persist the generated test plan to disk for later use\",\n    \"Reviews log messages showing test planning progress and statistics\",\n    \"Uses the resulting TestPlan object containing function groups, testability status, and testing priorities\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"analyzeFunctions\",\n      \"desc\": \"Extracts and normalizes function metadata from code analysis results\",\n      \"inputs\": \"codeAnalysis object containing function information\",\n      \"outputs\": \"Array of function objects with name, file, lines, complexity, parameters, and return type\"\n    },\n    {\n      \"name\": \"createTestPlan\",\n      \"desc\": \"Generates a prioritized test plan by sending function data to LLM with product and architecture context\",\n      \"inputs\": \"AnalysisContext, functions array, llmService, optional productDocs and architectureInsights\",\n      \"outputs\": \"TestPlan object with function groups, testability counts, and testing priorities\"\n    },\n    {\n      \"name\": \"saveTestPlan\",\n      \"desc\": \"Persists the generated test plan to a JSON file on disk\",\n      \"inputs\": \"TestPlan object and file path where to save\",\n      \"outputs\": \"File written to disk (void return)\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"testPlanTypes\",\n    \"testPrompts\",\n    \"AnalysisContext\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This service exists to bridge code analysis results with test generation by intelligently prioritizing which functions should be tested first. It solves the problem of determining testing priorities in large codebases by using LLM reasoning to consider complexity, dependencies, and business context rather than testing everything indiscriminately.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestSetupService.ts",
      "role": "Core Logic",
      "purpose": "Detects test environment configuration and generates AI-powered test setup plans for different programming languages and testing frameworks.",
      "userVisibleActions": [
        "Automatically detects the programming language used in the workspace (TypeScript, JavaScript, Python, Java, C++)",
        "Identifies existing testing frameworks (Jest, Mocha, pytest, JUnit, Google Test)",
        "Discovers test directories and configuration files",
        "Generates a test setup plan with recommended dependencies and configuration",
        "Provides step-by-step setup instructions for missing test infrastructure",
        "Reports whether the test environment is ready or needs setup"
      ],
      "developerVisibleActions": [
        "Call detectTestEnvironment() to scan workspace and identify language, framework, and existing test configuration",
        "Use generateTestSetupPlan() to create an AI-generated setup plan based on detected environment",
        "Execute executeSetupPlan() to automatically install dependencies and create configuration files",
        "Receive TestEnvironment object with details about detected language, framework, and file paths",
        "Get TestSetupPlan with missing dependencies, configuration templates, and setup steps",
        "Obtain SetupExecutionResult indicating success/failure and any errors encountered"
      ],
      "keyFunctions": [
        {
          "name": "detectTestEnvironment",
          "desc": "Scans workspace to detect programming language, testing framework, and existing test infrastructure",
          "inputs": "workspaceRoot (string path)",
          "outputs": "TestEnvironment object with language, framework, paths, and status"
        },
        {
          "name": "generateTestSetupPlan",
          "desc": "Uses LLM to generate a comprehensive test setup plan based on detected environment",
          "inputs": "TestEnvironment object, optional LLM provider",
          "outputs": "TestSetupPlan with dependencies, configuration, and setup steps"
        },
        {
          "name": "executeSetupPlan",
          "desc": "Executes the generated setup plan by installing dependencies and creating configuration files",
          "inputs": "TestSetupPlan object, workspaceRoot path",
          "outputs": "SetupExecutionResult with success status and execution details"
        },
        {
          "name": "getAllFiles",
          "desc": "Recursively retrieves all files in a directory for language detection",
          "inputs": "directory path (string)",
          "outputs": "Array of file paths"
        },
        {
          "name": "detectTestingFramework",
          "desc": "Identifies which testing framework is in use based on dependencies and config files",
          "inputs": "packageJsonPath (string)",
          "outputs": "Framework name string (jest, mocha, pytest, junit, etc.)"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "child_process",
        "./types/testSetupTypes",
        "../../prompts/testPrompts",
        "../../../logger"
      ],
      "intent": "Automates the detection and setup of test environments across multiple programming languages and testing frameworks, reducing manual configuration burden by using LLM to generate intelligent, context-aware setup plans tailored to the detected project structure.",
      "rawContent": "```json\n{\n  \"purpose\": \"Detects test environment configuration and generates AI-powered test setup plans for different programming languages and testing frameworks.\",\n  \"userVisibleActions\": [\n    \"Automatically detects the programming language used in the workspace (TypeScript, JavaScript, Python, Java, C++)\",\n    \"Identifies existing testing frameworks (Jest, Mocha, pytest, JUnit, Google Test)\",\n    \"Discovers test directories and configuration files\",\n    \"Generates a test setup plan with recommended dependencies and configuration\",\n    \"Provides step-by-step setup instructions for missing test infrastructure\",\n    \"Reports whether the test environment is ready or needs setup\"\n  ],\n  \"developerVisibleActions\": [\n    \"Call detectTestEnvironment() to scan workspace and identify language, framework, and existing test configuration\",\n    \"Use generateTestSetupPlan() to create an AI-generated setup plan based on detected environment\",\n    \"Execute executeSetupPlan() to automatically install dependencies and create configuration files\",\n    \"Receive TestEnvironment object with details about detected language, framework, and file paths\",\n    \"Get TestSetupPlan with missing dependencies, configuration templates, and setup steps\",\n    \"Obtain SetupExecutionResult indicating success/failure and any errors encountered\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"detectTestEnvironment\",\n      \"desc\": \"Scans workspace to detect programming language, testing framework, and existing test infrastructure\",\n      \"inputs\": \"workspaceRoot (string path)\",\n      \"outputs\": \"TestEnvironment object with language, framework, paths, and status\"\n    },\n    {\n      \"name\": \"generateTestSetupPlan\",\n      \"desc\": \"Uses LLM to generate a comprehensive test setup plan based on detected environment\",\n      \"inputs\": \"TestEnvironment object, optional LLM provider\",\n      \"outputs\": \"TestSetupPlan with dependencies, configuration, and setup steps\"\n    },\n    {\n      \"name\": \"executeSetupPlan\",\n      \"desc\": \"Executes the generated setup plan by installing dependencies and creating configuration files\",\n      \"inputs\": \"TestSetupPlan object, workspaceRoot path\",\n      \"outputs\": \"SetupExecutionResult with success status and execution details\"\n    },\n    {\n      \"name\": \"getAllFiles\",\n      \"desc\": \"Recursively retrieves all files in a directory for language detection\",\n      \"inputs\": \"directory path (string)\",\n      \"outputs\": \"Array of file paths\"\n    },\n    {\n      \"name\": \"detectTestingFramework\",\n      \"desc\": \"Identifies which testing framework is in use based on dependencies and config files\",\n      \"inputs\": \"packageJsonPath (string)\",\n      \"outputs\": \"Framework name string (jest, mocha, pytest, junit, etc.)\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"child_process\",\n    \"./types/testSetupTypes\",\n    \"../../prompts/testPrompts\",\n    \"../../../logger\"\n  ],\n  \"intent\": \"Automates the detection and setup of test environments across multiple programming languages and testing frameworks, reducing manual configuration burden by using LLM to generate intelligent, context-aware setup plans tailored to the detected project structure.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestValidationService.ts",
      "role": "Core Logic",
      "purpose": "Validates tests by running them and automatically fixing failures using LLM assistance",
      "userVisibleActions": [
        "Tests are automatically executed in the workspace",
        "Test results show pass/fail status with counts",
        "Failing tests are automatically fixed through multiple retry attempts",
        "Progress updates appear in logs showing test execution status",
        "Test reports are generated showing overall test health"
      ],
      "developerVisibleActions": [
        "Call runTests() to execute Jest tests in workspace and get execution results",
        "Call fixFailingTest() to automatically repair a failing test using LLM suggestions",
        "Receive TestExecutionResult objects containing passed/failed/error counts",
        "Access test reports and summaries showing test suite health metrics",
        "Configure maximum fix attempts to control auto-repair retry limit",
        "Get detailed failure information including error messages and stack traces"
      ],
      "keyFunctions": [
        {
          "name": "runTests",
          "desc": "Executes all tests or a specific test file and returns execution results with pass/fail counts",
          "inputs": "workspaceRoot: string, testFile?: string (optional specific test file)",
          "outputs": "Promise<TestExecutionResult[]> with test outcomes and statistics"
        },
        {
          "name": "fixFailingTest",
          "desc": "Attempts to automatically fix a failing test by using LLM to generate corrections",
          "inputs": "testFilePath: string, executionResult: TestExecutionResult, workspaceRoot: string, llmService: any, maxAttempts: number (default 3)",
          "outputs": "Promise<{ success: boolean; attempts: number; finalError?: string }> indicating fix outcome"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestExecutionService",
        "TestExecutionResult",
        "TestReport",
        "TestReportSummary",
        "buildFixPrompt",
        "SWLogger"
      ],
      "intent": "This service exists to automate test validation and repair in the development workflow. It solves the problem of manually fixing failing tests by using LLM intelligence to analyze test failures and automatically generate fixes, reducing developer time spent on test maintenance and improving test suite reliability.",
      "rawContent": "```json\n{\n  \"purpose\": \"Validates tests by running them and automatically fixing failures using LLM assistance\",\n  \"userVisibleActions\": [\n    \"Tests are automatically executed in the workspace\",\n    \"Test results show pass/fail status with counts\",\n    \"Failing tests are automatically fixed through multiple retry attempts\",\n    \"Progress updates appear in logs showing test execution status\",\n    \"Test reports are generated showing overall test health\"\n  ],\n  \"developerVisibleActions\": [\n    \"Call runTests() to execute Jest tests in workspace and get execution results\",\n    \"Call fixFailingTest() to automatically repair a failing test using LLM suggestions\",\n    \"Receive TestExecutionResult objects containing passed/failed/error counts\",\n    \"Access test reports and summaries showing test suite health metrics\",\n    \"Configure maximum fix attempts to control auto-repair retry limit\",\n    \"Get detailed failure information including error messages and stack traces\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"runTests\",\n      \"desc\": \"Executes all tests or a specific test file and returns execution results with pass/fail counts\",\n      \"inputs\": \"workspaceRoot: string, testFile?: string (optional specific test file)\",\n      \"outputs\": \"Promise<TestExecutionResult[]> with test outcomes and statistics\"\n    },\n    {\n      \"name\": \"fixFailingTest\",\n      \"desc\": \"Attempts to automatically fix a failing test by using LLM to generate corrections\",\n      \"inputs\": \"testFilePath: string, executionResult: TestExecutionResult, workspaceRoot: string, llmService: any, maxAttempts: number (default 3)\",\n      \"outputs\": \"Promise<{ success: boolean; attempts: number; finalError?: string }> indicating fix outcome\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestExecutionService\",\n    \"TestExecutionResult\",\n    \"TestReport\",\n    \"TestReportSummary\",\n    \"buildFixPrompt\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This service exists to automate test validation and repair in the development workflow. It solves the problem of manually fixing failing tests by using LLM intelligence to analyze test failures and automatically generate fixes, reducing developer time spent on test maintenance and improving test suite reliability.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/testExecutionService.ts",
      "role": "Core Logic",
      "purpose": "Executes test suites (Jest/Mocha) and captures their results, errors, and execution statistics",
      "userVisibleActions": [
        "Tests run and complete with pass/fail status",
        "Test execution progress is displayed",
        "Test failures show error messages and stack traces",
        "Test duration and timing information is shown",
        "Overall test summary shows passed/failed/error counts"
      ],
      "developerVisibleActions": [
        "Call runJest() or runMocha() to execute tests for specific files or entire test suite",
        "Receive structured test results with pass/fail counts and error details",
        "Get detailed error information including test names, messages, and stack traces",
        "Access test execution duration and performance metrics",
        "Handle test execution errors and timeouts gracefully"
      ],
      "keyFunctions": [
        {
          "name": "runJest",
          "desc": "Executes Jest test runner for a specific file or all tests",
          "inputs": "workspaceRoot (string), optional testFile (string)",
          "outputs": "Promise<TestExecutionResult[]> containing test outcomes, counts, and errors"
        },
        {
          "name": "runMocha",
          "desc": "Executes Mocha test runner for a specific file or all tests",
          "inputs": "workspaceRoot (string), optional testFile (string)",
          "outputs": "Promise<TestExecutionResult[]> containing test outcomes, counts, and errors"
        },
        {
          "name": "parseJestOutput",
          "desc": "Parses JSON output from Jest test runner into structured results",
          "inputs": "stdout (string), stderr (string)",
          "outputs": "TestExecutionResult[] with parsed test outcomes and error details"
        },
        {
          "name": "parseMochaOutput",
          "desc": "Parses JSON output from Mocha test runner into structured results",
          "inputs": "stdout (string), stderr (string)",
          "outputs": "TestExecutionResult[] with parsed test outcomes and error details"
        }
      ],
      "dependencies": [
        "child_process",
        "path",
        "./types/testResultTypes"
      ],
      "intent": "Provides a unified interface for running different test frameworks (Jest and Mocha) in the workspace, capturing their output, and converting it into a consistent format that can be displayed to users and processed by other parts of the application. Solves the problem of executing tests programmatically and handling their results reliably with proper error handling and timeout management.",
      "rawContent": "```json\n{\n  \"purpose\": \"Executes test suites (Jest/Mocha) and captures their results, errors, and execution statistics\",\n  \"userVisibleActions\": [\n    \"Tests run and complete with pass/fail status\",\n    \"Test execution progress is displayed\",\n    \"Test failures show error messages and stack traces\",\n    \"Test duration and timing information is shown\",\n    \"Overall test summary shows passed/failed/error counts\"\n  ],\n  \"developerVisibleActions\": [\n    \"Call runJest() or runMocha() to execute tests for specific files or entire test suite\",\n    \"Receive structured test results with pass/fail counts and error details\",\n    \"Get detailed error information including test names, messages, and stack traces\",\n    \"Access test execution duration and performance metrics\",\n    \"Handle test execution errors and timeouts gracefully\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"runJest\",\n      \"desc\": \"Executes Jest test runner for a specific file or all tests\",\n      \"inputs\": \"workspaceRoot (string), optional testFile (string)\",\n      \"outputs\": \"Promise<TestExecutionResult[]> containing test outcomes, counts, and errors\"\n    },\n    {\n      \"name\": \"runMocha\",\n      \"desc\": \"Executes Mocha test runner for a specific file or all tests\",\n      \"inputs\": \"workspaceRoot (string), optional testFile (string)\",\n      \"outputs\": \"Promise<TestExecutionResult[]> containing test outcomes, counts, and errors\"\n    },\n    {\n      \"name\": \"parseJestOutput\",\n      \"desc\": \"Parses JSON output from Jest test runner into structured results\",\n      \"inputs\": \"stdout (string), stderr (string)\",\n      \"outputs\": \"TestExecutionResult[] with parsed test outcomes and error details\"\n    },\n    {\n      \"name\": \"parseMochaOutput\",\n      \"desc\": \"Parses JSON output from Mocha test runner into structured results\",\n      \"inputs\": \"stdout (string), stderr (string)\",\n      \"outputs\": \"TestExecutionResult[] with parsed test outcomes and error details\"\n    }\n  ],\n  \"dependencies\": [\n    \"child_process\",\n    \"path\",\n    \"./types/testResultTypes\"\n  ],\n  \"intent\": \"Provides a unified interface for running different test frameworks (Jest and Mocha) in the workspace, capturing their output, and converting it into a consistent format that can be displayed to users and processed by other parts of the application. Solves the problem of executing tests programmatically and handling their results reliably with proper error handling and timeout management.\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-20T01:01:16.602Z"
  }
}