{
  "file": "src/ai/llmRateLimiter.ts",
  "role": "Core Logic",
  "purpose": "Prevents LLM API requests from exceeding provider rate limits by tracking and enforcing request quotas per time window",
  "userVisibleActions": [
    "User's AI requests are automatically throttled to prevent exceeding API limits",
    "User experiences prevented errors when too many AI requests are made too quickly",
    "User's requests are blocked when rate limits are reached within a time window"
  ],
  "developerVisibleActions": [
    "Developer checks if an AI request can be made before calling the API",
    "Developer records each successful AI request to update rate limit tracking",
    "Developer configures custom rate limits for different LLM providers (OpenAI, Claude)",
    "Developer receives boolean feedback on whether a request is allowed",
    "Developer sees default limits: OpenAI (60 req/min), Claude (50 req/min)"
  ],
  "keyFunctions": [
    {
      "name": "canMakeRequest",
      "desc": "Checks if a new request is allowed based on recent request history and configured limits",
      "inputs": "provider: LLMProvider ('openai' or 'claude')",
      "outputs": "boolean - true if request can proceed, false if rate limit reached"
    },
    {
      "name": "recordRequest",
      "desc": "Records a timestamp for a completed request to track usage against rate limits",
      "inputs": "provider: LLMProvider ('openai' or 'claude')",
      "outputs": "void - updates internal request history"
    },
    {
      "name": "configure",
      "desc": "Sets custom rate limit configuration for a specific LLM provider",
      "inputs": "provider: LLMProvider, config: RateLimitConfig (maxRequests, windowMs)",
      "outputs": "void - updates provider configuration"
    }
  ],
  "dependencies": [],
  "intent": "Protects the application from exceeding LLM API rate limits by implementing a sliding window rate limiter that tracks request timestamps per provider and enforces configured quotas, preventing API errors and service interruptions",
  "rawContent": "```json\n{\n  \"purpose\": \"Prevents LLM API requests from exceeding provider rate limits by tracking and enforcing request quotas per time window\",\n  \"userVisibleActions\": [\n    \"User's AI requests are automatically throttled to prevent exceeding API limits\",\n    \"User experiences prevented errors when too many AI requests are made too quickly\",\n    \"User's requests are blocked when rate limits are reached within a time window\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer checks if an AI request can be made before calling the API\",\n    \"Developer records each successful AI request to update rate limit tracking\",\n    \"Developer configures custom rate limits for different LLM providers (OpenAI, Claude)\",\n    \"Developer receives boolean feedback on whether a request is allowed\",\n    \"Developer sees default limits: OpenAI (60 req/min), Claude (50 req/min)\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"canMakeRequest\",\n      \"desc\": \"Checks if a new request is allowed based on recent request history and configured limits\",\n      \"inputs\": \"provider: LLMProvider ('openai' or 'claude')\",\n      \"outputs\": \"boolean - true if request can proceed, false if rate limit reached\"\n    },\n    {\n      \"name\": \"recordRequest\",\n      \"desc\": \"Records a timestamp for a completed request to track usage against rate limits\",\n      \"inputs\": \"provider: LLMProvider ('openai' or 'claude')\",\n      \"outputs\": \"void - updates internal request history\"\n    },\n    {\n      \"name\": \"configure\",\n      \"desc\": \"Sets custom rate limit configuration for a specific LLM provider\",\n      \"inputs\": \"provider: LLMProvider, config: RateLimitConfig (maxRequests, windowMs)\",\n      \"outputs\": \"void - updates provider configuration\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Protects the application from exceeding LLM API rate limits by implementing a sliding window rate limiter that tracks request timestamps per provider and enforces configured quotas, preventing API errors and service interruptions\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-20T00:48:03.544Z"
  }
}