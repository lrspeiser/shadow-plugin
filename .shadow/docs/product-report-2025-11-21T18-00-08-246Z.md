# Product Documentation Report

## Executive Summary

Shadow Watch is an AI-powered VS Code extension that transforms how developers understand, document, and maintain their codebases. By leveraging advanced language models (OpenAI GPT and Anthropic Claude), it automatically analyzes code architecture, generates comprehensive documentation, identifies quality issues, and creates test suites—all within the familiar VS Code environment. The extension continuously monitors code changes and provides actionable insights through intuitive tree-based interfaces, helping developers reduce technical debt, improve test coverage, and maintain up-to-date documentation without manual effort.

## Product Overview

Shadow Watch serves as an intelligent development companion that bridges the gap between static code analysis and actionable development guidance. It analyzes entire codebases to understand architecture, extract patterns, and identify improvement opportunities, then presents findings through multiple integrated VS Code interfaces including sidebar navigators, insights panels, the Problems panel, and output channels.

The extension operates autonomously once configured, automatically re-analyzing code when files are saved and maintaining synchronized analysis results across sessions. By connecting to AI language models, it generates human-readable documentation that explains what applications do from user perspectives, suggests specific refactorings with migration strategies, creates comprehensive test plans, and identifies architectural improvements.

### Value Proposition

- **Automated Documentation**: Eliminates manual documentation effort by generating comprehensive product and architecture documentation automatically
- **Intelligent Analysis**: Provides AI-powered insights that go beyond basic static analysis to understand code purpose and context
- **Test Automation**: Discovers untested functions and generates complete test suites with automatic validation and fixing
- **Real-time Monitoring**: Keeps analysis results synchronized with code changes through automatic file watching
- **Seamless Integration**: Works entirely within VS Code with no context switching required
- **Persistent Knowledge**: Caches all analysis results locally for instant access and historical tracking

## What It Does

### Core Capabilities

- **Comprehensive Code Analysis**: Analyzes entire codebase architecture by parsing Abstract Syntax Trees to extract metadata about functions, dependencies, complexity metrics, and code structure
- **AI-Powered Documentation Generation**: Generates human-readable product documentation describing what applications do, including overviews, features, user workflows, and architecture descriptions
- **Automated Test Suite Creation**: Identifies untested functions, generates unit tests in batches, executes them immediately, and automatically fixes failures using AI assistance
- **Real-time Code Monitoring**: Watches file changes throughout the workspace and automatically updates analysis results when code is modified
- **Quality Issue Detection**: Identifies circular dependencies, dead code, orphaned files, overly complex functions, large files, and missing test coverage
- **Intelligent Refactoring Recommendations**: Analyzes code complexity and suggests specific refactoring opportunities with detailed migration strategies
- **Test Coverage Mapping**: Links test files to source code to identify untested functions and components
- **Multi-Provider AI Support**: Supports both OpenAI GPT and Anthropic Claude with seamless provider switching
- **Diagnostic Integration**: Displays warnings and errors in VS Code's Problems panel with severity indicators
- **Direct Code Navigation**: Enables clicking on insights or issues to jump directly to corresponding source code locations

### Advanced Features

- **Iterative Analysis**: Progressively deepens code understanding by requesting additional file contents or grep searches as needed
- **Test Environment Detection**: Automatically identifies testing frameworks (Jest, Mocha, Vitest, Pytest), validates configuration, and detects missing dependencies
- **Self-Healing Tests**: Executes generated tests, detects failures, and automatically attempts fixes using AI assistance
- **Multi-Format Output**: Formats analysis results for different AI chat interfaces (Cursor IDE, ChatGPT, Claude)
- **Persistent Caching**: Stores all analysis results to disk with timestamp organization for instant workspace reopening
- **Rate Limiting & Retry Logic**: Manages API request throttling and handles temporary failures with exponential backoff
- **Batch Processing**: Generates tests in small batches to manage costs and provide incremental progress feedback
- **Entry Point Detection**: Identifies application entry points and visualizes code flow

## User Perspective

### GUI Interactions

Users interact with Shadow Watch through multiple integrated VS Code interfaces:

- **Sidebar Navigator**: Browse code analysis results in a hierarchical tree view showing architecture insights, test coverage, and code structure
- **Insights Panel**: View AI-generated recommendations and architecture insights in a dedicated panel with detailed explanations
- **Problems Panel**: See diagnostic warnings and errors with severity indicators for quick issue identification
- **Editor Navigation**: Click on any insight, issue, or analysis result to jump directly to the source code location with automatic cursor positioning
- **Status Bar**: Monitor analysis progress through status indicators and progress notifications
- **Test Results View**: Access detailed test execution results showing pass/fail status for each test file
- **Documentation Viewer**: View comprehensive product documentation generated from codebase analysis
- **Configuration Settings**: Switch between AI providers (OpenAI, Claude) and customize analysis behavior through VS Code settings
- **Copy to Clipboard**: Copy formatted analysis results for sharing or pasting into AI chat interfaces
- **Command Palette**: Trigger analysis, clear cache, and access all extension features through standard VS Code commands

### Workflows

1. **Initial Setup**: Install extension, configure AI provider API key, workspace automatically analyzed on activation
2. **Continuous Monitoring**: Extension watches file saves and automatically updates analysis results
3. **Documentation Generation**: Trigger product documentation generation, AI analyzes codebase and generates comprehensive docs
4. **Test Generation**: Run test generation command, extension identifies untested functions, creates test suite, executes and validates tests
5. **Issue Resolution**: View issues in Problems panel, click to navigate to source, implement fixes, analysis updates automatically
6. **Refactoring Guidance**: Review AI-generated refactoring recommendations with detailed migration steps
7. **Code Navigation**: Browse analysis results in tree view, click items to jump to relevant code locations

### CLI Interactions

Shadow Watch does not provide direct CLI interactions but supports integration into automated workflows.

### API Interactions

Shadow Watch does not expose a public API but internally integrates with AI provider APIs (OpenAI and Anthropic).

### CI/CD Integration

- **Automated Analysis**: Integrate code analysis into CI/CD pipelines through command-line execution
- **Test Validation**: Generate and validate test suites as part of automated build processes
- **Documentation Artifacts**: Produce documentation during deployment workflows
- **Cached Results**: Leverage cached analysis for faster subsequent pipeline runs

## Problems Solved

### Documentation Challenges

- **Manual Documentation Burden**: Eliminates time-consuming manual documentation by automatically generating comprehensive product and architecture docs
- **Documentation Staleness**: Keeps documentation synchronized with code through automatic regeneration when files change
- **Inconsistent Documentation**: Maintains consistency across team members through automated generation with standardized formats

### Code Quality Issues

- **Circular Dependencies**: Detects circular dependency bugs early in development before they cause runtime issues
- **Dead Code Accumulation**: Identifies unused code and orphaned files that waste maintenance effort
- **Technical Debt**: Highlights overly complex functions requiring refactoring with specific recommendations
- **Large File Management**: Detects "god objects" and large files that should be split into smaller components

### Testing Gaps

- **Test Coverage Blind Spots**: Identifies untested functions and automatically generates comprehensive test suites
- **Manual Test Creation**: Reduces time spent writing boilerplate test code through AI-powered generation
- **Test Quality Issues**: Validates generated tests by executing them and automatically fixing failures

### Developer Productivity

- **Codebase Navigation**: Saves time navigating large codebases by providing instant jumps from insights to source locations
- **Context Switching**: Prevents context switching by integrating all analysis, documentation, and testing tools within VS Code
- **Unfamiliar Code Understanding**: Reduces time understanding unfamiliar codebases through AI-generated architecture insights
- **API Rate Limits**: Prevents API rate limit errors through automatic throttling and intelligent retry logic

## Architecture Overview

Shadow Watch is built as a modular VS Code extension with a layered architecture that separates concerns between UI presentation, business logic, AI integration, and infrastructure services.

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    VS Code Extension Host                    │
├─────────────────────────────────────────────────────────────┤
│  Presentation Layer                                          │
│  ├─ Tree Views (Product Navigator, Insights, Test Results)  │
│  ├─ Webview Panels (Detailed Reports, Documentation)        │
│  ├─ Diagnostic Provider (Problems Panel Integration)        │
│  └─ Status Bar & Progress Notifications                     │
├─────────────────────────────────────────────────────────────┤
│  Application Layer                                           │
│  ├─ Command Handlers (Analyze, Generate Docs, Run Tests)    │
│  ├─ Navigation Handler (Code Location Jumping)              │
│  ├─ Configuration Manager (Settings & User Preferences)     │
│  └─ Bootstrap & Lifecycle Management                        │
├─────────────────────────────────────────────────────────────┤
│  Domain Layer                                                │
│  ├─ Code Analysis Engine (AST Parsing, Metrics)             │
│  ├─ Insight Generator (Quality Analysis, Recommendations)   │
│  ├─ Test Generation System (Planning, Generation, Validation)│
│  ├─ Documentation Formatter (Markdown Generation)           │
│  └─ Dependency Tracker (Relationship Mapping)               │
├─────────────────────────────────────────────────────────────┤
│  AI Integration Layer                                        │
│  ├─ Provider Factory (OpenAI, Claude)                       │
│  ├─ LLM Client (Request/Response Handling)                  │
│  ├─ Response Parser (JSON Extraction, Validation)           │
│  ├─ Rate Limiter (Quota Management)                         │
│  ├─ Retry Handler (Exponential Backoff)                     │
│  └─ Prompt Builders (Task-Specific Instructions)            │
├─────────────────────────────────────────────────────────────┤
│  Infrastructure Layer                                        │
│  ├─ File System Service (Cached I/O, Bulk Processing)       │
│  ├─ File Watcher (Change Detection, Event Handling)         │
│  ├─ Persistence Service (Analysis Result Storage)           │
│  ├─ Cache Manager (In-Memory & Disk Caching)                │
│  └─ Progress Service (User Feedback)                        │
└─────────────────────────────────────────────────────────────┘
         │                    │                    │
         ▼                    ▼                    ▼
    OpenAI API         Anthropic API        Local Filesystem
```

### Key Architectural Patterns

**Modular Service Architecture**: Each functional area (analysis, testing, documentation) is encapsulated in independent services that communicate through well-defined interfaces.

**Provider Abstraction**: AI language model providers are abstracted behind a unified interface, enabling seamless switching between OpenAI and Claude without changing consumer code.

**Event-Driven Updates**: File watchers detect changes and emit events that trigger automatic re-analysis, keeping all views synchronized with code state.

**Caching Strategy**: Multi-layered caching (in-memory file cache, persistent analysis results) optimizes performance and enables instant workspace reopening.

**Iterative Processing**: AI analysis uses an iterative workflow where the model can request additional context (file reads, grep searches) to progressively deepen understanding.

**Asynchronous Operations**: All long-running operations (analysis, AI requests, test execution) run asynchronously with progress feedback to maintain UI responsiveness.

**Error Resilience**: Comprehensive error handling with automatic retries, rate limiting, and fallback mechanisms ensures reliable operation despite API failures or network issues.

## Key Components & Modules

### Core Extension Components

#### Extension Bootstrap (`src/extension.ts`, `src/domain/bootstrap/`)
- **Purpose**: Main entry point that orchestrates extension initialization and lifecycle management
- **Responsibilities**: 
  - Initializes all extension components when VS Code loads
  - Registers commands for workspace analysis, documentation generation, and test execution
  - Sets up tree views, status bar indicators, and diagnostic panels
  - Manages file watchers and coordinates service dependencies
  - Handles extension activation, updates, and cleanup
- **Interactions**: Coordinates between all other modules as the central orchestrator

#### Configuration Manager (`src/config/configurationManager.ts`)
- **Purpose**: Centralizes all extension settings and preference management
- **Responsibilities**:
  - Provides unified access to user settings
  - Notifies components of configuration changes through reactive listeners
  - Manages LLM provider configuration (API keys, endpoints, model selection)
  - Controls analysis behavior (automatic triggers, inline hints, severity filtering)
  - Sets performance parameters (timeouts, file size limits, retry behavior)
- **Interactions**: All components query configuration and subscribe to change notifications

### Analysis & Insights

#### Code Analysis Engine (`src/analyzer.ts`, `src/analysis/`)
- **Purpose**: Performs comprehensive static code analysis by parsing Abstract Syntax Trees
- **Responsibilities**:
  - Extracts metadata about functions, dependencies, and code structure
  - Analyzes function complexity metrics and behavioral patterns
  - Maps relationships between functions and modules
  - Identifies conditional logic paths for branch coverage
  - Detects entry points and code flow patterns
- **Data Structures**: `AnalysisResult`, `FunctionInfo`, `CodeDependency`
- **Interactions**: Feeds results to insight generator and documentation generator

#### Insight Generator (`src/insightGenerator.ts`)
- **Purpose**: Analyzes code structure to identify quality issues and improvement opportunities
- **Responsibilities**:
  - Detects circular dependencies and orphaned files
  - Identifies large files and overly complex functions
  - Highlights dead code and untested components
  - Generates specific recommendations with severity levels
  - Produces actionable refactoring guidance
- **Data Structures**: `ArchitectureInsight`
- **Interactions**: Consumes analysis results, displays insights in tree views and Problems panel

#### Dependency Tracker
- **Purpose**: Maps relationships between code components to understand code flow
- **Responsibilities**:
  - Tracks import/export relationships between modules
  - Detects circular dependency chains
  - Identifies orphaned files with no incoming dependencies
  - Visualizes code flow through the application
- **Data Structures**: `CodeDependency`
- **Interactions**: Integrated within code analysis engine

### AI Integration Layer

#### LLM Provider System (`src/ai/providers/`)
- **Purpose**: Abstracts multiple AI language model providers behind unified interface
- **Responsibilities**:
  - Factory pattern for dynamic provider instantiation (OpenAI, Claude)
  - Sends messages and receives intelligent text responses
  - Generates structured JSON outputs based on schemas
  - Validates API keys and configuration
  - Supports streaming responses for real-time output
- **Interactions**: Used by all AI-powered features (documentation, testing, insights)

#### Rate Limiter (`src/ai/llmRateLimiter.ts`)
- **Purpose**: Manages API request throttling to prevent exceeding provider quotas
- **Responsibilities**:
  - Tracks request counts per time window
  - Enforces rate limits independently for different providers
  - Queues requests that would exceed limits
  - Provides quota status information
- **Interactions**: Wraps all AI provider requests

#### Retry Handler (`src/ai/llmRetryHandler.ts`)
- **Purpose**: Handles temporary API failures with intelligent backoff strategies
- **Responsibilities**:
  - Implements exponential backoff retry logic
  - Distinguishes between transient and permanent failures
  - Automatically retries failed requests with increasing delays
  - Tracks retry attempts and maximum retry limits
- **Interactions**: Wraps AI provider requests after rate limiting

#### Response Parser (`src/ai/llmResponseParser.ts`)
- **Purpose**: Converts unstructured LLM text responses into structured, typed data
- **Responsibilities**:
  - Extracts JSON from natural language responses
  - Validates responses against expected schemas
  - Provides fallback extraction when strict parsing fails
  - Parses file summaries, module summaries, and product documentation
- **Interactions**: Processes all LLM responses before consumption by other modules

### Documentation & Formatting

#### AI Documentation Generator (`src/llmIntegration.ts`)
- **Purpose**: Leverages language models to generate human-readable documentation
- **Responsibilities**:
  - Analyzes codebase to understand application purpose
  - Generates product overviews and architecture descriptions
  - Creates API documentation and feature descriptions
  - Produces documentation from code structure analysis
- **Data Structures**: `ProductDocumentation`
- **Interactions**: Uses AI provider system, formats output through documentation formatter

#### Documentation Formatter (`src/domain/formatters/documentationFormatter.ts`)
- **Purpose**: Transforms raw documentation data into polished Markdown documents
- **Responsibilities**:
  - Applies consistent formatting rules to documentation
  - Organizes content into clear sections (overview, features, architecture)
  - Groups features by user type (GUI, CLI, API) and functional domain
  - Adds quality indicators and metadata timestamps
  - Structures behavior descriptions using who/what/when/where/why/how framework
- **Interactions**: Consumes AI-generated documentation, produces final Markdown output

#### Prompt Builders (`src/domain/prompts/`)
- **Purpose**: Generates specialized instruction templates for guiding language models
- **Responsibilities**:
  - Creates prompts for product documentation generation
  - Builds prompts for test extraction and generation
  - Generates prompts for refactoring recommendations
  - Structures prompts for module and file-level documentation
  - Manages token budgets and context windows
- **Interactions**: Used by all AI-powered features to construct LLM requests

### Testing Automation

#### Test Generation System (`src/domain/services/testing/llmTestGenerationService.ts`)
- **Purpose**: Orchestrates end-to-end automated test generation workflow
- **Responsibilities**:
  - Manages test generation from discovery through validation
  - Generates unit tests in batches with progress tracking
  - Executes tests immediately after generation
  - Validates test quality and tracks statistics
  - Coordinates between all testing subcomponents
- **Data Structures**: `TestGenerationProgress`
- **Interactions**: Coordinates function extraction, test planning, test execution, and validation

#### Function Extraction Service (`src/domain/services/testing/llmFunctionExtractionService.ts`)
- **Purpose**: Identifies functions that need test coverage using AI analysis
- **Responsibilities**:
  - Scans source files to discover testable functions
  - Assesses function complexity and testability characteristics
  - Identifies functions requiring mocks or special setup
  - Extracts function signatures and metadata
- **Interactions**: First step in test generation workflow, feeds into test planning

#### Test Planning Service (`src/domain/services/testing/llmTestPlanningService.ts`)
- **Purpose**: Creates prioritized test plans based on function analysis
- **Responsibilities**:
  - Organizes functions by priority level (critical, high, medium, low)
  - Determines testing order based on complexity and importance
  - Creates structured test plans with metadata
  - Provides batch generation strategies
- **Data Structures**: `TestPlan`
- **Interactions**: Consumes function extraction results, feeds into test generation

#### Test Validation Service (`src/domain/services/testing/llmTestValidationService.ts`)
- **Purpose**: Executes tests and automatically fixes failures using AI assistance
- **Responsibilities**:
  - Runs generated test files and captures results
  - Detects test failures with detailed error messages
  - Automatically attempts to fix broken tests using AI
  - Tracks retry attempts and success rates
  - Provides comprehensive error reporting
- **Data Structures**: `TestExecutionResult`
- **Interactions**: Final step in test generation workflow, validates test quality

#### Test Configuration Service (`src/domain/services/testConfigurationService.ts`)
- **Purpose**: Automatically detects and validates test environment setup
- **Responsibilities**:
  - Identifies testing framework (Jest, Mocha, Vitest, Pytest)
  - Validates test configuration files
  - Detects missing dependencies
  - Provides setup recommendations
  - Checks for required test infrastructure
- **Interactions**: Ensures test environment is ready before test generation

### Infrastructure Services

#### File System Service (`src/infrastructure/fileSystem/`)
- **Purpose**: Optimized file system operations with intelligent caching
- **Responsibilities**:
  - Caches file contents in memory to reduce disk I/O
  - Processes multiple files in parallel
  - Automatically filters common non-source directories
  - Invalidates cache when files change
  - Provides consistent error handling
- **Interactions**: Used by all components that read project files

#### File Watcher Service (`src/domain/services/fileWatcherService.ts`, `src/fileWatcher.ts`)
- **Purpose**: Monitors file changes to trigger automatic re-analysis
- **Responsibilities**:
  - Watches for file saves and document changes
  - Detects modifications across the workspace
  - Triggers analysis updates when code changes
  - Maintains real-time synchronization of views
- **Interactions**: Triggers analysis engine when files change

#### Persistence Service (`src/infrastructure/persistence/analysisResultStore.ts`)
- **Purpose**: Manages persistent storage of analysis results
- **Responsibilities**:
  - Saves analysis results to `.shadow` directory
  - Creates timestamped folders for each analysis run
  - Stores product documentation, insights, and summaries
  - Maintains historical tracking of analysis evolution
  - Organizes artifacts in structured directory hierarchy
- **Interactions**: Stores output from analysis and documentation generation

#### Cache Manager (`src/cache.ts`)
- **Purpose**: Provides caching layer for instant workspace reopening
- **Responsibilities**:
  - Caches analysis results in memory and on disk
  - Enables instant access to previous analysis
  - Preserves results between VS Code sessions
  - Manages cache invalidation
- **Interactions**: Wraps analysis results for all consumers

#### Progress Service (`src/infrastructure/progressService.ts`)
- **Purpose**: Manages user feedback for long-running operations
- **Responsibilities**:
  - Displays progress notifications with status messages
  - Shows indicators in notification area and status bar
  - Allows users to cancel operations
  - Provides standardized progress feedback
  - Tracks operation status in real-time
- **Interactions**: Used by all long-running operations

### UI & Navigation

#### Navigation Handler (`src/domain/handlers/navigationHandler.ts`)
- **Purpose**: Handles code navigation from UI interactions
- **Responsibilities**:
  - Opens files when users click insights or results
  - Positions cursor at target line and column
  - Highlights relevant code ranges
  - Displays detailed item information in webview panels
  - Opens external links in browser
  - Provides error feedback for failed navigation
- **Interactions**: Responds to user clicks in tree views and panels

#### Context Builder (`src/context/analysisContextBuilder.ts`)
- **Purpose**: Transforms analysis results into LLM-ready context format
- **Responsibilities**:
  - Converts code analysis into structured format for AI consumption
  - Persists analysis data to workspace `.shadow/docs` directory
  - Maintains historical record for future reference
  - Enables analysis reuse across sessions
- **Interactions**: Bridge between analysis engine and AI integration layer

## Key Functions & Data Structures

### Core Extension Functions

#### `activate` (src/extension.ts)
**Purpose**: Extension entry point that initializes all components when VS Code loads the extension  
**Workflow**: Orchestrates setup of tree views, commands, file watchers, and AI services  
**Returns**: Promise that resolves when extension is fully activated

#### `analyzeWorkspace` (src/extension.ts)
**Purpose**: Triggers comprehensive analysis of the entire workspace  
**Workflow**: Scans all source files, extracts metadata, generates insights about code structure  
**Impact**: Updates all views with fresh analysis results

### Documentation Functions

#### `generateProductDocs`
**Purpose**: Invokes AI language model to generate comprehensive product documentation  
**Workflow**: Analyzes codebase structure → sends to LLM → parses response → formats output  
**Returns**: `ProductDocumentation` object with overview, features, and architecture

#### `formatDocumentation` (src/domain/formatters/documentationFormatter.ts)
**Purpose**: Converts raw product documentation data into polished Markdown documents  
**Workflow**: Takes structured data → applies formatting rules → organizes sections → adds metadata  
**Returns**: Formatted Markdown string ready for display or export

#### `parseProductDocumentation` (src/ai/llmResponseParser.ts)
**Purpose**: Extracts structured product documentation from LLM text responses  
**Workflow**: Attempts strict JSON parsing → falls back to extraction if needed → validates structure  
**Returns**: Typed `ProductDocumentation` object

### Testing Functions

#### `extractTestableFunctions` (src/domain/services/testing/llmFunctionExtractionService.ts)
**Purpose**: Uses AI to scan source files and identify functions that should have unit tests  
**Workflow**: Reads source files → sends to LLM for analysis → extracts function metadata  
**Returns**: Array of testable functions with complexity and priority information

#### `generateTestPlan` (src/domain/services/testing/llmTestPlanningService.ts)
**Purpose**: Creates prioritized test plans by analyzing functions  
**Workflow**: Takes function list → assesses priority → organizes by level → creates batches  
**Returns**: `TestPlan` object with functions grouped by priority

#### `generateTests` (src/domain/services/testing/llmTestGenerationService.ts)
**Purpose**: Generates unit test code for functions in small batches  
**Workflow**: Takes test plan → generates in batches → executes immediately → tracks progress  
**Returns**: `TestGenerationProgress` with statistics and results

#### `validateTests` (src/domain/services/testing/llmTestValidationService.ts)
**Purpose**: Runs generated test files and automatically fixes failures  
**Workflow**: Executes tests → detects failures → uses AI to fix → retries until success  
**Returns**: `TestExecutionResult` with pass/fail status and error details

#### `detectTestFramework` (src/domain/services/testConfigurationService.ts)
**Purpose**: Automatically identifies which testing framework is configured  
**Workflow**: Examines package.json → checks config files → analyzes test patterns  
**Returns**: Framework name (Jest, Mocha, Vitest, Pytest) and configuration status

### AI Integration Functions

#### `analyzeWithLLM` (src/llmIntegration.ts)
**Purpose**: Sends code analysis requests to configured AI provider  
**Workflow**: Builds prompt → checks rate limits → sends request → handles retries → parses response  
**Returns**: Structured insights extracted from natural language responses

#### `checkRateLimit` (src/ai/llmRateLimiter.ts)
**Purpose**: Enforces API rate limits by tracking request counts  
**Workflow**: Tracks requests per time window → checks against quota → queues if needed  
**Returns**: Boolean indicating whether request can proceed

#### `retryWithBackoff` (src/ai/llmRetryHandler.ts)
**Purpose**: Implements exponential backoff retry logic for failed API requests  
**Workflow**: Attempts request → if fails, waits exponentially longer → retries → reports success/failure  
**Returns**: Promise that resolves when request succeeds or max retries reached

### Analysis Functions

#### `processIncrementalRequests` (src/domain/services/incrementalAnalysisService.ts)
**Purpose**: Handles iterative analysis by processing file read and grep search requests from AI  
**Workflow**: Receives AI requests → reads files or searches code → sends back to AI → continues until complete  
**Returns**: Complete analysis results after iterative refinement

#### `navigateToCodeItem` (src/domain/handlers/navigationHandler.ts)
**Purpose**: Opens editor to specific file location when user clicks analysis results  
**Workflow**: Receives navigation request → opens file → positions cursor → highlights code  
**Returns**: void (side effect: opens editor)

### Core Data Structures

#### `AnalysisResult` (interface in src/analyzer.ts)
```typescript
interface AnalysisResult {
  files: FileMetadata[];           // All analyzed files with metadata
  functions: FunctionInfo[];       // All discovered functions
  dependencies: CodeDependency[];  // Dependency relationships
  testMappings: TestMapping[];     // Test-to-source mappings
  insights: ArchitectureInsight[]; // AI-generated recommendations
  metrics: QualityMetrics;         // Code quality statistics
  entryPoints: string[];           // Application entry points
  timestamp: Date;                 // Analysis execution time
}
```
**Purpose**: Comprehensive container for all code analysis results  
**Usage**: Passed between analysis engine, insight generator, and documentation generator

#### `ProductDocumentation` (interface in src/fileDocumentation.ts)
```typescript
interface ProductDocumentation {
  overview: string;                    // What the product is and does
  features: Feature[];                 // Key capabilities organized by category
  userPerspectives: UserPerspective[]; // How different users interact (GUI, CLI, API)
  workflows: Workflow[];               // Common usage workflows
  architecture: ArchitectureOverview;  // High-level architecture description
  problemsSolved: string[];            // Key problems addressed
  confidenceScore: number;             // AI confidence in documentation accuracy
}
```
**Purpose**: Structured documentation describing application from user perspective  
**Usage**: Generated by AI, formatted for display, stored for historical tracking

#### `TestPlan` (interface in src/domain/services/testing/types/testPlanTypes.ts)
```typescript
interface TestPlan {
  criticalPriority: TestableFunctionGroup;  // Must-test functions
  highPriority: TestableFunctionGroup;      // Important functions
  mediumPriority: TestableFunctionGroup;    // Standard functions
  lowPriority: TestableFunctionGroup;       // Nice-to-have functions
  totalFunctions: number;                   // Count of all testable functions
  estimatedTime: string;                    // Estimated generation time
}
```
**Purpose**: Organizes functions for test generation by priority  
**Usage**: Created by test planning service, consumed by test generation service

#### `TestGenerationProgress` (interface in src/domain/services/testing/types/testPlanTypes.ts)
```typescript
interface TestGenerationProgress {
  phase: TestPhase;              // Current phase (setup, planning, generation, validation)
  totalFunctions: number;        // Total functions to test
  processedFunctions: number;    // Functions processed so far
  generatedTests: number;        // Tests successfully generated
  passedTests: number;           // Tests that passed validation
  failedTests: number;           // Tests that failed
  currentBatch: number;          // Current batch number
  totalBatches: number;          // Total batches to process
  message: string;               // Current status message
}
```
**Purpose**: Tracks progress through test generation phases  
**Usage**: Updated throughout test generation workflow, displayed in UI

#### `TestExecutionResult` (interface in src/domain/services/testing/types/testResultTypes.ts)
```typescript
interface TestExecutionResult {
  testFile: string;              // Path to test file
  passed: boolean;               // Overall pass/fail status
  executionTime: number;         // Duration in milliseconds
  testCount: number;             // Number of tests executed
  passCount: number;             // Number of tests passed
  failCount: number;             // Number of tests failed
  errors: TestError[];           // Detailed error messages with stack traces
  output: string;                // Raw test runner output
}
```
**Purpose**: Contains results from running test files  
**Usage**: Generated by test execution, used by validation service to fix failures

#### `ArchitectureInsight` (interface in src/analyzer.ts)
```typescript
interface ArchitectureInsight {
  category: InsightCategory;     // Type (dependency, quality, testing, refactoring)
  severity: Severity;            // Critical, warning, info
  title: string;                 // Brief description
  description: string;           // Detailed explanation
  location?: CodeLocation;       // File and line number
  recommendation: string;        // Specific action to take
  estimatedEffort: string;       // Time/complexity estimate
}
```
**Purpose**: AI-generated recommendation about code architecture or quality  
**Usage**: Displayed in insights panel, Problems panel, and tree views

#### `FunctionInfo` (interface in src/analyzer.ts)
```typescript
interface FunctionInfo {
  name: string;                  // Function identifier
  filePath: string;              // Source file location
  startLine: number;             // Starting line number
  parameters: Parameter[];       // Function parameters with types
  returnType?: string;           // Return type if available
  dependencies: string[];        // Functions/modules this depends on
  complexity: number;            // Cyclomatic complexity score
  isAsync: boolean;              // Whether function is async
  isExported: boolean;           // Whether function is exported
  documentation?: string;        // JSDoc or similar documentation
}
```
**Purpose**: Detailed metadata about a function  
**Usage**: Extracted by analysis engine, used for test generation and insights

#### `CodeDependency` (interface in src/analyzer.ts)
```typescript
interface CodeDependency {
  source: string;                // Importing file path
  target: string;                // Imported file/module path
  type: DependencyType;          // Import, require, etc.
  isCircular: boolean;           // Whether part of circular chain
  isExternal: boolean;           // Whether external package
}
```
**Purpose**: Represents dependency relationships between code files  
**Usage**: Used to detect circular dependencies and understand code flow

#### `TestMapping` (interface in src/analyzer.ts)
```typescript
interface TestMapping {
  testFile: string;              // Path to test file
  sourceFile: string;            // Path to source file being tested
  coverage: CoverageInfo;        // Coverage statistics
  untestedFunctions: string[];   // Functions lacking tests
}
```
**Purpose**: Links test files to source code files  
**Usage**: Identifies coverage gaps, enables test-to-source navigation

#### `LLMProviderConfig` (interface in src/config/configurationManager.