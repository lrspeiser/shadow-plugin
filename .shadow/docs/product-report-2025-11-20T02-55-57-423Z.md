# Product Documentation Report

## Executive Summary

Shadow Watch is a Visual Studio Code extension that revolutionizes software development workflows by providing AI-powered code intelligence, automated documentation generation, and intelligent test creation. The extension analyzes codebases to generate comprehensive documentation explaining what applications do from a user perspective, identifies architectural patterns and code quality issues, and automatically creates unit tests with validation and fixing capabilities. By integrating deeply with VS Code's interface, Shadow Watch enables developers to navigate seamlessly between AI-generated insights and source code while maintaining up-to-date documentation through automatic re-analysis on file changes.

## Product Overview

Shadow Watch serves as an intelligent development assistant that eliminates the burden of manual documentation writing and accelerates code understanding for software development teams. The extension leverages large language models (OpenAI GPT and Anthropic Claude) to analyze codebases, understand their structure and purpose, and generate human-readable documentation that explains code functionality from multiple perspectives.

The extension addresses critical pain points in modern software development: outdated documentation, slow onboarding for new team members, time-consuming test creation, and difficulty understanding unfamiliar codebases. Shadow Watch solves these challenges through automated analysis that runs continuously in the background, keeping documentation synchronized with code changes and providing immediate insights into code quality, architecture patterns, and potential improvements.

Beyond documentation, Shadow Watch functions as a comprehensive code intelligence platform that identifies circular dependencies, orphaned files, code smells, and refactoring opportunities. The testing capabilities extend from generating prioritized test plans based on code complexity to creating complete unit tests with proper mocks and assertions, validating test correctness through execution, and automatically fixing failing tests through iterative AI-powered corrections.

The extension integrates naturally into existing development workflows, surfacing insights through multiple interfaces: inline diagnostics displaying code issues as squiggly underlines in the editor, browsable tree views in the sidebar showing code structure and statistics, exportable markdown reports suitable for documentation repositories, and clickable navigation that jumps directly from insights to relevant source code locations.

## What It Does

### Code Analysis & Documentation
- Analyzes entire workspaces or individual files to generate comprehensive product documentation
- Creates module-level documentation describing user-facing capabilities and system contributions
- Generates file-level documentation explaining purpose, key functions, and architectural roles
- Identifies architecture patterns, design principles, and structural organization
- Detects code quality issues including circular dependencies, orphaned files, and code smells
- Provides refactoring recommendations with detailed migration instructions
- Calculates codebase statistics including total files, lines of code, and entry points

### Testing Capabilities
- Generates prioritized unit test plans based on function complexity and importance
- Creates complete unit test code with appropriate framework setup, mocks, and assertions
- Automatically validates tests by executing them and capturing detailed results
- Fixes failing tests iteratively through AI-powered analysis of error messages
- Detects test framework configuration and identifies missing dependencies
- Supports multiple test frameworks including Jest, Mocha, Pytest, and JUnit
- Provides comprehensive test reports with pass/fail counts, execution duration, and coverage metrics

### User Interface Features
- Displays inline diagnostics showing code issues as squiggly underlines in the editor
- Provides hierarchical tree views in the sidebar for browsing code structure and analysis results
- Shows real-time progress indicators in the status bar during analysis operations
- Enables click-through navigation from insights directly to source code locations
- Presents formatted documentation and reports in readable markdown
- Offers webview panels for detailed test result visualization
- Updates automatically when files are saved or modified

### AI Integration & Configuration
- Supports multiple AI providers including OpenAI GPT and Anthropic Claude
- Allows switching between different language models through configuration
- Formats output in multiple styles optimized for different AI assistants (Cursor, ChatGPT, Generic, Compact)
- Implements rate limiting to prevent API quota exhaustion
- Provides automatic retry logic with exponential backoff for transient failures
- Caches analysis results for fast repeated access without re-scanning
- Enables incremental analysis that progressively gathers code context across multiple AI requests

### Export & Sharing
- Exports analysis results and documentation to markdown files
- Saves results to `.shadow` directory with unique identifiers and timestamps
- Creates documentation artifacts suitable for version control systems
- Produces test code that integrates with existing CI/CD pipelines
- Generates architecture insights that inform code review processes
- Maintains historical records of all analysis runs for tracking changes over time

## User Perspective

### GUI Interface

Developers interact with Shadow Watch primarily through Visual Studio Code's integrated interface:

**Sidebar Tree Views**: Browse hierarchical views of code analysis results organized by modules, files, and functions. Click on any item to navigate directly to its source code location. View codebase statistics including total files, lines of code, and complexity metrics.

**Inline Diagnostics**: See code quality issues highlighted with squiggly underlines directly in the editor. Hover over underlined code to view detailed diagnostic messages. Access all diagnostics through the Problems panel with severity-based filtering.

**Status Bar Indicators**: Monitor real-time progress during long-running operations such as workspace analysis, test generation, or documentation creation. View completion percentages and operation status at a glance.

**Markdown Documentation**: Review generated documentation in formatted markdown within VS Code's preview pane. Navigate through sections covering product overview, features, architecture, and improvement recommendations.

**Test Result Webviews**: Examine comprehensive test reports showing passed/failed counts, execution duration, coverage metrics, and detailed error messages with stack traces for failing tests.

**Action Buttons**: Trigger analysis operations, generate tests, export results, or switch AI providers through accessible command buttons in the interface.

### CLI Interface

Power users can execute extension commands efficiently through keyboard shortcuts and the command palette:

**Command Palette Actions**: Access all extension functionality by pressing `Cmd+Shift+P` (Mac) or `Ctrl+Shift+P` (Windows/Linux) and typing "Shadow Watch" to see available commands.

**File Analysis**: Trigger analysis on the currently open file using keyboard shortcuts, enabling quick documentation generation for specific components.

**Workspace Analysis**: Execute full workspace analysis to generate comprehensive product documentation covering the entire codebase.

**Provider Switching**: Toggle between OpenAI and Claude AI providers through command palette actions without navigating settings.

**Copy to Clipboard**: Copy generated insights, documentation, or test code to the clipboard for use in external tools, documentation systems, or communication platforms.

### API Configuration

Developers configure Shadow Watch through Visual Studio Code's settings interface:

**AI Provider Settings**: Configure API keys for OpenAI and Anthropic Claude, select preferred language models, and adjust temperature and token limit parameters.

**Analysis Behavior**: Enable or disable automatic analysis on file save, choose which file patterns to analyze, and set diagnostic severity levels.

**Output Formatting**: Select output format (Cursor, ChatGPT, Generic, or Compact) to optimize AI-generated content for specific workflows or assistant tools.

**Display Options**: Control whether inline hints appear in the editor, adjust diagnostic filtering, and customize tree view display preferences.

**Testing Configuration**: Specify test framework preferences, configure test execution timeouts, and set test output directories.

### CI/CD Integration

Shadow Watch outputs integrate seamlessly with continuous integration and deployment pipelines:

**Documentation Artifacts**: Export markdown documentation files to documentation repositories, commit them to version control, and publish them alongside code changes.

**Test Suite Integration**: Generated unit tests follow project conventions and framework configurations, enabling immediate integration into existing CI/CD test suites without modification.

**Architecture Reports**: Produce architecture insights and code quality assessments that can be incorporated into automated code review processes or quality gates.

**Historical Tracking**: Maintain versioned documentation in the `.shadow` directory with timestamps and unique identifiers for tracking documentation evolution alongside code changes.

## Problems Solved

### Documentation Challenges
- **Eliminates manual documentation writing**: Automatically generates comprehensive product and code documentation, saving hours of manual effort and ensuring consistency across large codebases.
- **Prevents outdated documentation**: Continuously updates analysis when code changes, maintaining synchronization between documentation and implementation without manual intervention.
- **Maintains consistent quality**: Applies standardized AI analysis across all documentation, eliminating variations in style, detail level, and completeness that occur with manual documentation.
- **Simplifies onboarding**: Generates clear explanations of what code does from a user perspective, accelerating new team member understanding of complex systems.

### Code Understanding
- **Reduces time understanding unfamiliar code**: Provides AI-generated summaries and architectural insights that quickly explain code structure, purpose, and relationships without reading every line.
- **Accelerates code reviews**: Offers immediate insights into code structure, potential issues, and quality concerns, enabling reviewers to focus on critical aspects rather than basic comprehension.
- **Identifies technical debt**: Automatically detects circular dependencies, orphaned files, code smells, and other architectural issues that signal technical debt requiring attention.
- **Explains architectural patterns**: Recognizes and documents design patterns, structural organization, and system architecture to help developers understand high-level design decisions.

### Testing Efficiency
- **Speeds up test creation**: Automatically generates unit tests with proper setup, mocks, and assertions, eliminating the time-consuming task of writing boilerplate test code.
- **Reduces debugging time**: Automatically fixes failing tests through iterative AI-powered corrections that analyze error messages and regenerate test code until tests pass.
- **Ensures test coverage**: Creates prioritized test plans that identify which functions should be tested based on complexity and importance, ensuring critical code receives appropriate test coverage.
- **Handles test configuration**: Automatically detects project test framework configuration and identifies missing dependencies, eliminating manual setup challenges.

### Development Workflow
- **Improves code quality**: Identifies refactoring opportunities, code organization issues, and potential improvements proactively during development rather than during late-stage reviews.
- **Enables seamless navigation**: Provides clickable navigation from analysis insights directly to source code locations, reducing context switching and manual file searches.
- **Maintains analysis currency**: Monitors file changes and automatically triggers re-analysis when files are saved, ensuring insights reflect the current state of the codebase.
- **Supports multiple workflows**: Offers flexible AI provider selection and output formats, adapting to different development workflows and AI assistant preferences.

## Architecture Overview

Shadow Watch implements a layered architecture that separates concerns across infrastructure, domain logic, and user interface components:

### Layered Architecture

**Infrastructure Layer**: Manages external integrations and low-level services including AI provider communication (OpenAI and Anthropic Claude), file system operations with caching for performance optimization, and persistent storage of analysis results. This layer abstracts provider-specific implementations behind unified interfaces, enabling seamless switching between AI models.

**Domain Layer**: Contains core business logic organized into specialized services that orchestrate complex workflows. Analysis services perform code scanning, AST parsing, and metadata extraction. Testing services handle test plan generation, test code creation, validation, and automatic fixing. Documentation services coordinate AI interactions for generating product, module, and file-level documentation. Prompt builders standardize all AI interactions by constructing consistent, structured prompts.

**User Interface Layer**: Provides Visual Studio Code integration through multiple interfaces including tree view providers for browsing analysis results, diagnostic providers for displaying inline code issues, webview panels for detailed reports, status bar indicators for progress tracking, and command handlers for user actions. This layer translates user interactions into domain service invocations and renders results in appropriate UI components.

### Key Architectural Patterns

**Request-Response Pattern**: All AI interactions follow a structured request-response pattern where analysis operations construct detailed prompts with code context, send requests to language models through the provider abstraction layer, and parse structured responses into typed objects representing documentation, insights, or test plans.

**Repository Pattern**: Analysis results are persisted through repository interfaces that abstract storage implementation details. The system maintains both in-memory caches for performance and on-disk storage in the `.shadow` directory for persistence across sessions.

**Observer Pattern**: Configuration changes trigger notifications to all registered observers, ensuring that components immediately reflect new settings such as AI provider switches, output format changes, or diagnostic severity adjustments.

**Factory Pattern**: Provider factories instantiate appropriate AI provider implementations based on configuration, enabling runtime selection between OpenAI and Claude without code changes.

**Incremental Processing**: Analysis operates incrementally by making initial AI requests, processing responses that may include additional information requests (file reads or grep searches), fetching requested data, and continuing analysis until completion. This enables thorough analysis without overwhelming context limits.

### Data Flow

1. **User Action**: User triggers analysis through command palette, file save, or UI button click
2. **Command Handler**: Receives action and delegates to appropriate domain service
3. **Analysis Service**: Coordinates workflow including code scanning, prompt construction, and AI requests
4. **AI Provider**: Processes structured prompts and returns text or JSON responses
5. **Parser**: Extracts typed objects from AI responses and validates structure
6. **Repository**: Persists results to disk with timestamps and unique identifiers
7. **UI Update**: Refreshes tree views, diagnostics, and status indicators to reflect new analysis
8. **Cache Update**: Updates in-memory caches for fast subsequent access

### State Management

State is managed through centralized managers that track:
- Analysis progress including current phase, completion percentage, and cancellation state
- Configuration settings with automatic synchronization across all components
- Test generation status tracking which functions have been tested and validation results
- File watching state monitoring which files have changed and require re-analysis

### Reliability Mechanisms

**Rate Limiting**: Tracks API request frequency and enforces configurable limits to prevent quota exhaustion, queuing requests when limits are approached.

**Retry Logic**: Automatically retries failed AI requests with exponential backoff for transient failures such as rate limits, network timeouts, or temporary service unavailability.

**Caching**: Implements multi-level caching including in-memory file content caches with automatic invalidation on changes and on-disk analysis result caches for cross-session persistence.

**Error Handling**: Provides comprehensive error handling with user-friendly messages, detailed logging for debugging, and graceful degradation when AI services are unavailable.

## Key Components & Modules

### Extension Core (src/)

**Extension Entry Point (`extension.ts`)**: Initializes all extension components during VS Code activation, registers commands with their handlers, sets up UI components including tree views and diagnostics providers, establishes file monitoring for automatic re-analysis, and coordinates between AI services, analyzers, and user interface elements. This orchestrator serves as the central hub connecting all extension capabilities.

**LLM Service (`llmService.ts`)**: Coordinates AI-powered code analysis and documentation generation by managing interactions with AI providers. Constructs prompts for different analysis types, sends requests through the provider abstraction layer, parses responses into structured data, and handles errors gracefully. This service acts as the primary interface between extension features and AI capabilities.

**LLM Integration (`llmIntegration.ts`)**: Manages state, commands, and UI updates for all AI-powered features. Tracks analysis progress across long-running operations, updates status bar indicators and progress notifications, handles command execution for user-triggered actions, and coordinates UI refreshes when analysis completes. This component bridges AI functionality with VS Code's user interface.

**Insights Tree View (`insightsTreeView.ts`)**: Provides the hierarchical sidebar view displaying analysis results organized by modules, files, and functions. Enables browsing code structure with expandable/collapsible nodes, supports click-through navigation to source code locations, displays statistics and metadata for each code element, and updates automatically when analysis results change.

**Analyzer (`analyzer.ts`)**: Defines core data structures and interfaces for code analysis throughout the application including interfaces for product documentation, module summaries, file summaries, architecture insights, function metadata, codebase statistics, and diagnostic information. This module establishes type contracts ensuring consistent data handling.

### Analysis Engine (src/analysis/)

**Enhanced Analyzer (`enhancedAnalyzer.ts`)**: Performs deep static analysis of TypeScript and JavaScript files using Abstract Syntax Tree (AST) parsing. Extracts comprehensive function metadata including signatures, parameters, return types, complexity metrics, and dependency chains. Identifies behavioral patterns such as queries, mutations, validations, and side effects. Analyzes branch coverage showing different execution paths. This analyzer provides the foundational code understanding powering all analysis features.

**Function Analyzer (`functionAnalyzer.ts`)**: Focuses on individual function analysis to extract detailed metadata. Determines function signatures with precise type information, identifies all dependencies and relationships with other functions, calculates complexity metrics based on branches and nesting, detects state mutations and potential side effects, and generates refactoring recommendations when functions exceed complexity thresholds.

### AI Infrastructure (src/ai/)

**Rate Limiter (`llmRateLimiter.ts`)**: Prevents API quota exhaustion by tracking request frequency across different time windows. Enforces configurable rate limits per provider, queues requests when approaching limits, and provides feedback about remaining capacity. This ensures reliable AI service availability without hitting provider quotas.

**Retry Handler (`llmRetryHandler.ts`)**: Automatically retries failed AI requests using exponential backoff strategies. Handles transient failures including rate limit errors, network timeouts, and temporary service unavailability. Distinguishes between retryable and permanent errors, limiting retry attempts to prevent infinite loops while maximizing success rates.

**Response Parser (`llmResponseParser.ts`)**: Extracts structured data from AI text responses and converts them into typed objects. Parses file summaries, module summaries, product documentation, architecture insights, and test plans from natural language responses. Validates JSON structures and handles malformed responses gracefully, ensuring reliable data extraction from AI-generated content.

### AI Providers (src/ai/providers/)

**Provider Factory (`providerFactory.ts`)**: Creates AI provider instances based on configuration settings. Automatically selects and initializes the appropriate provider (OpenAI or Claude) based on available API keys and user preferences. Implements factory pattern enabling runtime provider switching without code modifications.

**OpenAI Provider (`openAIProvider.ts`)**: Implements communication with OpenAI's GPT models using the OpenAI API. Handles authentication with API keys, formats messages according to OpenAI's conversation format, supports both streaming and complete responses, and manages model-specific parameters like temperature and token limits.

**Claude Provider (`claudeProvider.ts`)**: Implements communication with Anthropic's Claude models using the Anthropic API. Translates requests into Claude's expected format, manages API authentication and versioning, handles Claude-specific features and limitations, and provides consistent interface matching OpenAI provider for seamless switching.

**Provider Interface (`ILLMProvider.ts`)**: Defines the contract that all AI providers must implement, ensuring consistent behavior across different AI services. Specifies methods for sending messages, requesting structured JSON responses, streaming responses, and managing conversation history. This abstraction enables provider-agnostic code throughout the extension.

### Domain Services (src/domain/services/)

**Incremental Analysis Service (`incrementalAnalysisService.ts`)**: Orchestrates multi-round AI analysis that progressively gathers code context. Makes initial analysis requests, processes AI responses that may request additional information, fetches requested files or performs code searches, continues analysis with new context until completion, and prevents infinite loops through iteration limits and safeguards.

**File Watcher Service (`fileWatcherService.ts`)**: Monitors workspace for file system changes and triggers appropriate responses. Detects file creation, modification, and deletion events, filters changes to ignore irrelevant files (node_modules, build artifacts), triggers automatic re-analysis when source files are saved, and notifies UI components to refresh their displays reflecting current file state.

**Test Configuration Service (`testConfigurationService.ts`)**: Automatically detects and validates test environment configuration. Identifies project test framework (Jest, Mocha, Pytest, JUnit) by analyzing configuration files, detects missing test dependencies by checking package.json or requirements.txt, validates test directory structure and naming conventions, and provides recommendations for setup when configuration is incomplete.

### Testing Services (src/domain/services/testing/)

**Test Planning Service (`llmTestPlanningService.ts`)**: Analyzes codebases to create prioritized test plans determining which functions should be tested and in what order. Evaluates function complexity using metrics like branches and dependencies, assesses function importance based on usage and position in architecture, groups related functions for coherent test organization, and generates structured test plans with priorities and metadata.

**Test Generation Service (`llmTestGenerationService.ts`)**: Generates complete unit test code using AI analysis of function implementations. Creates tests with appropriate framework setup (describe blocks, test suites), generates proper mocks for external dependencies, writes assertions validating expected behavior, includes edge cases and error scenarios, and formats code according to project conventions.

**Test Validation Service (`llmTestValidationService.ts`)**: Executes generated tests and automatically fixes failures through iterative AI corrections. Runs tests using the project's test framework, captures detailed results including error messages and stack traces, analyzes failure reasons using AI, regenerates failing tests with corrections, and iterates until tests pass or maximum attempts are reached.

**Test Execution Service (`testExecutionService.ts`)**: Runs test suites using appropriate test frameworks and captures structured results. Detects and configures correct test runner (npm test, pytest, mvn test), executes tests in isolated environments, parses test output into structured formats, extracts pass/fail counts, execution duration, and coverage metrics, and provides detailed error information for debugging.

### Configuration & Context (src/config/, src/context/)

**Configuration Manager (`configurationManager.ts`)**: Centralizes management of all extension settings providing unified access to user preferences. Reads settings from VS Code configuration, implements reactive architecture notifying components of changes, manages AI provider selection and model configuration, controls analysis behavior and UI display options, and ensures consistent settings access throughout the extension.

**Analysis Context Manager (`analysisContextManager.ts`)**: Manages persistence and formatting of code analysis results optimized for AI consumption. Automatically captures and saves analysis results to `.shadow/docs/code-analysis.json`, transforms raw analysis into LLM-optimized structured format, maintains context across VS Code sessions enabling reuse without re-analysis, and provides fast access to previously analyzed code for AI features.

### Formatters & Prompts (src/domain/formatters/, src/domain/prompts/)

**Documentation Formatter (`documentationFormatter.ts`)**: Converts raw product data and AI insights into polished, human-readable Markdown documents. Formats product overviews with comprehensive feature descriptions, organizes information by interface type (GUI, CLI, API, CI/CD), presents technical architecture details with clear structure, displays quality assessments and improvement suggestions, and generates timestamps and metadata for version tracking.

**Prompt Builder (`promptBuilder.ts`)**: Constructs standardized prompts for all AI interactions ensuring consistent formatting and predictable outputs. Builds prompts for product documentation generation from codebase summaries, creates architecture analysis prompts with configurable depth, generates test planning prompts aligned with project conventions, and standardizes prompt structure for reliable AI response parsing.

**Refactoring Prompt Builder (`refactoringPromptBuilder.ts`)**: Creates detailed prompts for AI-based refactoring analysis. Generates function extraction recommendations with dependency analysis, provides step-by-step migration instructions with code examples, analyzes caller-callee relationships revealing impact, and formats before-and-after code samples for clarity.

### Infrastructure (src/infrastructure/)

**Progress Service (`progressService.ts`)**: Provides centralized progress notification management for long-running operations. Displays progress indicators with titles and messages, updates incrementally showing completion percentages, supports operation cancellation when applicable, presents notifications in appropriate UI locations (notification area, status bar), and ensures consistent progress reporting across all features.

**Analysis Result Repository (`analysisResultRepository.ts`)**: Persists analysis outputs to the local filesystem in organized structure. Saves product documentation to `.shadow/docs` with timestamps, stores architecture insights in `.shadow/architecture`, organizes results by analysis run with unique identifiers, maintains file-level documentation with complete metadata, and creates summary files for each analysis session.

**File System Services (`fileSystem/`)**: Optimizes file access and processing with intelligent caching and filtering. Implements in-memory file content cache with automatic invalidation on changes, filters files to skip non-source directories (node_modules, .git, dist), processes multiple files in parallel for performance, and standardizes file operations with consistent error handling.

### Bootstrap & Handlers (src/domain/bootstrap/, src/domain/handlers/)

**Bootstrap Service (`bootstrapService.ts`)**: Initializes all extension components during VS Code activation coordinating setup of the entire system. Registers commands mapping user actions to handlers, configures UI components including tree views and diagnostics, establishes automatic file watching and re-analysis triggers, and ensures cohesive operation of all extension features.

**Command Registration (`commandRegistration.ts`)**: Maps user actions to their execution handlers making features accessible through command palette, context menus, and keyboard shortcuts. Registers commands for workspace and file analysis, insight copying and navigation, AI provider switching, test generation and validation, and documentation export.

**Navigation Handler (`navigationHandler.ts`)**: Manages all editor interactions triggered by user selections in analysis views. Opens files in VS Code editor when users click code items, positions cursor at exact line and column locations, displays relevant code context in viewport, handles navigation errors with clear user feedback, and supports navigation to files, functions, API endpoints, and other code elements.

## Key Functions & Data Structures

### Core Analysis Functions

**`activate(context: ExtensionContext)`** (src/extension.ts)
- **Purpose**: Main entry point initializing the VS Code extension
- **Behavior**: Registers all commands, sets up UI components (tree views, diagnostics, status bar), initializes file monitoring for automatic re-analysis, configures AI providers based on settings, and establishes event listeners for file changes and configuration updates
- **Returns**: Void (activates extension side effects)

**`analyzeCodebase(context: ExtensionContext)`** (src/extension.ts)
- **Purpose**: Analyzes entire workspace to generate comprehensive product documentation
- **Behavior**: Scans all source files in workspace, extracts file summaries and module capabilities, generates product-level documentation explaining what the application does, creates architecture insights identifying patterns and issues, saves results to `.shadow` directory with timestamps
- **Returns**: Promise resolving to product documentation object

**`analyzeFile(uri: Uri)`** (src/extension.ts)
- **Purpose**: Analyzes single file to generate focused documentation
- **Behavior**: Reads specified file content, performs AST-based analysis to extract function metadata, generates file-level documentation describing purpose and key functions, updates tree view and diagnostics with new analysis, caches results for fast subsequent access
- **Returns**: Promise resolving to file summary object

**`generateArchitectureInsights(codebase: CodebaseInfo)`** (src/llmService.ts)
- **Purpose**: Creates AI-generated insights about codebase architecture and quality
- **Behavior**: Constructs detailed prompt with codebase structure and statistics, sends request to configured AI provider, parses response extracting patterns, dependencies, and issues, identifies circular dependencies, orphaned files, and code smells, formats insights with recommendations for improvements
- **Returns**: Promise resolving to architecture insight object

**`enhancedAnalyze(filePath: string, fileContent: string)`** (src/analysis/enhancedAnalyzer.ts)
- **Purpose**: Performs deep AST-based analysis of TypeScript/JavaScript files
- **Behavior**: Parses file content into Abstract Syntax Tree, traverses AST extracting function declarations and expressions, analyzes each function for metadata (signature, parameters, return type), identifies dependencies and relationships between functions, calculates complexity metrics based on branches and nesting, detects behavioral patterns (queries, mutations, side effects)
- **Returns**: Promise resolving to array of function metadata objects

**`analyzeFunction(functionNode: Node, sourceFile: SourceFile)`** (src/analysis/functionAnalyzer.ts)
- **Purpose**: Extracts detailed metadata about individual function
- **Behavior**: Parses function signature with precise type information, identifies all called functions and imported dependencies, calculates cyclomatic complexity from conditional branches, detects state mutations and side effect operations, generates refactoring recommendations when complexity exceeds thresholds
- **Returns**: Function metadata object with comprehensive details

### Testing Functions

**`generateTestPlan(projectPath: string, functions: FunctionMetadata[])`** (src/domain/services/testing/llmTestPlanningService.ts)
- **Purpose**: Creates prioritized test plan determining which functions to test
- **Behavior**: Analyzes function complexity using metrics like branches and dependencies, assesses importance based on function usage and architectural position, groups related functions for coherent test organization, prioritizes critical and complex functions first, generates structured test plan with priorities and metadata
- **Returns**: Promise resolving to test plan object

**`generateTests(testPlan: TestPlan, onProgress: ProgressCallback)`** (src/domain/services/testing/llmTestGenerationService.ts)
- **Purpose**: Generates unit test code for functions in batches
- **Behavior**: Processes functions from test plan in priority order, constructs AI prompts with function implementation and context, generates test code with framework-appropriate setup (describe blocks), creates mocks for external dependencies with proper stubs, writes assertions validating expected behavior and edge cases, reports progress for each function being tested
- **Returns**: Promise resolving to test generation result object

**`validateAndFixTests(testFile: string, maxAttempts: number)`** (src/domain/services/testing/llmTestValidationService.ts)
- **Purpose**: Executes tests and automatically fixes failures through AI corrections
- **Behavior**: Runs generated tests using project test framework, captures detailed execution results including errors and stack traces, analyzes failure reasons using AI prompt with error context, regenerates failing tests with corrections addressing root causes, iterates validation and fixing until tests pass or max attempts reached
- **Returns**: Promise resolving to validation result with final test status

**`executeTests(testCommand: string, workingDirectory: string)`** (src/domain/services/testing/testExecutionService.ts)
- **Purpose**: Runs test suites and captures structured results
- **Behavior**: Executes test command in shell (npm test, pytest, etc.), captures stdout/stderr output from test runner, parses output extracting pass/fail counts and execution duration, identifies individual test failures with error messages and stack traces, calculates coverage metrics when available
- **Returns**: Promise resolving to test execution result object

**`detectTestSetup(projectPath: string)`** (src/domain/services/testConfigurationService.ts)
- **Purpose**: Automatically identifies project test framework configuration
- **Behavior**: Searches for test configuration files (jest.config.js, .mocharc.json, pytest.ini), parses package.json or requirements.txt for test dependencies, validates test directory structure and naming conventions, identifies missing dependencies or configuration issues, provides recommendations for completing test setup
- **Returns**: Promise resolving to test setup configuration object

### Navigation & UI Functions

**`navigateToCodeItem(item: CodeItem)`** (src/domain/handlers/navigationHandler.ts)
- **Purpose**: Opens file in editor and positions cursor at specific location
- **Behavior**: Resolves file path from code item reference, opens document in VS Code editor, positions cursor at exact line and column, scrolls viewport to show relevant code context, handles errors when files cannot be found with user-friendly messages
- **Returns**: Promise resolving when navigation completes

**`performIncrementalAnalysis(initialPrompt: string, maxRounds: number)`** (src/domain/services/incrementalAnalysisService.ts)
- **Purpose**: Executes iterative AI analysis progressively gathering code context
- **Behavior**: Sends initial prompt to AI provider, processes response checking for file or grep requests, fetches requested information (file contents or search results), constructs follow-up prompt with newly gathered context, continues iterations until AI indicates completion or max rounds reached, implements safeguards preventing infinite loops
- **Returns**: Promise resolving to final analysis result

**`watchWorkspace(workspaceRoot: string, onChange: ChangeCallback)`** (src/domain/services/fileWatcherService.ts)
- **Purpose**: Monitors workspace for file changes triggering automatic responses
- **Behavior**: Establishes file system watchers on workspace directory, filters changes to ignore non-source files and directories, detects create, modify, and delete events, triggers callback with change details for source file modifications, automatically re-analyzes changed files when auto-analysis enabled
- **Returns**: Disposable object for stopping file watching

### AI Integration Functions

**`buildProductDocumentationPrompt(filesSummary: FileSummary[], stats: CodebaseStats)`** (src/domain/prompts/promptBuilder.ts)
- **Purpose**: Constructs AI prompt for generating product documentation
- **Behavior**: Combines file summaries into cohesive codebase overview, includes statistics showing project size and complexity, specifies desired documentation structure (overview, features, architecture), provides context about user perspectives (GUI, CLI, API, CI/CD), formats prompt with clear instructions for AI model
- **Returns**: String containing formatted prompt

**`buildRefactoringPrompt(functionMetadata: FunctionMetadata[], largeFiles: FileInfo[])`** (src/domain/prompts/refactoringPromptBuilder.ts)
- **Purpose**: Creates detailed prompt for AI-based refactoring analysis
- **Behavior**: Includes function implementations and complexity metrics, identifies large or complex files requiring refactoring, specifies desired output format (function extraction plans with dependencies), requests step-by-step migration instructions with code examples, provides context about project structure and conventions
- **Returns**: String containing formatted refactoring prompt

**`rateLimitedRequest<T>(provider: ILLMProvider, request: () => Promise<T>)`** (src/ai/llmRateLimiter.ts)
- **Purpose**: Enforces rate limits on AI API requests preventing quota exhaustion
- **Behavior**: Tracks request frequency across sliding time windows, checks if new request would exceed configured rate limit, queues request if limit approached or delays execution, executes request when capacity available, updates request tracking after completion
- **Returns**: Promise resolving to request result after rate limiting

**`retryWithBackoff<T>(operation: () => Promise<T>, maxRetries: number)`** (src/ai/llmRetryHandler.ts)
- **Purpose**: Automatically retries failed AI requests with exponential backoff
- **Behavior**: Attempts operation execution catching any errors, analyzes error to determine if retryable (rate limit, network timeout), waits increasing duration between retries (exponential backoff), re-attempts operation up to maximum retry count, throws final error if all retries exhausted
- **Returns**: Promise resolving to successful operation result

**`parseFileSummary(response: string)`** (src/ai/llmResponseParser.ts)
- **Purpose**: Extracts structured file documentation from AI text response
- **Behavior**: Searches response for JSON or structured text containing file summary, parses purpose, key functions, and role information, validates required fields are present, handles malformed responses gracefully with defaults, converts parsed data into typed FileSummary object
- **Returns**: FileSummary object or null if parsing fails

### Formatting Functions

**`formatProductDocumentation(doc: ProductDocumentation)`** (src/domain/formatters/documentationFormatter.ts)
- **Purpose**: Converts product documentation data into formatted Markdown
- **Behavior**: Creates document header with product name and timestamp, formats overview section with comprehensive description, organizes features by interface type with clear headings, presents architecture details with structure and patterns, includes quality assessments and improvement suggestions, generates table of contents for navigation
- **Returns**: String containing formatted Markdown document

### Key Data Structures

**ProductDocumentation** (src/fileDocumentation.ts)
```typescript
interface ProductDocumentation {
  name: string;                    // Product or application name
  type: string;                    // Product category or type
  description: string;             //