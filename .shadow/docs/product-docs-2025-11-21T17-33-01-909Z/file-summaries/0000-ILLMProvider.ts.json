{
  "file": "src/ai/providers/ILLMProvider.ts",
  "role": "Core Logic",
  "purpose": "Defines the standard interface for all LLM (Large Language Model) provider implementations to ensure consistent AI integration across different providers like OpenAI and Claude.",
  "userVisibleActions": [
    "User receives AI-generated text responses to their queries",
    "User receives structured JSON data from AI models when requesting formatted output",
    "User can work with different AI providers (OpenAI, Claude, custom) transparently without changing their workflow",
    "User gets file and grep search requests suggested by the AI based on analysis needs"
  ],
  "developerVisibleActions": [
    "Developer implements this interface to add support for new LLM providers",
    "Developer checks if an AI provider is configured and ready using isConfigured()",
    "Developer sends text-based requests to AI models with customizable parameters (temperature, max tokens, model selection)",
    "Developer requests structured JSON output from AI models with optional schema validation",
    "Developer receives additional context requests (file reads, grep searches) from AI responses",
    "Developer accesses raw provider responses for debugging or advanced processing",
    "Developer identifies which provider is being used via getName()"
  ],
  "keyFunctions": [
    {
      "name": "isConfigured",
      "desc": "Verifies if the LLM provider has valid credentials and is ready to process requests",
      "inputs": "none",
      "outputs": "boolean indicating configuration status"
    },
    {
      "name": "sendRequest",
      "desc": "Sends a prompt to the LLM and retrieves a text response with optional configuration like temperature and token limits",
      "inputs": "LLMRequestOptions (messages, model, temperature, maxTokens, systemPrompt, responseFormat)",
      "outputs": "Promise<LLMResponse> containing generated text content, finish reason, model used, and raw response"
    },
    {
      "name": "sendStructuredRequest",
      "desc": "Sends a prompt expecting structured JSON output, optionally validated against a schema, with additional file/grep requests",
      "inputs": "LLMRequestOptions and optional schema for validation",
      "outputs": "Promise<StructuredOutputResponse<T>> containing parsed data and optional file/grep requests"
    },
    {
      "name": "getName",
      "desc": "Returns the identifier of the LLM provider for logging and user display",
      "inputs": "none",
      "outputs": "string with provider name"
    }
  ],
  "dependencies": [],
  "intent": "This interface exists to abstract away differences between various LLM providers (OpenAI, Claude, custom implementations), allowing the codebase to work with any AI provider through a unified API. It solves the problem of vendor lock-in and enables easy switching or addition of new AI providers without changing consuming code. It also standardizes how structured outputs and additional context requests are handled across providers.",
  "rawContent": "```json\n{\n  \"purpose\": \"Defines the standard interface for all LLM (Large Language Model) provider implementations to ensure consistent AI integration across different providers like OpenAI and Claude.\",\n  \"userVisibleActions\": [\n    \"User receives AI-generated text responses to their queries\",\n    \"User receives structured JSON data from AI models when requesting formatted output\",\n    \"User can work with different AI providers (OpenAI, Claude, custom) transparently without changing their workflow\",\n    \"User gets file and grep search requests suggested by the AI based on analysis needs\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer implements this interface to add support for new LLM providers\",\n    \"Developer checks if an AI provider is configured and ready using isConfigured()\",\n    \"Developer sends text-based requests to AI models with customizable parameters (temperature, max tokens, model selection)\",\n    \"Developer requests structured JSON output from AI models with optional schema validation\",\n    \"Developer receives additional context requests (file reads, grep searches) from AI responses\",\n    \"Developer accesses raw provider responses for debugging or advanced processing\",\n    \"Developer identifies which provider is being used via getName()\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Verifies if the LLM provider has valid credentials and is ready to process requests\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating configuration status\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a prompt to the LLM and retrieves a text response with optional configuration like temperature and token limits\",\n      \"inputs\": \"LLMRequestOptions (messages, model, temperature, maxTokens, systemPrompt, responseFormat)\",\n      \"outputs\": \"Promise<LLMResponse> containing generated text content, finish reason, model used, and raw response\"\n    },\n    {\n      \"name\": \"sendStructuredRequest\",\n      \"desc\": \"Sends a prompt expecting structured JSON output, optionally validated against a schema, with additional file/grep requests\",\n      \"inputs\": \"LLMRequestOptions and optional schema for validation\",\n      \"outputs\": \"Promise<StructuredOutputResponse<T>> containing parsed data and optional file/grep requests\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the identifier of the LLM provider for logging and user display\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string with provider name\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"This interface exists to abstract away differences between various LLM providers (OpenAI, Claude, custom implementations), allowing the codebase to work with any AI provider through a unified API. It solves the problem of vendor lock-in and enables easy switching or addition of new AI providers without changing consuming code. It also standardizes how structured outputs and additional context requests are handled across providers.\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T17:34:20.051Z"
  }
}