{
  "module": "src/domain/services/testing",
  "moduleType": "tests",
  "capabilities": [
    "Automated discovery and analysis of testable functions across the workspace",
    "AI-powered test generation with automatic execution and validation",
    "Intelligent test planning and prioritization based on code complexity",
    "Automatic test environment setup and configuration",
    "Self-healing test validation with automatic failure detection and fixing",
    "Comprehensive test execution with detailed pass/fail reporting"
  ],
  "summary": "The testing services module provides an end-to-end AI-powered testing workflow that automates the entire process from test environment setup to test generation, execution, and validation. Users can leverage LLM-based analysis to discover testable functions in their codebase, assess their complexity and testability characteristics, and receive prioritized recommendations for which functions need testing coverage.\n\nThe module handles the complete testing lifecycle automatically: it detects the programming language and testing framework, sets up the necessary test infrastructure including directories and dependencies, generates unit tests in batches for selected functions, executes them immediately, and validates the results. When tests fail, the system automatically attempts to fix them using AI assistance, providing progress feedback throughout each stage.\n\nUsers benefit from intelligent analysis that identifies functions requiring mocks, rates complexity levels, and avoids false positives in testability assessments. The workflow provides continuous feedback showing batch processing status, test generation progress, execution results with pass/fail counts, and detailed error information when issues occur. This comprehensive automation reduces manual testing effort while improving test coverage quality.",
  "files": [
    {
      "file": "src/domain/services/testing/llmFunctionExtractionService.ts",
      "role": "Core Logic",
      "purpose": "Extracts testable functions from workspace code files using an LLM to identify function metadata, complexity, and testability characteristics.",
      "userVisibleActions": [
        "Automated analysis of code files to identify functions that can be tested",
        "Assessment of function testability (low/medium/high) with explanations",
        "Detection of functions that require mocking due to external dependencies",
        "Complexity rating (low/medium/high) assigned to each discovered function",
        "Progress feedback showing batch processing status during analysis"
      ],
      "developerVisibleActions": [
        "Call extractFunctionsFromWorkspace() to analyze multiple code files in batches",
        "Call extractFunctionsFromSingleFile() to analyze a single file in isolation",
        "Receive structured function metadata including name, location, parameters, return types, and dependencies",
        "Configure batch size (maxFilesPerBatch) to control token usage and API limits",
        "Get detailed extraction results with total function count and files analyzed",
        "Access logging output showing processing progress and any extraction errors"
      ],
      "keyFunctions": [
        {
          "name": "extractFunctionsFromWorkspace",
          "desc": "Analyzes all code files in workspace to extract function metadata using LLM in configurable batches",
          "inputs": "workspaceRoot path, array of code file paths, llmService instance, optional maxFilesPerBatch",
          "outputs": "Array of ExtractedFunction objects with metadata about each discovered function"
        },
        {
          "name": "extractFunctionsFromSingleFile",
          "desc": "Analyzes a single code file to extract all functions and their metadata using LLM",
          "inputs": "File path, file content, llmService instance",
          "outputs": "Array of ExtractedFunction objects from the analyzed file"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "CodeAnalysis",
        "buildFunctionExtractionPrompt",
        "buildSingleFileExtractionPrompt",
        "SWLogger"
      ],
      "intent": "Replaces regex-based function extraction with LLM-powered analysis to accurately identify testable functions while avoiding false positives from control flow keywords (if, while, for, etc.). Enables intelligent test generation by providing rich metadata about each function including complexity, dependencies, API calls, and testability assessment.",
      "rawContent": "```json\n{\n  \"purpose\": \"Extracts testable functions from workspace code files using an LLM to identify function metadata, complexity, and testability characteristics.\",\n  \"userVisibleActions\": [\n    \"Automated analysis of code files to identify functions that can be tested\",\n    \"Assessment of function testability (low/medium/high) with explanations\",\n    \"Detection of functions that require mocking due to external dependencies\",\n    \"Complexity rating (low/medium/high) assigned to each discovered function\",\n    \"Progress feedback showing batch processing status during analysis\"\n  ],\n  \"developerVisibleActions\": [\n    \"Call extractFunctionsFromWorkspace() to analyze multiple code files in batches\",\n    \"Call extractFunctionsFromSingleFile() to analyze a single file in isolation\",\n    \"Receive structured function metadata including name, location, parameters, return types, and dependencies\",\n    \"Configure batch size (maxFilesPerBatch) to control token usage and API limits\",\n    \"Get detailed extraction results with total function count and files analyzed\",\n    \"Access logging output showing processing progress and any extraction errors\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"extractFunctionsFromWorkspace\",\n      \"desc\": \"Analyzes all code files in workspace to extract function metadata using LLM in configurable batches\",\n      \"inputs\": \"workspaceRoot path, array of code file paths, llmService instance, optional maxFilesPerBatch\",\n      \"outputs\": \"Array of ExtractedFunction objects with metadata about each discovered function\"\n    },\n    {\n      \"name\": \"extractFunctionsFromSingleFile\",\n      \"desc\": \"Analyzes a single code file to extract all functions and their metadata using LLM\",\n      \"inputs\": \"File path, file content, llmService instance\",\n      \"outputs\": \"Array of ExtractedFunction objects from the analyzed file\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"CodeAnalysis\",\n    \"buildFunctionExtractionPrompt\",\n    \"buildSingleFileExtractionPrompt\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"Replaces regex-based function extraction with LLM-powered analysis to accurately identify testable functions while avoiding false positives from control flow keywords (if, while, for, etc.). Enables intelligent test generation by providing rich metadata about each function including complexity, dependencies, API calls, and testability assessment.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestGenerationService.ts",
      "role": "Core Logic",
      "purpose": "Generates unit tests for code functions in small batches using an LLM service, executing them and tracking generation state",
      "userVisibleActions": [
        "Tests are generated automatically for selected functions in the workspace",
        "Progress updates show which function is being processed and how many are complete",
        "Generated tests are saved to the test directory and can be executed immediately",
        "Test execution results show pass/fail status and coverage information",
        "Failed test generation attempts are reported with error messages"
      ],
      "developerVisibleActions": [
        "Developer triggers test generation for a batch of functions from their codebase",
        "Service reads the source code of each function to understand its behavior",
        "LLM generates test code based on the function's implementation and context",
        "Generated tests are written to the file system in the test directory",
        "Tests are automatically executed to verify they work correctly",
        "Service tracks which functions have tests generated, which passed, and which failed",
        "Developer receives a map of results showing success/failure for each function",
        "Existing mock files are detected and included in the generation context"
      ],
      "keyFunctions": [
        {
          "name": "generateTestBatch",
          "desc": "Generates tests for multiple functions in a batch, processing them sequentially",
          "inputs": "Array of TestableFunction objects, workspace root path, LLM service instance, optional progress callback",
          "outputs": "Map of function names to TestGenerationResult objects containing success/failure status"
        },
        {
          "name": "extractFunctionSource",
          "desc": "Reads and extracts the source code for a specific function from the file system",
          "inputs": "TestableFunction object, workspace root path",
          "outputs": "String containing the function's source code"
        },
        {
          "name": "extractFileContext",
          "desc": "Extracts import/export context from the source file to provide additional context for test generation",
          "inputs": "File path, workspace root path",
          "outputs": "String containing file context information"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestableFunction",
        "TestGenerationState",
        "TestGenerationResult",
        "buildGenerationPrompt",
        "TestExecutionService",
        "SWLogger"
      ],
      "intent": "This file exists to automate the tedious process of writing unit tests by leveraging LLM capabilities to generate tests incrementally in manageable batches, ensuring each function has proper test coverage without overwhelming the system or developer with too many tests at once.",
      "rawContent": "```json\n{\n  \"purpose\": \"Generates unit tests for code functions in small batches using an LLM service, executing them and tracking generation state\",\n  \"userVisibleActions\": [\n    \"Tests are generated automatically for selected functions in the workspace\",\n    \"Progress updates show which function is being processed and how many are complete\",\n    \"Generated tests are saved to the test directory and can be executed immediately\",\n    \"Test execution results show pass/fail status and coverage information\",\n    \"Failed test generation attempts are reported with error messages\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer triggers test generation for a batch of functions from their codebase\",\n    \"Service reads the source code of each function to understand its behavior\",\n    \"LLM generates test code based on the function's implementation and context\",\n    \"Generated tests are written to the file system in the test directory\",\n    \"Tests are automatically executed to verify they work correctly\",\n    \"Service tracks which functions have tests generated, which passed, and which failed\",\n    \"Developer receives a map of results showing success/failure for each function\",\n    \"Existing mock files are detected and included in the generation context\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"generateTestBatch\",\n      \"desc\": \"Generates tests for multiple functions in a batch, processing them sequentially\",\n      \"inputs\": \"Array of TestableFunction objects, workspace root path, LLM service instance, optional progress callback\",\n      \"outputs\": \"Map of function names to TestGenerationResult objects containing success/failure status\"\n    },\n    {\n      \"name\": \"extractFunctionSource\",\n      \"desc\": \"Reads and extracts the source code for a specific function from the file system\",\n      \"inputs\": \"TestableFunction object, workspace root path\",\n      \"outputs\": \"String containing the function's source code\"\n    },\n    {\n      \"name\": \"extractFileContext\",\n      \"desc\": \"Extracts import/export context from the source file to provide additional context for test generation\",\n      \"inputs\": \"File path, workspace root path\",\n      \"outputs\": \"String containing file context information\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestableFunction\",\n    \"TestGenerationState\",\n    \"TestGenerationResult\",\n    \"buildGenerationPrompt\",\n    \"TestExecutionService\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This file exists to automate the tedious process of writing unit tests by leveraging LLM capabilities to generate tests incrementally in manageable batches, ensuring each function has proper test coverage without overwhelming the system or developer with too many tests at once.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestPlanningService.ts",
      "role": "Core Logic",
      "purpose": "Creates prioritized test plans by analyzing code functions and determining which ones need testing using LLM-based analysis.",
      "userVisibleActions": [
        "Receives an AI-generated test plan that prioritizes which functions in the codebase need testing",
        "Gets test plans based on intelligent analysis of function complexity and importance",
        "Benefits from improved test coverage recommendations that avoid false positives from control flow keywords"
      ],
      "developerVisibleActions": [
        "Calls analyzeFunctions to extract testable functions from workspace code files using LLM analysis",
        "Receives structured test plan data with function metadata (name, file, lines, complexity, parameters, return type)",
        "Uses LLM service to intelligently identify actual functions versus control flow statements",
        "Accesses legacy regex-based function extraction as fallback (deprecated)",
        "Gets functions converted to test plan format ready for prioritization"
      ],
      "keyFunctions": [
        {
          "name": "analyzeFunctions",
          "desc": "Extracts and analyzes testable functions from workspace code files using LLM instead of regex to avoid capturing control flow keywords",
          "inputs": "workspaceRoot (string), codeFiles (string array), llmService (LLM service instance)",
          "outputs": "Array of function metadata objects in test plan format"
        },
        {
          "name": "analyzeFunctionsLegacy",
          "desc": "Deprecated regex-based function extraction that incorrectly captured control flow keywords like for, if, switch",
          "inputs": "codeAnalysis (code analysis object)",
          "outputs": "Array of function metadata objects"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "testPlanTypes",
        "testPrompts",
        "CodeAnalysis",
        "SWLogger",
        "LLMFunctionExtractionService"
      ],
      "intent": "This file solves the problem of accurately identifying which functions in a codebase should be tested by using LLM analysis to distinguish real functions from control flow statements (for, if, switch), replacing unreliable regex-based extraction that created false positives. It enables intelligent test planning by providing structured function metadata that can be prioritized for test generation.",
      "rawContent": "```json\n{\n  \"purpose\": \"Creates prioritized test plans by analyzing code functions and determining which ones need testing using LLM-based analysis.\",\n  \"userVisibleActions\": [\n    \"Receives an AI-generated test plan that prioritizes which functions in the codebase need testing\",\n    \"Gets test plans based on intelligent analysis of function complexity and importance\",\n    \"Benefits from improved test coverage recommendations that avoid false positives from control flow keywords\"\n  ],\n  \"developerVisibleActions\": [\n    \"Calls analyzeFunctions to extract testable functions from workspace code files using LLM analysis\",\n    \"Receives structured test plan data with function metadata (name, file, lines, complexity, parameters, return type)\",\n    \"Uses LLM service to intelligently identify actual functions versus control flow statements\",\n    \"Accesses legacy regex-based function extraction as fallback (deprecated)\",\n    \"Gets functions converted to test plan format ready for prioritization\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"analyzeFunctions\",\n      \"desc\": \"Extracts and analyzes testable functions from workspace code files using LLM instead of regex to avoid capturing control flow keywords\",\n      \"inputs\": \"workspaceRoot (string), codeFiles (string array), llmService (LLM service instance)\",\n      \"outputs\": \"Array of function metadata objects in test plan format\"\n    },\n    {\n      \"name\": \"analyzeFunctionsLegacy\",\n      \"desc\": \"Deprecated regex-based function extraction that incorrectly captured control flow keywords like for, if, switch\",\n      \"inputs\": \"codeAnalysis (code analysis object)\",\n      \"outputs\": \"Array of function metadata objects\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"testPlanTypes\",\n    \"testPrompts\",\n    \"CodeAnalysis\",\n    \"SWLogger\",\n    \"LLMFunctionExtractionService\"\n  ],\n  \"intent\": \"This file solves the problem of accurately identifying which functions in a codebase should be tested by using LLM analysis to distinguish real functions from control flow statements (for, if, switch), replacing unreliable regex-based extraction that created false positives. It enables intelligent test planning by providing structured function metadata that can be prioritized for test generation.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestSetupService.ts",
      "role": "Core Logic",
      "purpose": "Detects test environment configuration and generates automated test setup plans using LLM analysis.",
      "userVisibleActions": [
        "Automatically detects the programming language and testing framework in the workspace",
        "Identifies existing test infrastructure (package.json, jest config, test directories)",
        "Generates a test setup plan with configuration recommendations",
        "Creates missing test directories and configuration files",
        "Installs required testing dependencies automatically",
        "Provides feedback on setup progress and results"
      ],
      "developerVisibleActions": [
        "Scans workspace to identify TypeScript, JavaScript, Python, Java, or C++ projects",
        "Detects testing frameworks (Jest, Mocha, pytest, JUnit, Google Test)",
        "Analyzes package.json and configuration files to understand existing setup",
        "Generates LLM prompts to create customized test configurations",
        "Executes setup commands (npm install, directory creation, file generation)",
        "Returns structured setup results with success/failure status"
      ],
      "keyFunctions": [
        {
          "name": "detectTestEnvironment",
          "desc": "Analyzes workspace to identify programming language, testing framework, and existing configuration",
          "inputs": "workspaceRoot (string path)",
          "outputs": "TestEnvironment object with language, framework, and configuration details"
        },
        {
          "name": "getAllFiles",
          "desc": "Recursively scans directory to collect all files for language detection",
          "inputs": "dirPath (string), fileList (optional array)",
          "outputs": "Array of file paths"
        },
        {
          "name": "generateSetupPlan",
          "desc": "Creates an LLM-based test setup plan based on detected environment",
          "inputs": "environment (TestEnvironment), llmService",
          "outputs": "TestSetupPlan with configuration steps and recommendations"
        },
        {
          "name": "executeSetupPlan",
          "desc": "Runs the generated setup plan by creating files and installing dependencies",
          "inputs": "plan (TestSetupPlan), workspaceRoot (string)",
          "outputs": "SetupExecutionResult with success status and messages"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "child_process",
        "testSetupTypes",
        "testPrompts",
        "SWLogger"
      ],
      "intent": "This service exists to eliminate manual test environment setup by automatically detecting the project type and generating appropriate test configurations. It solves the problem of developers needing to manually configure testing frameworks, create test directories, and install dependencies by using LLM intelligence to understand the project structure and generate appropriate setup steps.",
      "rawContent": "```json\n{\n  \"purpose\": \"Detects test environment configuration and generates automated test setup plans using LLM analysis.\",\n  \"userVisibleActions\": [\n    \"Automatically detects the programming language and testing framework in the workspace\",\n    \"Identifies existing test infrastructure (package.json, jest config, test directories)\",\n    \"Generates a test setup plan with configuration recommendations\",\n    \"Creates missing test directories and configuration files\",\n    \"Installs required testing dependencies automatically\",\n    \"Provides feedback on setup progress and results\"\n  ],\n  \"developerVisibleActions\": [\n    \"Scans workspace to identify TypeScript, JavaScript, Python, Java, or C++ projects\",\n    \"Detects testing frameworks (Jest, Mocha, pytest, JUnit, Google Test)\",\n    \"Analyzes package.json and configuration files to understand existing setup\",\n    \"Generates LLM prompts to create customized test configurations\",\n    \"Executes setup commands (npm install, directory creation, file generation)\",\n    \"Returns structured setup results with success/failure status\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"detectTestEnvironment\",\n      \"desc\": \"Analyzes workspace to identify programming language, testing framework, and existing configuration\",\n      \"inputs\": \"workspaceRoot (string path)\",\n      \"outputs\": \"TestEnvironment object with language, framework, and configuration details\"\n    },\n    {\n      \"name\": \"getAllFiles\",\n      \"desc\": \"Recursively scans directory to collect all files for language detection\",\n      \"inputs\": \"dirPath (string), fileList (optional array)\",\n      \"outputs\": \"Array of file paths\"\n    },\n    {\n      \"name\": \"generateSetupPlan\",\n      \"desc\": \"Creates an LLM-based test setup plan based on detected environment\",\n      \"inputs\": \"environment (TestEnvironment), llmService\",\n      \"outputs\": \"TestSetupPlan with configuration steps and recommendations\"\n    },\n    {\n      \"name\": \"executeSetupPlan\",\n      \"desc\": \"Runs the generated setup plan by creating files and installing dependencies\",\n      \"inputs\": \"plan (TestSetupPlan), workspaceRoot (string)\",\n      \"outputs\": \"SetupExecutionResult with success status and messages\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"child_process\",\n    \"testSetupTypes\",\n    \"testPrompts\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This service exists to eliminate manual test environment setup by automatically detecting the project type and generating appropriate test configurations. It solves the problem of developers needing to manually configure testing frameworks, create test directories, and install dependencies by using LLM intelligence to understand the project structure and generate appropriate setup steps.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestValidationService.ts",
      "role": "Core Logic",
      "purpose": "Validates test files by running them, detecting failures, and automatically fixing broken tests using LLM assistance",
      "userVisibleActions": [
        "Tests are automatically executed and results are reported (passed/failed counts)",
        "Failing tests are identified and reported with error details",
        "Tests are automatically fixed when failures are detected",
        "Progress updates show which attempt is being made to fix a test (e.g., 'attempt 2/3')",
        "Final success or failure status is reported after fix attempts"
      ],
      "developerVisibleActions": [
        "Developer triggers test validation for a workspace or specific test file",
        "System runs Jest tests and captures execution results",
        "System reads failing test code and generates LLM prompts to fix issues",
        "System applies LLM-suggested fixes by updating test files",
        "System re-runs tests to verify fixes worked",
        "Developer receives structured test reports with pass/fail statistics",
        "System logs all validation and fix attempts for debugging"
      ],
      "keyFunctions": [
        {
          "name": "runTests",
          "desc": "Executes all tests or a specific test file and returns results with pass/fail statistics",
          "inputs": "workspaceRoot (string), optional testFile (string)",
          "outputs": "Array of TestExecutionResult objects with passed/failed/error counts"
        },
        {
          "name": "fixFailingTest",
          "desc": "Attempts to automatically fix a failing test by using LLM to analyze errors and suggest corrections",
          "inputs": "testFilePath, executionResult, workspaceRoot, llmService, maxAttempts (default 3)",
          "outputs": "Object with success status, number of attempts made, and optional final error message"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestExecutionService",
        "TestExecutionResult",
        "TestReport",
        "TestReportSummary",
        "buildFixPrompt",
        "SWLogger"
      ],
      "intent": "This file exists to automate the test validation workflow by running tests, detecting failures, and using AI to automatically fix broken tests instead of requiring manual developer intervention. It solves the problem of time-consuming manual test debugging and repair by leveraging LLM capabilities to understand test failures and generate fixes.",
      "rawContent": "```json\n{\n  \"purpose\": \"Validates test files by running them, detecting failures, and automatically fixing broken tests using LLM assistance\",\n  \"userVisibleActions\": [\n    \"Tests are automatically executed and results are reported (passed/failed counts)\",\n    \"Failing tests are identified and reported with error details\",\n    \"Tests are automatically fixed when failures are detected\",\n    \"Progress updates show which attempt is being made to fix a test (e.g., 'attempt 2/3')\",\n    \"Final success or failure status is reported after fix attempts\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer triggers test validation for a workspace or specific test file\",\n    \"System runs Jest tests and captures execution results\",\n    \"System reads failing test code and generates LLM prompts to fix issues\",\n    \"System applies LLM-suggested fixes by updating test files\",\n    \"System re-runs tests to verify fixes worked\",\n    \"Developer receives structured test reports with pass/fail statistics\",\n    \"System logs all validation and fix attempts for debugging\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"runTests\",\n      \"desc\": \"Executes all tests or a specific test file and returns results with pass/fail statistics\",\n      \"inputs\": \"workspaceRoot (string), optional testFile (string)\",\n      \"outputs\": \"Array of TestExecutionResult objects with passed/failed/error counts\"\n    },\n    {\n      \"name\": \"fixFailingTest\",\n      \"desc\": \"Attempts to automatically fix a failing test by using LLM to analyze errors and suggest corrections\",\n      \"inputs\": \"testFilePath, executionResult, workspaceRoot, llmService, maxAttempts (default 3)\",\n      \"outputs\": \"Object with success status, number of attempts made, and optional final error message\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestExecutionService\",\n    \"TestExecutionResult\",\n    \"TestReport\",\n    \"TestReportSummary\",\n    \"buildFixPrompt\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This file exists to automate the test validation workflow by running tests, detecting failures, and using AI to automatically fix broken tests instead of requiring manual developer intervention. It solves the problem of time-consuming manual test debugging and repair by leveraging LLM capabilities to understand test failures and generate fixes.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/testExecutionService.ts",
      "role": "Core Logic",
      "purpose": "Executes Jest tests and captures their results, providing detailed test execution information including passes, failures, and errors.",
      "userVisibleActions": [
        "Run all tests in the workspace",
        "Run tests for a specific test file",
        "View test execution results with pass/fail counts",
        "See detailed error messages and stack traces when tests fail",
        "Get notified when test execution times out or fails to run",
        "View test execution duration"
      ],
      "developerVisibleActions": [
        "Execute Jest tests programmatically via Node.js child process",
        "Parse Jest JSON output to extract test results",
        "Handle test execution errors and timeouts gracefully",
        "Capture both stdout and stderr from test execution",
        "Convert raw Jest output into structured TestExecutionResult objects",
        "Process individual test suite results with detailed pass/fail/error breakdowns",
        "Extract error details including test names, messages, and stack traces"
      ],
      "keyFunctions": [
        {
          "name": "runJest",
          "desc": "Executes Jest tests for a specific file or all tests in the workspace",
          "inputs": "workspaceRoot (string), optional testFile (string)",
          "outputs": "Promise<TestExecutionResult[]> containing test results with status, counts, and error details"
        },
        {
          "name": "parseJestOutput",
          "desc": "Parses Jest JSON output from stdout/stderr into structured test result objects",
          "inputs": "stdout (string), stderr (string)",
          "outputs": "TestExecutionResult[] array with parsed test data"
        }
      ],
      "dependencies": [
        "child_process",
        "path",
        "./types/testResultTypes"
      ],
      "intent": "This file exists to provide a reliable interface for running Jest tests within a VS Code extension or development environment, handling the complexity of executing external test commands, parsing their output, and converting it into a structured format that can be displayed to users or processed by other services. It solves the problem of bridging between Jest's command-line interface and the application's need for structured test execution data.",
      "rawContent": "```json\n{\n  \"purpose\": \"Executes Jest tests and captures their results, providing detailed test execution information including passes, failures, and errors.\",\n  \"userVisibleActions\": [\n    \"Run all tests in the workspace\",\n    \"Run tests for a specific test file\",\n    \"View test execution results with pass/fail counts\",\n    \"See detailed error messages and stack traces when tests fail\",\n    \"Get notified when test execution times out or fails to run\",\n    \"View test execution duration\"\n  ],\n  \"developerVisibleActions\": [\n    \"Execute Jest tests programmatically via Node.js child process\",\n    \"Parse Jest JSON output to extract test results\",\n    \"Handle test execution errors and timeouts gracefully\",\n    \"Capture both stdout and stderr from test execution\",\n    \"Convert raw Jest output into structured TestExecutionResult objects\",\n    \"Process individual test suite results with detailed pass/fail/error breakdowns\",\n    \"Extract error details including test names, messages, and stack traces\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"runJest\",\n      \"desc\": \"Executes Jest tests for a specific file or all tests in the workspace\",\n      \"inputs\": \"workspaceRoot (string), optional testFile (string)\",\n      \"outputs\": \"Promise<TestExecutionResult[]> containing test results with status, counts, and error details\"\n    },\n    {\n      \"name\": \"parseJestOutput\",\n      \"desc\": \"Parses Jest JSON output from stdout/stderr into structured test result objects\",\n      \"inputs\": \"stdout (string), stderr (string)\",\n      \"outputs\": \"TestExecutionResult[] array with parsed test data\"\n    }\n  ],\n  \"dependencies\": [\n    \"child_process\",\n    \"path\",\n    \"./types/testResultTypes\"\n  ],\n  \"intent\": \"This file exists to provide a reliable interface for running Jest tests within a VS Code extension or development environment, handling the complexity of executing external test commands, parsing their output, and converting it into a structured format that can be displayed to users or processed by other services. It solves the problem of bridging between Jest's command-line interface and the application's need for structured test execution data.\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T17:51:07.786Z"
  }
}