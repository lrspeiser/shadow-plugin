{
  "module": "src/domain/prompts",
  "moduleType": "other",
  "capabilities": [
    "Generates structured prompts for AI-powered code analysis and documentation",
    "Creates prompts for extracting testable functions and methods from source code",
    "Builds prompts for comprehensive project architecture analysis",
    "Generates prompts for automated test plan creation and test code generation",
    "Produces prompts for code refactoring recommendations with migration plans",
    "Constructs prompts for module and file-level documentation generation"
  ],
  "summary": "The prompts module serves as the AI prompt engineering layer for the application, generating specialized instruction templates that guide language models to perform various code analysis, documentation, and testing tasks. It provides a collection of prompt builders that create structured, consistent instructions for LLMs to analyze codebases, extract information, and generate outputs like documentation, test plans, and refactoring recommendations.\n\nThis module enables users to leverage AI capabilities across multiple workflows including project documentation generation, test strategy development, code refactoring guidance, and automated test implementation. Each prompt builder is designed to ensure the LLM receives clear, contextual instructions that produce actionable results. The module handles the complexity of translating user intentions into effective AI prompts, managing token budgets, and structuring inputs for optimal LLM performance.\n\nThe prompt builders work together to support end-to-end workflows: from analyzing project architecture and documenting code structure, to identifying testable components and generating comprehensive test suites, to recommending refactoring strategies with detailed migration steps. This creates a cohesive AI-assisted development experience where users can generate high-quality documentation, testing artifacts, and improvement recommendations through natural language interactions with the codebase.",
  "files": [
    {
      "file": "src/domain/prompts/functionExtractionPrompt.ts",
      "role": "Core Logic",
      "purpose": "Builds a prompt template for LLM-based extraction of testable functions, methods, and classes from source code files, replacing regex-based extraction that incorrectly captured control flow keywords.",
      "userVisibleActions": [
        "N/A - Internal prompt generation utility with no direct user interface"
      ],
      "developerVisibleActions": [
        "Developer calls buildFunctionExtractionPrompt() with an array of source files to analyze",
        "Receives a formatted prompt string that can be sent to an LLM for code analysis",
        "Can specify maximum functions to extract per file via maxFunctionsPerFile parameter",
        "System automatically truncates files over 10,000 characters to prevent token limit issues",
        "System includes file paths, languages, and formatted source code in the prompt",
        "System instructs LLM to extract functions/methods/classes while excluding control flow statements",
        "System requests structured output including name, file path, line numbers, complexity, dependencies, and visibility for each extracted item"
      ],
      "keyFunctions": [
        {
          "name": "buildFunctionExtractionPrompt",
          "desc": "Constructs an LLM prompt for extracting testable code elements from source files with specific rules to avoid control flow keywords",
          "inputs": "files (array of objects with path, content, language), maxFunctionsPerFile (optional number, default 50)",
          "outputs": "Formatted prompt string containing instructions and source code for LLM analysis"
        }
      ],
      "dependencies": [],
      "intent": "This file exists to solve the problem of accurately extracting testable functions from source code by using LLM-based analysis instead of regex patterns. It addresses the specific issue where regex incorrectly captured control flow keywords (if, for, while, etc.) as functions. The prompt template ensures the LLM understands what constitutes a valid testable function versus control flow statements, enabling more accurate test generation workflows.",
      "rawContent": "```json\n{\n  \"purpose\": \"Builds a prompt template for LLM-based extraction of testable functions, methods, and classes from source code files, replacing regex-based extraction that incorrectly captured control flow keywords.\",\n  \"userVisibleActions\": [\n    \"N/A - Internal prompt generation utility with no direct user interface\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer calls buildFunctionExtractionPrompt() with an array of source files to analyze\",\n    \"Receives a formatted prompt string that can be sent to an LLM for code analysis\",\n    \"Can specify maximum functions to extract per file via maxFunctionsPerFile parameter\",\n    \"System automatically truncates files over 10,000 characters to prevent token limit issues\",\n    \"System includes file paths, languages, and formatted source code in the prompt\",\n    \"System instructs LLM to extract functions/methods/classes while excluding control flow statements\",\n    \"System requests structured output including name, file path, line numbers, complexity, dependencies, and visibility for each extracted item\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"buildFunctionExtractionPrompt\",\n      \"desc\": \"Constructs an LLM prompt for extracting testable code elements from source files with specific rules to avoid control flow keywords\",\n      \"inputs\": \"files (array of objects with path, content, language), maxFunctionsPerFile (optional number, default 50)\",\n      \"outputs\": \"Formatted prompt string containing instructions and source code for LLM analysis\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"This file exists to solve the problem of accurately extracting testable functions from source code by using LLM-based analysis instead of regex patterns. It addresses the specific issue where regex incorrectly captured control flow keywords (if, for, while, etc.) as functions. The prompt template ensures the LLM understands what constitutes a valid testable function versus control flow statements, enabling more accurate test generation workflows.\"\n}\n```"
    },
    {
      "file": "src/domain/prompts/promptBuilder.ts",
      "role": "Core Logic",
      "purpose": "Constructs structured prompts for AI/LLM analysis tasks across the entire codebase, ensuring consistent and effective communication with language models.",
      "userVisibleActions": [
        "Analyzes project architecture and generates comprehensive documentation",
        "Creates product-level documentation from codebase analysis",
        "Generates test plans for code files with coverage recommendations",
        "Produces test code based on defined test scenarios",
        "Documents individual files with user-facing behavior descriptions",
        "Summarizes code modules and their relationships"
      ],
      "developerVisibleActions": [
        "Provides centralized prompt generation for all LLM analysis operations",
        "Builds architecture analysis prompts with codebase context and product documentation",
        "Constructs product documentation prompts from analysis context",
        "Creates file-specific analysis prompts with role-based categorization",
        "Generates module rollup prompts for aggregating file summaries",
        "Builds test planning prompts with existing test awareness and function metadata",
        "Produces test code generation prompts with source code and test framework details",
        "Eliminates prompt duplication across different analysis services",
        "Supports multiple programming languages and test frameworks"
      ],
      "keyFunctions": [
        {
          "name": "buildArchitecturePrompt",
          "desc": "Constructs a prompt for analyzing overall system architecture",
          "inputs": "context (AnalysisContext), optional codeAnalysis, productDocs, productPurposeAnalysis, fileAccessHelper",
          "outputs": "string prompt for LLM consumption"
        },
        {
          "name": "buildProductDocsPrompt",
          "desc": "Creates a prompt for generating product-level documentation",
          "inputs": "context (AnalysisContext)",
          "outputs": "string prompt for documentation generation"
        },
        {
          "name": "buildProductPurposePrompt",
          "desc": "Generates a prompt for understanding the product's core purpose",
          "inputs": "productDocs (EnhancedProductDocumentation), context (AnalysisContext)",
          "outputs": "string prompt for purpose analysis"
        },
        {
          "name": "buildFileAnalysisPrompt",
          "desc": "Creates a prompt for analyzing individual code files",
          "inputs": "file (FileInfo), content (string), role (string)",
          "outputs": "string prompt for file analysis"
        },
        {
          "name": "buildModuleRollupPrompt",
          "desc": "Constructs a prompt for aggregating multiple file summaries into module documentation",
          "inputs": "modulePath (string), moduleType (string), files (FileSummary[])",
          "outputs": "string prompt for module summary"
        },
        {
          "name": "buildProductLevelPrompt",
          "desc": "Builds a comprehensive prompt for product-wide documentation",
          "inputs": "fileSummaries, moduleSummaries, analysis (CodeAnalysis), fileAccessHelper",
          "outputs": "string prompt for product documentation"
        },
        {
          "name": "buildPerFileTestPlanPrompt",
          "desc": "Creates a prompt for generating test plans for specific files",
          "inputs": "filePath, fileContent, functionMetadata, existingTests, language, testFramework, optional projectSummary",
          "outputs": "string prompt for test planning"
        },
        {
          "name": "buildTestCodeGenerationPrompt",
          "desc": "Generates a prompt for creating actual test code from test plans",
          "inputs": "testPlanItem, sourceCode, functionCode, language, testFramework",
          "outputs": "string prompt for test code generation"
        }
      ],
      "dependencies": [
        "../../llmService",
        "../../analyzer",
        "../../fileDocumentation",
        "../../fileAccessHelper"
      ],
      "intent": "This file exists to eliminate duplication and ensure consistency across all AI/LLM interactions by centralizing prompt construction logic. It solves the problem of scattered, inconsistent prompt generation throughout the codebase, making it easier to maintain and improve how the system communicates with language models for analysis, documentation, and test generation tasks.",
      "rawContent": "```json\n{\n  \"purpose\": \"Constructs structured prompts for AI/LLM analysis tasks across the entire codebase, ensuring consistent and effective communication with language models.\",\n  \"userVisibleActions\": [\n    \"Analyzes project architecture and generates comprehensive documentation\",\n    \"Creates product-level documentation from codebase analysis\",\n    \"Generates test plans for code files with coverage recommendations\",\n    \"Produces test code based on defined test scenarios\",\n    \"Documents individual files with user-facing behavior descriptions\",\n    \"Summarizes code modules and their relationships\"\n  ],\n  \"developerVisibleActions\": [\n    \"Provides centralized prompt generation for all LLM analysis operations\",\n    \"Builds architecture analysis prompts with codebase context and product documentation\",\n    \"Constructs product documentation prompts from analysis context\",\n    \"Creates file-specific analysis prompts with role-based categorization\",\n    \"Generates module rollup prompts for aggregating file summaries\",\n    \"Builds test planning prompts with existing test awareness and function metadata\",\n    \"Produces test code generation prompts with source code and test framework details\",\n    \"Eliminates prompt duplication across different analysis services\",\n    \"Supports multiple programming languages and test frameworks\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"buildArchitecturePrompt\",\n      \"desc\": \"Constructs a prompt for analyzing overall system architecture\",\n      \"inputs\": \"context (AnalysisContext), optional codeAnalysis, productDocs, productPurposeAnalysis, fileAccessHelper\",\n      \"outputs\": \"string prompt for LLM consumption\"\n    },\n    {\n      \"name\": \"buildProductDocsPrompt\",\n      \"desc\": \"Creates a prompt for generating product-level documentation\",\n      \"inputs\": \"context (AnalysisContext)\",\n      \"outputs\": \"string prompt for documentation generation\"\n    },\n    {\n      \"name\": \"buildProductPurposePrompt\",\n      \"desc\": \"Generates a prompt for understanding the product's core purpose\",\n      \"inputs\": \"productDocs (EnhancedProductDocumentation), context (AnalysisContext)\",\n      \"outputs\": \"string prompt for purpose analysis\"\n    },\n    {\n      \"name\": \"buildFileAnalysisPrompt\",\n      \"desc\": \"Creates a prompt for analyzing individual code files\",\n      \"inputs\": \"file (FileInfo), content (string), role (string)\",\n      \"outputs\": \"string prompt for file analysis\"\n    },\n    {\n      \"name\": \"buildModuleRollupPrompt\",\n      \"desc\": \"Constructs a prompt for aggregating multiple file summaries into module documentation\",\n      \"inputs\": \"modulePath (string), moduleType (string), files (FileSummary[])\",\n      \"outputs\": \"string prompt for module summary\"\n    },\n    {\n      \"name\": \"buildProductLevelPrompt\",\n      \"desc\": \"Builds a comprehensive prompt for product-wide documentation\",\n      \"inputs\": \"fileSummaries, moduleSummaries, analysis (CodeAnalysis), fileAccessHelper\",\n      \"outputs\": \"string prompt for product documentation\"\n    },\n    {\n      \"name\": \"buildPerFileTestPlanPrompt\",\n      \"desc\": \"Creates a prompt for generating test plans for specific files\",\n      \"inputs\": \"filePath, fileContent, functionMetadata, existingTests, language, testFramework, optional projectSummary\",\n      \"outputs\": \"string prompt for test planning\"\n    },\n    {\n      \"name\": \"buildTestCodeGenerationPrompt\",\n      \"desc\": \"Generates a prompt for creating actual test code from test plans\",\n      \"inputs\": \"testPlanItem, sourceCode, functionCode, language, testFramework\",\n      \"outputs\": \"string prompt for test code generation\"\n    }\n  ],\n  \"dependencies\": [\n    \"../../llmService\",\n    \"../../analyzer\",\n    \"../../fileDocumentation\",\n    \"../../fileAccessHelper\"\n  ],\n  \"intent\": \"This file exists to eliminate duplication and ensure consistency across all AI/LLM interactions by centralizing prompt construction logic. It solves the problem of scattered, inconsistent prompt generation throughout the codebase, making it easier to maintain and improve how the system communicates with language models for analysis, documentation, and test generation tasks.\"\n}\n```"
    },
    {
      "file": "src/domain/prompts/refactoringPromptBuilder.ts",
      "role": "Core Logic",
      "purpose": "Generates AI prompts that instruct an LLM to create detailed code refactoring plans with function extraction recommendations.",
      "userVisibleActions": [
        "Receives detailed refactoring recommendations for improving code organization",
        "Gets step-by-step migration plans for extracting functions to new files",
        "Views before/after code examples showing proposed refactoring changes",
        "Sees analysis of function dependencies and which code calls what"
      ],
      "developerVisibleActions": [
        "Builds comprehensive prompts that guide LLMs to generate refactoring reports",
        "Includes code analysis data, product documentation, and architecture insights in prompts",
        "Generates prompts that request function extraction plans with dependencies and migration steps",
        "Provides function-level analysis including responsibilities, dependencies, and dependents",
        "Creates prompts requesting before/after code examples for refactoring changes"
      ],
      "keyFunctions": [
        {
          "name": "buildDetailedRefactoringPrompt",
          "desc": "Creates a comprehensive prompt for generating detailed refactoring recommendations",
          "inputs": "context (analysis settings), codeAnalysis (parsed code structure), productDocs (optional documentation), architectureInsights (optional LLM insights), functionAnalyses (optional function metadata)",
          "outputs": "String prompt for LLM to generate refactoring report"
        },
        {
          "name": "buildBasePrompt",
          "desc": "Constructs the foundational prompt section with project context and code analysis",
          "inputs": "context, codeAnalysis, productDocs, architectureInsights",
          "outputs": "String containing base prompt instructions"
        },
        {
          "name": "buildFunctionAnalysisSection",
          "desc": "Adds detailed function analysis information to the prompt",
          "inputs": "functionAnalyses (array of function metadata)",
          "outputs": "String section describing function responsibilities and relationships"
        },
        {
          "name": "buildExtractionRequirementsSection",
          "desc": "Specifies requirements for LLM to generate function extraction plans",
          "inputs": "None",
          "outputs": "String section with extraction planning instructions"
        }
      ],
      "dependencies": [
        "../../analyzer (CodeAnalysis, FileInfo, FunctionMetadata)",
        "../../llmService (AnalysisContext, LLMInsights)",
        "../../fileDocumentation (EnhancedProductDocumentation)"
      ],
      "intent": "This file exists to translate code analysis results into structured prompts that guide AI models to generate actionable refactoring recommendations. It solves the problem of getting useful, detailed refactoring suggestions by carefully crafting prompts that include function dependencies, extraction plans, and migration steps rather than generic improvement advice.",
      "rawContent": "```json\n{\n  \"purpose\": \"Generates AI prompts that instruct an LLM to create detailed code refactoring plans with function extraction recommendations.\",\n  \"userVisibleActions\": [\n    \"Receives detailed refactoring recommendations for improving code organization\",\n    \"Gets step-by-step migration plans for extracting functions to new files\",\n    \"Views before/after code examples showing proposed refactoring changes\",\n    \"Sees analysis of function dependencies and which code calls what\"\n  ],\n  \"developerVisibleActions\": [\n    \"Builds comprehensive prompts that guide LLMs to generate refactoring reports\",\n    \"Includes code analysis data, product documentation, and architecture insights in prompts\",\n    \"Generates prompts that request function extraction plans with dependencies and migration steps\",\n    \"Provides function-level analysis including responsibilities, dependencies, and dependents\",\n    \"Creates prompts requesting before/after code examples for refactoring changes\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"buildDetailedRefactoringPrompt\",\n      \"desc\": \"Creates a comprehensive prompt for generating detailed refactoring recommendations\",\n      \"inputs\": \"context (analysis settings), codeAnalysis (parsed code structure), productDocs (optional documentation), architectureInsights (optional LLM insights), functionAnalyses (optional function metadata)\",\n      \"outputs\": \"String prompt for LLM to generate refactoring report\"\n    },\n    {\n      \"name\": \"buildBasePrompt\",\n      \"desc\": \"Constructs the foundational prompt section with project context and code analysis\",\n      \"inputs\": \"context, codeAnalysis, productDocs, architectureInsights\",\n      \"outputs\": \"String containing base prompt instructions\"\n    },\n    {\n      \"name\": \"buildFunctionAnalysisSection\",\n      \"desc\": \"Adds detailed function analysis information to the prompt\",\n      \"inputs\": \"functionAnalyses (array of function metadata)\",\n      \"outputs\": \"String section describing function responsibilities and relationships\"\n    },\n    {\n      \"name\": \"buildExtractionRequirementsSection\",\n      \"desc\": \"Specifies requirements for LLM to generate function extraction plans\",\n      \"inputs\": \"None\",\n      \"outputs\": \"String section with extraction planning instructions\"\n    }\n  ],\n  \"dependencies\": [\n    \"../../analyzer (CodeAnalysis, FileInfo, FunctionMetadata)\",\n    \"../../llmService (AnalysisContext, LLMInsights)\",\n    \"../../fileDocumentation (EnhancedProductDocumentation)\"\n  ],\n  \"intent\": \"This file exists to translate code analysis results into structured prompts that guide AI models to generate actionable refactoring recommendations. It solves the problem of getting useful, detailed refactoring suggestions by carefully crafting prompts that include function dependencies, extraction plans, and migration steps rather than generic improvement advice.\"\n}\n```"
    },
    {
      "file": "src/domain/prompts/testPrompts.ts",
      "role": "Core Logic",
      "purpose": "Provides prompt builders that generate structured instructions for an LLM to analyze codebases and create test plans, configurations, and test implementations.",
      "userVisibleActions": [
        "Automatically analyzes the project structure and suggests appropriate testing frameworks",
        "Generates prioritized test plans based on code complexity and risk",
        "Creates test implementations for specific functions",
        "Provides test setup recommendations including dependencies and configurations"
      ],
      "developerVisibleActions": [
        "Call buildSetupPrompt() with workspace info to get LLM prompt for test configuration recommendations",
        "Call buildPlanningPrompt() with code analysis to get LLM prompt for test strategy and prioritization",
        "Call buildImplementationPrompt() with function details to get LLM prompt for generating actual test code",
        "Receive structured JSON responses from LLM containing test plans, configurations, and code",
        "Prompts guide LLM to consider code complexity, dependencies, edge cases, and testing best practices"
      ],
      "keyFunctions": [
        {
          "name": "buildSetupPrompt",
          "desc": "Creates a prompt asking LLM to analyze codebase structure and recommend optimal test setup",
          "inputs": "workspaceRoot: string, fileList: string[], packageJsonContent?: string",
          "outputs": "Formatted prompt string requesting JSON response with language, framework, dependencies, and config files"
        },
        {
          "name": "buildPlanningPrompt",
          "desc": "Creates a prompt asking LLM to generate a prioritized test plan based on code analysis",
          "inputs": "context: CodeAnalysis, functions: any[], productDocs?: any, architectureInsights?: any",
          "outputs": "Formatted prompt string requesting JSON response with priority levels, coverage targets, and testing strategy"
        },
        {
          "name": "buildImplementationPrompt",
          "desc": "Creates a prompt asking LLM to generate actual test code for a specific function",
          "inputs": "testableFunction: TestableFunction, setupInfo: any, contextualInfo?: any",
          "outputs": "Formatted prompt string requesting complete test implementation with setup, assertions, and edge cases"
        }
      ],
      "dependencies": [
        "../../analyzer (CodeAnalysis type)",
        "../services/testing/types/testPlanTypes (TestableFunction type)"
      ],
      "intent": "This file exists to standardize how the system communicates with LLMs for test generation tasks. It encapsulates the complex prompt engineering needed to get high-quality, structured test recommendations and implementations from language models, ensuring consistent formatting and comprehensive coverage of testing concerns.",
      "rawContent": "```json\n{\n  \"purpose\": \"Provides prompt builders that generate structured instructions for an LLM to analyze codebases and create test plans, configurations, and test implementations.\",\n  \"userVisibleActions\": [\n    \"Automatically analyzes the project structure and suggests appropriate testing frameworks\",\n    \"Generates prioritized test plans based on code complexity and risk\",\n    \"Creates test implementations for specific functions\",\n    \"Provides test setup recommendations including dependencies and configurations\"\n  ],\n  \"developerVisibleActions\": [\n    \"Call buildSetupPrompt() with workspace info to get LLM prompt for test configuration recommendations\",\n    \"Call buildPlanningPrompt() with code analysis to get LLM prompt for test strategy and prioritization\",\n    \"Call buildImplementationPrompt() with function details to get LLM prompt for generating actual test code\",\n    \"Receive structured JSON responses from LLM containing test plans, configurations, and code\",\n    \"Prompts guide LLM to consider code complexity, dependencies, edge cases, and testing best practices\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"buildSetupPrompt\",\n      \"desc\": \"Creates a prompt asking LLM to analyze codebase structure and recommend optimal test setup\",\n      \"inputs\": \"workspaceRoot: string, fileList: string[], packageJsonContent?: string\",\n      \"outputs\": \"Formatted prompt string requesting JSON response with language, framework, dependencies, and config files\"\n    },\n    {\n      \"name\": \"buildPlanningPrompt\",\n      \"desc\": \"Creates a prompt asking LLM to generate a prioritized test plan based on code analysis\",\n      \"inputs\": \"context: CodeAnalysis, functions: any[], productDocs?: any, architectureInsights?: any\",\n      \"outputs\": \"Formatted prompt string requesting JSON response with priority levels, coverage targets, and testing strategy\"\n    },\n    {\n      \"name\": \"buildImplementationPrompt\",\n      \"desc\": \"Creates a prompt asking LLM to generate actual test code for a specific function\",\n      \"inputs\": \"testableFunction: TestableFunction, setupInfo: any, contextualInfo?: any\",\n      \"outputs\": \"Formatted prompt string requesting complete test implementation with setup, assertions, and edge cases\"\n    }\n  ],\n  \"dependencies\": [\n    \"../../analyzer (CodeAnalysis type)\",\n    \"../services/testing/types/testPlanTypes (TestableFunction type)\"\n  ],\n  \"intent\": \"This file exists to standardize how the system communicates with LLMs for test generation tasks. It encapsulates the complex prompt engineering needed to get high-quality, structured test recommendations and implementations from language models, ensuring consistent formatting and comprehensive coverage of testing concerns.\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T17:50:34.834Z"
  }
}