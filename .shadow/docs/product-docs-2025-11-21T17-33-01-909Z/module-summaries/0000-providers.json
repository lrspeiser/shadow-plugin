{
  "module": "src/ai/providers",
  "moduleType": "other",
  "capabilities": [
    "Unified interface for interacting with multiple AI language model providers (OpenAI GPT and Anthropic Claude)",
    "Seamless switching between different AI providers based on configuration without changing workflows",
    "Structured JSON output generation from AI models using defined schemas",
    "Automatic request retry logic with exponential backoff for failed API calls",
    "Real-time streaming responses for immediate AI output display",
    "API key validation and configuration checking before processing requests",
    "Automatic extraction and parsing of JSON data from AI responses",
    "AI-suggested file and grep search requests based on analysis needs"
  ],
  "summary": "The AI Providers module serves as the abstraction layer for integrating multiple AI language model services into the application. It provides a standardized interface that allows users to interact with different AI providers (currently OpenAI's GPT models and Anthropic's Claude) transparently, without needing to understand the underlying implementation differences. The module handles provider instantiation, configuration management, and ensures consistent behavior across all supported AI services.\n\nUsers can send messages to AI models and receive intelligent text responses, generate structured JSON outputs based on predefined schemas, and benefit from automatic error handling including retry logic for transient failures. The provider factory pattern enables dynamic selection of the active AI provider based on user configuration, making it easy to switch between OpenAI and Claude depending on preferences or requirements. All AI-powered features in the application leverage this module to deliver consistent, reliable AI interactions regardless of which provider is configured.\n\nThe module emphasizes reliability and user experience through features like streaming response support for real-time output display, automatic configuration validation to catch setup issues early, and intelligent parsing of AI responses to extract structured data. This architecture ensures that as new AI providers become available, they can be integrated seamlessly without disrupting existing user workflows or requiring changes to other parts of the application.",
  "files": [
    {
      "file": "src/ai/providers/ILLMProvider.ts",
      "role": "Core Logic",
      "purpose": "Defines the standard interface for all LLM (Large Language Model) provider implementations to ensure consistent AI integration across different providers like OpenAI and Claude.",
      "userVisibleActions": [
        "User receives AI-generated text responses to their queries",
        "User receives structured JSON data from AI models when requesting formatted output",
        "User can work with different AI providers (OpenAI, Claude, custom) transparently without changing their workflow",
        "User gets file and grep search requests suggested by the AI based on analysis needs"
      ],
      "developerVisibleActions": [
        "Developer implements this interface to add support for new LLM providers",
        "Developer checks if an AI provider is configured and ready using isConfigured()",
        "Developer sends text-based requests to AI models with customizable parameters (temperature, max tokens, model selection)",
        "Developer requests structured JSON output from AI models with optional schema validation",
        "Developer receives additional context requests (file reads, grep searches) from AI responses",
        "Developer accesses raw provider responses for debugging or advanced processing",
        "Developer identifies which provider is being used via getName()"
      ],
      "keyFunctions": [
        {
          "name": "isConfigured",
          "desc": "Verifies if the LLM provider has valid credentials and is ready to process requests",
          "inputs": "none",
          "outputs": "boolean indicating configuration status"
        },
        {
          "name": "sendRequest",
          "desc": "Sends a prompt to the LLM and retrieves a text response with optional configuration like temperature and token limits",
          "inputs": "LLMRequestOptions (messages, model, temperature, maxTokens, systemPrompt, responseFormat)",
          "outputs": "Promise<LLMResponse> containing generated text content, finish reason, model used, and raw response"
        },
        {
          "name": "sendStructuredRequest",
          "desc": "Sends a prompt expecting structured JSON output, optionally validated against a schema, with additional file/grep requests",
          "inputs": "LLMRequestOptions and optional schema for validation",
          "outputs": "Promise<StructuredOutputResponse<T>> containing parsed data and optional file/grep requests"
        },
        {
          "name": "getName",
          "desc": "Returns the identifier of the LLM provider for logging and user display",
          "inputs": "none",
          "outputs": "string with provider name"
        }
      ],
      "dependencies": [],
      "intent": "This interface exists to abstract away differences between various LLM providers (OpenAI, Claude, custom implementations), allowing the codebase to work with any AI provider through a unified API. It solves the problem of vendor lock-in and enables easy switching or addition of new AI providers without changing consuming code. It also standardizes how structured outputs and additional context requests are handled across providers.",
      "rawContent": "```json\n{\n  \"purpose\": \"Defines the standard interface for all LLM (Large Language Model) provider implementations to ensure consistent AI integration across different providers like OpenAI and Claude.\",\n  \"userVisibleActions\": [\n    \"User receives AI-generated text responses to their queries\",\n    \"User receives structured JSON data from AI models when requesting formatted output\",\n    \"User can work with different AI providers (OpenAI, Claude, custom) transparently without changing their workflow\",\n    \"User gets file and grep search requests suggested by the AI based on analysis needs\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer implements this interface to add support for new LLM providers\",\n    \"Developer checks if an AI provider is configured and ready using isConfigured()\",\n    \"Developer sends text-based requests to AI models with customizable parameters (temperature, max tokens, model selection)\",\n    \"Developer requests structured JSON output from AI models with optional schema validation\",\n    \"Developer receives additional context requests (file reads, grep searches) from AI responses\",\n    \"Developer accesses raw provider responses for debugging or advanced processing\",\n    \"Developer identifies which provider is being used via getName()\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Verifies if the LLM provider has valid credentials and is ready to process requests\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating configuration status\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a prompt to the LLM and retrieves a text response with optional configuration like temperature and token limits\",\n      \"inputs\": \"LLMRequestOptions (messages, model, temperature, maxTokens, systemPrompt, responseFormat)\",\n      \"outputs\": \"Promise<LLMResponse> containing generated text content, finish reason, model used, and raw response\"\n    },\n    {\n      \"name\": \"sendStructuredRequest\",\n      \"desc\": \"Sends a prompt expecting structured JSON output, optionally validated against a schema, with additional file/grep requests\",\n      \"inputs\": \"LLMRequestOptions and optional schema for validation\",\n      \"outputs\": \"Promise<StructuredOutputResponse<T>> containing parsed data and optional file/grep requests\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the identifier of the LLM provider for logging and user display\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string with provider name\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"This interface exists to abstract away differences between various LLM providers (OpenAI, Claude, custom implementations), allowing the codebase to work with any AI provider through a unified API. It solves the problem of vendor lock-in and enables easy switching or addition of new AI providers without changing consuming code. It also standardizes how structured outputs and additional context requests are handled across providers.\"\n}\n```"
    },
    {
      "file": "src/ai/providers/anthropicProvider.ts",
      "role": "Core Logic",
      "purpose": "Provides integration with Anthropic's Claude AI models for sending chat requests and receiving AI-generated responses",
      "userVisibleActions": [
        "Sends messages to Claude AI and receives intelligent responses",
        "Generates structured JSON outputs from Claude based on schemas",
        "Automatically retries failed requests with exponential backoff",
        "Validates Claude API configuration before allowing requests",
        "Extracts and parses JSON from Claude's responses automatically"
      ],
      "developerVisibleActions": [
        "Configure Claude API key through configuration manager to enable the provider",
        "Send chat requests with system prompts, conversation history, and model selection",
        "Request structured outputs by providing a JSON schema and receive validated responses",
        "Check if Claude is configured and available before making requests",
        "Handle errors when API key is missing or requests fail",
        "Automatically converts OpenAI-format messages to Claude's format",
        "Receives responses with token usage information and content"
      ],
      "keyFunctions": [
        {
          "name": "isConfigured",
          "desc": "Checks if Claude API key is set up and provider is ready to use",
          "inputs": "none",
          "outputs": "boolean indicating configuration status"
        },
        {
          "name": "getName",
          "desc": "Returns the provider identifier",
          "inputs": "none",
          "outputs": "string 'claude'"
        },
        {
          "name": "sendRequest",
          "desc": "Sends a chat completion request to Claude with messages and options",
          "inputs": "LLMRequestOptions with messages, model, maxTokens, systemPrompt",
          "outputs": "LLMResponse with content, model, and token usage"
        },
        {
          "name": "sendStructuredOutputRequest",
          "desc": "Sends a request to Claude and ensures the response matches a provided JSON schema",
          "inputs": "LLMRequestOptions plus JSON schema definition",
          "outputs": "StructuredOutputResponse with validated JSON data and metadata"
        },
        {
          "name": "initialize",
          "desc": "Sets up the Claude client with API key from configuration",
          "inputs": "none (reads from config)",
          "outputs": "void (initializes internal client)"
        }
      ],
      "dependencies": [
        "@anthropic-ai/sdk",
        "../../config/configurationManager",
        "../../utils/jsonExtractor",
        "./ILLMProvider"
      ],
      "intent": "This file exists to abstract and implement the specific integration with Anthropic's Claude API, converting between the application's generic LLM interface and Claude's specific API format, handling authentication, request formatting, response parsing, and error handling for Claude-specific operations.",
      "rawContent": "```json\n{\n  \"purpose\": \"Provides integration with Anthropic's Claude AI models for sending chat requests and receiving AI-generated responses\",\n  \"userVisibleActions\": [\n    \"Sends messages to Claude AI and receives intelligent responses\",\n    \"Generates structured JSON outputs from Claude based on schemas\",\n    \"Automatically retries failed requests with exponential backoff\",\n    \"Validates Claude API configuration before allowing requests\",\n    \"Extracts and parses JSON from Claude's responses automatically\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure Claude API key through configuration manager to enable the provider\",\n    \"Send chat requests with system prompts, conversation history, and model selection\",\n    \"Request structured outputs by providing a JSON schema and receive validated responses\",\n    \"Check if Claude is configured and available before making requests\",\n    \"Handle errors when API key is missing or requests fail\",\n    \"Automatically converts OpenAI-format messages to Claude's format\",\n    \"Receives responses with token usage information and content\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Checks if Claude API key is set up and provider is ready to use\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating configuration status\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the provider identifier\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string 'claude'\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a chat completion request to Claude with messages and options\",\n      \"inputs\": \"LLMRequestOptions with messages, model, maxTokens, systemPrompt\",\n      \"outputs\": \"LLMResponse with content, model, and token usage\"\n    },\n    {\n      \"name\": \"sendStructuredOutputRequest\",\n      \"desc\": \"Sends a request to Claude and ensures the response matches a provided JSON schema\",\n      \"inputs\": \"LLMRequestOptions plus JSON schema definition\",\n      \"outputs\": \"StructuredOutputResponse with validated JSON data and metadata\"\n    },\n    {\n      \"name\": \"initialize\",\n      \"desc\": \"Sets up the Claude client with API key from configuration\",\n      \"inputs\": \"none (reads from config)\",\n      \"outputs\": \"void (initializes internal client)\"\n    }\n  ],\n  \"dependencies\": [\n    \"@anthropic-ai/sdk\",\n    \"../../config/configurationManager\",\n    \"../../utils/jsonExtractor\",\n    \"./ILLMProvider\"\n  ],\n  \"intent\": \"This file exists to abstract and implement the specific integration with Anthropic's Claude API, converting between the application's generic LLM interface and Claude's specific API format, handling authentication, request formatting, response parsing, and error handling for Claude-specific operations.\"\n}\n```"
    },
    {
      "file": "src/ai/providers/openAIProvider.ts",
      "role": "Core Logic",
      "purpose": "Implements the OpenAI API provider to send chat completion requests and handle structured responses using OpenAI's GPT models",
      "userVisibleActions": [
        "Sends messages to OpenAI's GPT models and receives AI-generated responses",
        "Supports structured JSON output format when requested",
        "Processes streaming responses for real-time AI output display",
        "Validates API key configuration before allowing requests",
        "Returns error messages when OpenAI API key is not configured"
      ],
      "developerVisibleActions": [
        "Provides ILLMProvider interface implementation for OpenAI",
        "Automatically initializes OpenAI client with API key from configuration manager",
        "Defaults to 'gpt-4o' model when no model is specified",
        "Sets 5-minute timeout for all OpenAI API requests",
        "Combines system prompts with conversation messages automatically",
        "Extracts and parses JSON from structured responses",
        "Handles streaming responses with token-by-token content delivery",
        "Reports finish reasons (stop, length, content_filter, etc.) from API responses"
      ],
      "keyFunctions": [
        {
          "name": "initialize",
          "desc": "Sets up OpenAI client with API key from configuration",
          "inputs": "none",
          "outputs": "void"
        },
        {
          "name": "isConfigured",
          "desc": "Checks if OpenAI client is ready with valid API key",
          "inputs": "none",
          "outputs": "boolean"
        },
        {
          "name": "getName",
          "desc": "Returns provider identifier",
          "inputs": "none",
          "outputs": "string 'openai'"
        },
        {
          "name": "sendRequest",
          "desc": "Sends chat completion request to OpenAI API with messages and options",
          "inputs": "LLMRequestOptions (model, messages, systemPrompt, responseFormat)",
          "outputs": "Promise<LLMResponse> with content and finish reason"
        },
        {
          "name": "sendStructuredRequest",
          "desc": "Sends request expecting JSON response and parses it into structured data",
          "inputs": "LLMRequestOptions with JSON response format",
          "outputs": "Promise<StructuredOutputResponse> with parsed JSON data"
        },
        {
          "name": "sendStreamingRequest",
          "desc": "Sends streaming request and yields response tokens in real-time",
          "inputs": "LLMRequestOptions",
          "outputs": "AsyncGenerator yielding content chunks and finish reason"
        }
      ],
      "dependencies": [
        "openai",
        "configurationManager",
        "jsonExtractor",
        "ILLMProvider"
      ],
      "intent": "This file exists to integrate OpenAI's GPT models into the application by providing a standardized interface for chat completions, structured outputs, and streaming responses. It solves the problem of communicating with OpenAI's API while abstracting away provider-specific details through the ILLMProvider interface, allowing the application to easily swap AI providers.",
      "rawContent": "```json\n{\n  \"purpose\": \"Implements the OpenAI API provider to send chat completion requests and handle structured responses using OpenAI's GPT models\",\n  \"userVisibleActions\": [\n    \"Sends messages to OpenAI's GPT models and receives AI-generated responses\",\n    \"Supports structured JSON output format when requested\",\n    \"Processes streaming responses for real-time AI output display\",\n    \"Validates API key configuration before allowing requests\",\n    \"Returns error messages when OpenAI API key is not configured\"\n  ],\n  \"developerVisibleActions\": [\n    \"Provides ILLMProvider interface implementation for OpenAI\",\n    \"Automatically initializes OpenAI client with API key from configuration manager\",\n    \"Defaults to 'gpt-4o' model when no model is specified\",\n    \"Sets 5-minute timeout for all OpenAI API requests\",\n    \"Combines system prompts with conversation messages automatically\",\n    \"Extracts and parses JSON from structured responses\",\n    \"Handles streaming responses with token-by-token content delivery\",\n    \"Reports finish reasons (stop, length, content_filter, etc.) from API responses\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"initialize\",\n      \"desc\": \"Sets up OpenAI client with API key from configuration\",\n      \"inputs\": \"none\",\n      \"outputs\": \"void\"\n    },\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Checks if OpenAI client is ready with valid API key\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns provider identifier\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string 'openai'\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends chat completion request to OpenAI API with messages and options\",\n      \"inputs\": \"LLMRequestOptions (model, messages, systemPrompt, responseFormat)\",\n      \"outputs\": \"Promise<LLMResponse> with content and finish reason\"\n    },\n    {\n      \"name\": \"sendStructuredRequest\",\n      \"desc\": \"Sends request expecting JSON response and parses it into structured data\",\n      \"inputs\": \"LLMRequestOptions with JSON response format\",\n      \"outputs\": \"Promise<StructuredOutputResponse> with parsed JSON data\"\n    },\n    {\n      \"name\": \"sendStreamingRequest\",\n      \"desc\": \"Sends streaming request and yields response tokens in real-time\",\n      \"inputs\": \"LLMRequestOptions\",\n      \"outputs\": \"AsyncGenerator yielding content chunks and finish reason\"\n    }\n  ],\n  \"dependencies\": [\n    \"openai\",\n    \"configurationManager\",\n    \"jsonExtractor\",\n    \"ILLMProvider\"\n  ],\n  \"intent\": \"This file exists to integrate OpenAI's GPT models into the application by providing a standardized interface for chat completions, structured outputs, and streaming responses. It solves the problem of communicating with OpenAI's API while abstracting away provider-specific details through the ILLMProvider interface, allowing the application to easily swap AI providers.\"\n}\n```"
    },
    {
      "file": "src/ai/providers/providerFactory.ts",
      "role": "Core Logic",
      "purpose": "Creates and manages AI language model provider instances (OpenAI and Anthropic/Claude) with lazy initialization and configuration checking",
      "userVisibleActions": [
        "Switches between different AI providers (OpenAI or Claude) based on configuration",
        "Uses the currently configured AI provider for all AI-powered features",
        "Gets error feedback when an unknown AI provider is selected"
      ],
      "developerVisibleActions": [
        "Retrieve a specific AI provider instance by name (openai or claude)",
        "Get the currently configured AI provider based on user settings",
        "Check if a specific AI provider is properly configured with credentials",
        "Get a list of all properly configured AI providers available for use",
        "Providers are created lazily only when first requested to save resources"
      ],
      "keyFunctions": [
        {
          "name": "getProvider",
          "desc": "Returns the provider instance for the specified provider type (openai or claude)",
          "inputs": "provider: LLMProvider ('openai' | 'claude')",
          "outputs": "ILLMProvider instance"
        },
        {
          "name": "getCurrentProvider",
          "desc": "Returns the currently configured provider based on user settings",
          "inputs": "none",
          "outputs": "ILLMProvider instance"
        },
        {
          "name": "isProviderConfigured",
          "desc": "Checks if a provider has valid configuration and credentials",
          "inputs": "provider: LLMProvider ('openai' | 'claude')",
          "outputs": "boolean indicating if provider is ready to use"
        },
        {
          "name": "getConfiguredProviders",
          "desc": "Returns a list of all providers that are properly configured",
          "inputs": "none",
          "outputs": "Array of configured LLMProvider names"
        }
      ],
      "dependencies": [
        "./ILLMProvider",
        "./openAIProvider",
        "./anthropicProvider",
        "../../config/configurationManager"
      ],
      "intent": "Centralizes AI provider creation and management to avoid duplicate instances, enable easy switching between providers, and provide a consistent interface for checking provider availability and configuration status",
      "rawContent": "```json\n{\n  \"purpose\": \"Creates and manages AI language model provider instances (OpenAI and Anthropic/Claude) with lazy initialization and configuration checking\",\n  \"userVisibleActions\": [\n    \"Switches between different AI providers (OpenAI or Claude) based on configuration\",\n    \"Uses the currently configured AI provider for all AI-powered features\",\n    \"Gets error feedback when an unknown AI provider is selected\"\n  ],\n  \"developerVisibleActions\": [\n    \"Retrieve a specific AI provider instance by name (openai or claude)\",\n    \"Get the currently configured AI provider based on user settings\",\n    \"Check if a specific AI provider is properly configured with credentials\",\n    \"Get a list of all properly configured AI providers available for use\",\n    \"Providers are created lazily only when first requested to save resources\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"getProvider\",\n      \"desc\": \"Returns the provider instance for the specified provider type (openai or claude)\",\n      \"inputs\": \"provider: LLMProvider ('openai' | 'claude')\",\n      \"outputs\": \"ILLMProvider instance\"\n    },\n    {\n      \"name\": \"getCurrentProvider\",\n      \"desc\": \"Returns the currently configured provider based on user settings\",\n      \"inputs\": \"none\",\n      \"outputs\": \"ILLMProvider instance\"\n    },\n    {\n      \"name\": \"isProviderConfigured\",\n      \"desc\": \"Checks if a provider has valid configuration and credentials\",\n      \"inputs\": \"provider: LLMProvider ('openai' | 'claude')\",\n      \"outputs\": \"boolean indicating if provider is ready to use\"\n    },\n    {\n      \"name\": \"getConfiguredProviders\",\n      \"desc\": \"Returns a list of all providers that are properly configured\",\n      \"inputs\": \"none\",\n      \"outputs\": \"Array of configured LLMProvider names\"\n    }\n  ],\n  \"dependencies\": [\n    \"./ILLMProvider\",\n    \"./openAIProvider\",\n    \"./anthropicProvider\",\n    \"../../config/configurationManager\"\n  ],\n  \"intent\": \"Centralizes AI provider creation and management to avoid duplicate instances, enable easy switching between providers, and provide a consistent interface for checking provider availability and configuration status\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T17:48:24.069Z"
  }
}