{
  "module": "src/ai/providers",
  "moduleType": "other",
  "capabilities": [
    "Provides unified interface for multiple AI language model providers (OpenAI and Anthropic Claude)",
    "Generates text responses from AI models based on user prompts",
    "Produces structured JSON outputs from AI models using defined schemas",
    "Automatically selects and initializes the configured AI provider",
    "Handles provider configuration validation and error reporting",
    "Enforces 5-minute timeout protection for long-running AI requests"
  ],
  "summary": "This module serves as the AI provider abstraction layer, enabling the application to integrate with multiple AI language model services through a unified interface. Users interact with AI capabilities without needing to know which provider (OpenAI or Claude) is being used behind the scenes. The module automatically instantiates the appropriate provider based on system configuration and API key availability.\n\nUsers can generate both free-form text responses and structured JSON outputs from AI models. When requesting AI assistance, prompts are routed through the configured provider, which handles the underlying API communication and response formatting. The module ensures reliable operation by validating provider configuration, managing timeouts, and providing clear error messages when API keys are missing or invalid.\n\nThe provider factory pattern allows seamless switching between AI services, giving system administrators flexibility in choosing their preferred AI backend while maintaining consistent functionality for end users. All AI interactions are protected by a 5-minute timeout to prevent indefinite waiting, and the system gracefully handles scenarios where providers are not properly configured.",
  "files": [
    {
      "file": "src/ai/providers/ILLMProvider.ts",
      "role": "Core Logic",
      "purpose": "Defines the contract for integrating different AI language model providers (OpenAI, Claude, etc.) into the system",
      "userVisibleActions": [
        "User receives text responses from AI models through any configured provider",
        "User receives structured JSON responses from AI models for data parsing",
        "User gets appropriate error handling when AI provider is not configured"
      ],
      "developerVisibleActions": [
        "Developer implements this interface to add support for new AI providers",
        "Developer sends text prompts with optional system instructions and conversation history",
        "Developer requests structured JSON output with optional schema validation",
        "Developer checks if a provider is properly configured before use",
        "Developer controls AI behavior via temperature and token limits",
        "Developer receives file and grep requests from AI responses for code exploration"
      ],
      "keyFunctions": [
        {
          "name": "isConfigured",
          "desc": "Checks if the AI provider has valid credentials and is ready to use",
          "inputs": "none",
          "outputs": "boolean indicating configuration status"
        },
        {
          "name": "sendRequest",
          "desc": "Sends a prompt with messages to the AI and gets back a text response",
          "inputs": "LLMRequestOptions with model, messages, system prompt, temperature, max tokens",
          "outputs": "LLMResponse with content string and metadata"
        },
        {
          "name": "sendStructuredRequest",
          "desc": "Sends a prompt and gets back parsed JSON data with optional follow-up requests",
          "inputs": "LLMRequestOptions and optional JSON schema for validation",
          "outputs": "StructuredOutputResponse with typed data and optional file/grep requests"
        },
        {
          "name": "getName",
          "desc": "Returns the identifier name of the provider",
          "inputs": "none",
          "outputs": "string with provider name"
        }
      ],
      "dependencies": [],
      "intent": "This interface exists to provide a unified abstraction layer over different AI language model providers (like OpenAI, Claude, or custom models), allowing the application to switch between providers or use multiple providers without changing the consuming code. It solves the problem of vendor lock-in and enables flexible AI provider selection while maintaining consistent request/response handling across all implementations.",
      "rawContent": "```json\n{\n  \"purpose\": \"Defines the contract for integrating different AI language model providers (OpenAI, Claude, etc.) into the system\",\n  \"userVisibleActions\": [\n    \"User receives text responses from AI models through any configured provider\",\n    \"User receives structured JSON responses from AI models for data parsing\",\n    \"User gets appropriate error handling when AI provider is not configured\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer implements this interface to add support for new AI providers\",\n    \"Developer sends text prompts with optional system instructions and conversation history\",\n    \"Developer requests structured JSON output with optional schema validation\",\n    \"Developer checks if a provider is properly configured before use\",\n    \"Developer controls AI behavior via temperature and token limits\",\n    \"Developer receives file and grep requests from AI responses for code exploration\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Checks if the AI provider has valid credentials and is ready to use\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating configuration status\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a prompt with messages to the AI and gets back a text response\",\n      \"inputs\": \"LLMRequestOptions with model, messages, system prompt, temperature, max tokens\",\n      \"outputs\": \"LLMResponse with content string and metadata\"\n    },\n    {\n      \"name\": \"sendStructuredRequest\",\n      \"desc\": \"Sends a prompt and gets back parsed JSON data with optional follow-up requests\",\n      \"inputs\": \"LLMRequestOptions and optional JSON schema for validation\",\n      \"outputs\": \"StructuredOutputResponse with typed data and optional file/grep requests\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the identifier name of the provider\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string with provider name\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"This interface exists to provide a unified abstraction layer over different AI language model providers (like OpenAI, Claude, or custom models), allowing the application to switch between providers or use multiple providers without changing the consuming code. It solves the problem of vendor lock-in and enables flexible AI provider selection while maintaining consistent request/response handling across all implementations.\"\n}\n```"
    },
    {
      "file": "src/ai/providers/anthropicProvider.ts",
      "role": "Core Logic",
      "purpose": "Provides integration with Anthropic's Claude AI models for text generation and structured output requests",
      "userVisibleActions": [
        "Sends prompts to Claude AI and receives text responses",
        "Generates structured JSON outputs from Claude based on schemas",
        "Receives error messages when Claude API is not configured",
        "Experiences 5-minute timeout for long-running requests"
      ],
      "developerVisibleActions": [
        "Configure Claude API key through configuration manager",
        "Send text generation requests with custom prompts and parameters",
        "Request structured JSON outputs by providing JSON schemas",
        "Receive responses with content and token usage information",
        "Handle provider availability checks before making requests",
        "Access Claude-specific models like claude-sonnet-4-5"
      ],
      "keyFunctions": [
        {
          "name": "isConfigured",
          "desc": "Checks if Claude API key is configured and provider is ready to use",
          "inputs": "none",
          "outputs": "boolean indicating configuration status"
        },
        {
          "name": "sendRequest",
          "desc": "Sends a text generation request to Claude with messages and system prompt",
          "inputs": "LLMRequestOptions (messages, model, systemPrompt, maxTokens)",
          "outputs": "LLMResponse with content and token usage"
        },
        {
          "name": "sendStructuredOutputRequest",
          "desc": "Requests Claude to generate output matching a specific JSON schema",
          "inputs": "LLMRequestOptions with jsonSchema",
          "outputs": "StructuredOutputResponse with parsed JSON object"
        },
        {
          "name": "getName",
          "desc": "Returns the provider identifier name",
          "inputs": "none",
          "outputs": "string 'claude'"
        },
        {
          "name": "initialize",
          "desc": "Sets up the Anthropic client with API key from configuration",
          "inputs": "none",
          "outputs": "void"
        }
      ],
      "dependencies": [
        "@anthropic-ai/sdk",
        "../../config/configurationManager",
        "../../utils/jsonExtractor",
        "./ILLMProvider"
      ],
      "intent": "This file exists to enable the application to use Anthropic's Claude AI models as an alternative to OpenAI, providing AI-powered text generation and structured output capabilities with Claude-specific message formatting and API integration",
      "rawContent": "```json\n{\n  \"purpose\": \"Provides integration with Anthropic's Claude AI models for text generation and structured output requests\",\n  \"userVisibleActions\": [\n    \"Sends prompts to Claude AI and receives text responses\",\n    \"Generates structured JSON outputs from Claude based on schemas\",\n    \"Receives error messages when Claude API is not configured\",\n    \"Experiences 5-minute timeout for long-running requests\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure Claude API key through configuration manager\",\n    \"Send text generation requests with custom prompts and parameters\",\n    \"Request structured JSON outputs by providing JSON schemas\",\n    \"Receive responses with content and token usage information\",\n    \"Handle provider availability checks before making requests\",\n    \"Access Claude-specific models like claude-sonnet-4-5\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Checks if Claude API key is configured and provider is ready to use\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating configuration status\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a text generation request to Claude with messages and system prompt\",\n      \"inputs\": \"LLMRequestOptions (messages, model, systemPrompt, maxTokens)\",\n      \"outputs\": \"LLMResponse with content and token usage\"\n    },\n    {\n      \"name\": \"sendStructuredOutputRequest\",\n      \"desc\": \"Requests Claude to generate output matching a specific JSON schema\",\n      \"inputs\": \"LLMRequestOptions with jsonSchema\",\n      \"outputs\": \"StructuredOutputResponse with parsed JSON object\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the provider identifier name\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string 'claude'\"\n    },\n    {\n      \"name\": \"initialize\",\n      \"desc\": \"Sets up the Anthropic client with API key from configuration\",\n      \"inputs\": \"none\",\n      \"outputs\": \"void\"\n    }\n  ],\n  \"dependencies\": [\n    \"@anthropic-ai/sdk\",\n    \"../../config/configurationManager\",\n    \"../../utils/jsonExtractor\",\n    \"./ILLMProvider\"\n  ],\n  \"intent\": \"This file exists to enable the application to use Anthropic's Claude AI models as an alternative to OpenAI, providing AI-powered text generation and structured output capabilities with Claude-specific message formatting and API integration\"\n}\n```"
    },
    {
      "file": "src/ai/providers/openAIProvider.ts",
      "role": "Core Logic",
      "purpose": "Provides integration with OpenAI's API to send chat completion requests and receive AI-generated responses",
      "userVisibleActions": [
        "Receives AI-generated text responses from OpenAI models (like GPT-4)",
        "Gets structured JSON responses when requesting formatted output",
        "Experiences timeout after 5 minutes if AI response takes too long",
        "Sees error messages when OpenAI API key is not configured"
      ],
      "developerVisibleActions": [
        "Configure OpenAI API key through configuration manager to enable the provider",
        "Send chat completion requests with system prompts and conversation messages",
        "Request structured JSON output by specifying response format",
        "Check if provider is configured before making requests",
        "Receive responses with content, finish reason, and token usage information",
        "Handle errors when API key is missing or requests fail"
      ],
      "keyFunctions": [
        {
          "name": "initialize",
          "desc": "Sets up the OpenAI client with API key from configuration",
          "inputs": "None",
          "outputs": "void"
        },
        {
          "name": "isConfigured",
          "desc": "Checks if the provider has a valid API key and is ready to use",
          "inputs": "None",
          "outputs": "boolean indicating configuration status"
        },
        {
          "name": "getName",
          "desc": "Returns the provider identifier",
          "inputs": "None",
          "outputs": "string 'openai'"
        },
        {
          "name": "sendRequest",
          "desc": "Sends a chat completion request to OpenAI and returns the response",
          "inputs": "LLMRequestOptions (model, messages, system prompt, response format)",
          "outputs": "Promise<LLMResponse> with content, finish reason, and usage"
        },
        {
          "name": "sendRequestStructured",
          "desc": "Sends a request expecting structured JSON output and parses the result",
          "inputs": "LLMRequestOptions with json_object response format",
          "outputs": "Promise<StructuredOutputResponse> with parsed JSON data and raw content"
        }
      ],
      "dependencies": [
        "openai",
        "../../config/configurationManager",
        "../../utils/jsonExtractor",
        "./ILLMProvider"
      ],
      "intent": "This file exists to abstract OpenAI's API behind a common provider interface, allowing the application to send AI requests and receive responses while handling configuration, error cases, and both text and structured JSON outputs consistently with other AI providers",
      "rawContent": "```json\n{\n  \"purpose\": \"Provides integration with OpenAI's API to send chat completion requests and receive AI-generated responses\",\n  \"userVisibleActions\": [\n    \"Receives AI-generated text responses from OpenAI models (like GPT-4)\",\n    \"Gets structured JSON responses when requesting formatted output\",\n    \"Experiences timeout after 5 minutes if AI response takes too long\",\n    \"Sees error messages when OpenAI API key is not configured\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure OpenAI API key through configuration manager to enable the provider\",\n    \"Send chat completion requests with system prompts and conversation messages\",\n    \"Request structured JSON output by specifying response format\",\n    \"Check if provider is configured before making requests\",\n    \"Receive responses with content, finish reason, and token usage information\",\n    \"Handle errors when API key is missing or requests fail\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"initialize\",\n      \"desc\": \"Sets up the OpenAI client with API key from configuration\",\n      \"inputs\": \"None\",\n      \"outputs\": \"void\"\n    },\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Checks if the provider has a valid API key and is ready to use\",\n      \"inputs\": \"None\",\n      \"outputs\": \"boolean indicating configuration status\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the provider identifier\",\n      \"inputs\": \"None\",\n      \"outputs\": \"string 'openai'\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a chat completion request to OpenAI and returns the response\",\n      \"inputs\": \"LLMRequestOptions (model, messages, system prompt, response format)\",\n      \"outputs\": \"Promise<LLMResponse> with content, finish reason, and usage\"\n    },\n    {\n      \"name\": \"sendRequestStructured\",\n      \"desc\": \"Sends a request expecting structured JSON output and parses the result\",\n      \"inputs\": \"LLMRequestOptions with json_object response format\",\n      \"outputs\": \"Promise<StructuredOutputResponse> with parsed JSON data and raw content\"\n    }\n  ],\n  \"dependencies\": [\n    \"openai\",\n    \"../../config/configurationManager\",\n    \"../../utils/jsonExtractor\",\n    \"./ILLMProvider\"\n  ],\n  \"intent\": \"This file exists to abstract OpenAI's API behind a common provider interface, allowing the application to send AI requests and receive responses while handling configuration, error cases, and both text and structured JSON outputs consistently with other AI providers\"\n}\n```"
    },
    {
      "file": "src/ai/providers/providerFactory.ts",
      "role": "Core Logic",
      "purpose": "Creates and manages AI language model provider instances (OpenAI and Claude) based on configuration",
      "userVisibleActions": [
        "AI responses come from the configured provider (OpenAI or Claude)",
        "System automatically uses the provider specified in settings",
        "AI features only work when at least one provider is properly configured with API keys"
      ],
      "developerVisibleActions": [
        "Request an AI provider instance by name ('openai' or 'claude')",
        "Get the currently configured provider without specifying which one",
        "Check if a specific provider has valid configuration/API keys",
        "Retrieve a list of all properly configured providers available for use",
        "Provider instances are reused (singleton pattern) to avoid recreating connections"
      ],
      "keyFunctions": [
        {
          "name": "getProvider",
          "desc": "Returns a provider instance for the specified AI service",
          "inputs": "provider type ('openai' or 'claude')",
          "outputs": "ILLMProvider instance for making AI requests"
        },
        {
          "name": "getCurrentProvider",
          "desc": "Returns the provider instance that matches the user's current configuration setting",
          "inputs": "none (reads from config)",
          "outputs": "ILLMProvider instance of the configured provider"
        },
        {
          "name": "isProviderConfigured",
          "desc": "Checks whether a specific provider has valid configuration and can be used",
          "inputs": "provider type ('openai' or 'claude')",
          "outputs": "boolean indicating if the provider is ready to use"
        },
        {
          "name": "getConfiguredProviders",
          "desc": "Returns a list of all providers that are properly configured and available",
          "inputs": "none",
          "outputs": "array of provider names that have valid configuration"
        }
      ],
      "dependencies": [
        "ILLMProvider",
        "OpenAIProvider",
        "AnthropicProvider",
        "configurationManager"
      ],
      "intent": "Centralizes AI provider creation and management so the rest of the application doesn't need to know which AI service is being used or handle provider instantiation - it acts as a single point of access for all AI providers and ensures only one instance of each provider exists throughout the application lifecycle",
      "rawContent": "```json\n{\n  \"purpose\": \"Creates and manages AI language model provider instances (OpenAI and Claude) based on configuration\",\n  \"userVisibleActions\": [\n    \"AI responses come from the configured provider (OpenAI or Claude)\",\n    \"System automatically uses the provider specified in settings\",\n    \"AI features only work when at least one provider is properly configured with API keys\"\n  ],\n  \"developerVisibleActions\": [\n    \"Request an AI provider instance by name ('openai' or 'claude')\",\n    \"Get the currently configured provider without specifying which one\",\n    \"Check if a specific provider has valid configuration/API keys\",\n    \"Retrieve a list of all properly configured providers available for use\",\n    \"Provider instances are reused (singleton pattern) to avoid recreating connections\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"getProvider\",\n      \"desc\": \"Returns a provider instance for the specified AI service\",\n      \"inputs\": \"provider type ('openai' or 'claude')\",\n      \"outputs\": \"ILLMProvider instance for making AI requests\"\n    },\n    {\n      \"name\": \"getCurrentProvider\",\n      \"desc\": \"Returns the provider instance that matches the user's current configuration setting\",\n      \"inputs\": \"none (reads from config)\",\n      \"outputs\": \"ILLMProvider instance of the configured provider\"\n    },\n    {\n      \"name\": \"isProviderConfigured\",\n      \"desc\": \"Checks whether a specific provider has valid configuration and can be used\",\n      \"inputs\": \"provider type ('openai' or 'claude')\",\n      \"outputs\": \"boolean indicating if the provider is ready to use\"\n    },\n    {\n      \"name\": \"getConfiguredProviders\",\n      \"desc\": \"Returns a list of all providers that are properly configured and available\",\n      \"inputs\": \"none\",\n      \"outputs\": \"array of provider names that have valid configuration\"\n    }\n  ],\n  \"dependencies\": [\n    \"ILLMProvider\",\n    \"OpenAIProvider\",\n    \"AnthropicProvider\",\n    \"configurationManager\"\n  ],\n  \"intent\": \"Centralizes AI provider creation and management so the rest of the application doesn't need to know which AI service is being used or handle provider instantiation - it acts as a single point of access for all AI providers and ensures only one instance of each provider exists throughout the application lifecycle\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T04:24:57.413Z"
  }
}