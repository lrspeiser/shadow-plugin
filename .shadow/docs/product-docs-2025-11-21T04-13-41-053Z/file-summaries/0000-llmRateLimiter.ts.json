{
  "file": "src/ai/llmRateLimiter.ts",
  "role": "Core Logic",
  "purpose": "Manages rate limiting for LLM API requests to prevent exceeding provider rate limits",
  "userVisibleActions": [
    "User's AI requests are automatically throttled to prevent API rate limit errors",
    "User experiences smooth AI interactions without hitting rate limit errors from OpenAI or Claude",
    "User's rapid successive AI requests are controlled to stay within provider limits"
  ],
  "developerVisibleActions": [
    "Developer can check if an AI request is allowed before making it to avoid rate limit errors",
    "Developer records each AI API request to track usage against rate limits",
    "Developer can configure custom rate limits for OpenAI (default: 60 requests/minute) and Claude (default: 50 requests/minute)",
    "System automatically cleans up old request history outside the time window"
  ],
  "keyFunctions": [
    {
      "name": "constructor",
      "desc": "Initializes rate limiter with default limits for OpenAI (60 req/min) and Claude (50 req/min)",
      "inputs": "none",
      "outputs": "RateLimiter instance"
    },
    {
      "name": "configure",
      "desc": "Sets custom rate limit configuration for a specific LLM provider",
      "inputs": "provider (openai or claude), config (maxRequests and windowMs)",
      "outputs": "void"
    },
    {
      "name": "canMakeRequest",
      "desc": "Checks if a new request can be made without exceeding the rate limit",
      "inputs": "provider (openai or claude)",
      "outputs": "boolean (true if request allowed, false if rate limited)"
    },
    {
      "name": "recordRequest",
      "desc": "Records timestamp of a completed request for rate limit tracking",
      "inputs": "provider (openai or claude)",
      "outputs": "void"
    }
  ],
  "dependencies": [],
  "intent": "Prevents the application from exceeding LLM provider API rate limits by tracking request frequency per provider, protecting against rate limit errors and potential API access suspension",
  "rawContent": "```json\n{\n  \"purpose\": \"Manages rate limiting for LLM API requests to prevent exceeding provider rate limits\",\n  \"userVisibleActions\": [\n    \"User's AI requests are automatically throttled to prevent API rate limit errors\",\n    \"User experiences smooth AI interactions without hitting rate limit errors from OpenAI or Claude\",\n    \"User's rapid successive AI requests are controlled to stay within provider limits\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer can check if an AI request is allowed before making it to avoid rate limit errors\",\n    \"Developer records each AI API request to track usage against rate limits\",\n    \"Developer can configure custom rate limits for OpenAI (default: 60 requests/minute) and Claude (default: 50 requests/minute)\",\n    \"System automatically cleans up old request history outside the time window\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"constructor\",\n      \"desc\": \"Initializes rate limiter with default limits for OpenAI (60 req/min) and Claude (50 req/min)\",\n      \"inputs\": \"none\",\n      \"outputs\": \"RateLimiter instance\"\n    },\n    {\n      \"name\": \"configure\",\n      \"desc\": \"Sets custom rate limit configuration for a specific LLM provider\",\n      \"inputs\": \"provider (openai or claude), config (maxRequests and windowMs)\",\n      \"outputs\": \"void\"\n    },\n    {\n      \"name\": \"canMakeRequest\",\n      \"desc\": \"Checks if a new request can be made without exceeding the rate limit\",\n      \"inputs\": \"provider (openai or claude)\",\n      \"outputs\": \"boolean (true if request allowed, false if rate limited)\"\n    },\n    {\n      \"name\": \"recordRequest\",\n      \"desc\": \"Records timestamp of a completed request for rate limit tracking\",\n      \"inputs\": \"provider (openai or claude)\",\n      \"outputs\": \"void\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Prevents the application from exceeding LLM provider API rate limits by tracking request frequency per provider, protecting against rate limit errors and potential API access suspension\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T04:13:51.112Z"
  }
}