{
  "module": "src/ai",
  "moduleType": "other",
  "capabilities": [
    "Automatic rate limiting for LLM API requests across multiple providers (OpenAI, Claude)",
    "Intelligent retry handling with exponential backoff for failed API requests",
    "Structured parsing of AI-generated documentation into typed objects",
    "Transparent error recovery without user intervention for temporary failures",
    "Provider-specific rate limit enforcement (OpenAI: 60 requests/min, Claude: 50 requests/min)",
    "Fallback text parsing when AI returns non-JSON responses"
  ],
  "summary": "This module provides the core infrastructure for reliable and efficient communication with LLM providers. It ensures that all AI requests are automatically throttled to stay within API quotas, handles transient failures gracefully through automatic retries, and transforms raw AI responses into structured, typed documentation objects.\n\nWhen users trigger AI-powered documentation generation, this module works behind the scenes to manage the request lifecycle. Rate limiting prevents API quota exhaustion by enforcing provider-specific request limits, while the retry handler automatically recovers from temporary failures like rate limit errors or timeouts using exponential backoff. The response parser then converts AI-generated text into organized documentation structures for file summaries, module summaries, insights, and product documentation.\n\nThe user experience is seamless: requests are automatically queued when rate limits are approached, failures are retried transparently, and only non-recoverable errors surface to the user after all retry attempts are exhausted. This creates a reliable AI documentation pipeline that handles the complexities of API management, allowing users to focus on generating high-quality documentation without worrying about API limits or transient errors.",
  "files": [
    {
      "file": "src/ai/llmRateLimiter.ts",
      "role": "Core Logic",
      "purpose": "Manages rate limiting for LLM API requests to prevent exceeding provider API quotas",
      "userVisibleActions": [
        "API requests to OpenAI and Claude are automatically throttled to stay within rate limits",
        "Requests that would exceed rate limits are prevented from being sent",
        "Different rate limits are enforced for different LLM providers (OpenAI: 60/min, Claude: 50/min)"
      ],
      "developerVisibleActions": [
        "Check if an API request is allowed before making it using canMakeRequest()",
        "Record successful API requests with recordRequest() to track usage",
        "Configure custom rate limits for providers using configure()",
        "Track request history per provider automatically",
        "Get automatic cleanup of old request timestamps outside the time window"
      ],
      "keyFunctions": [
        {
          "name": "constructor",
          "desc": "Initializes rate limiter with default limits for OpenAI (60/min) and Claude (50/min)",
          "inputs": "none",
          "outputs": "RateLimiter instance"
        },
        {
          "name": "configure",
          "desc": "Sets custom rate limit configuration for a specific LLM provider",
          "inputs": "provider (LLMProvider), config (RateLimitConfig with maxRequests and windowMs)",
          "outputs": "void"
        },
        {
          "name": "canMakeRequest",
          "desc": "Checks if a new request is allowed based on current rate limit status",
          "inputs": "provider (LLMProvider)",
          "outputs": "boolean - true if request can be made, false if rate limit would be exceeded"
        },
        {
          "name": "recordRequest",
          "desc": "Records a request timestamp for rate limiting tracking",
          "inputs": "provider (LLMProvider)",
          "outputs": "void"
        }
      ],
      "dependencies": [],
      "intent": "This file exists to protect against exceeding API rate limits from LLM providers (OpenAI and Claude) by tracking request frequency and enforcing configurable throttling windows, preventing API errors and potential service disruptions",
      "rawContent": "```json\n{\n  \"purpose\": \"Manages rate limiting for LLM API requests to prevent exceeding provider API quotas\",\n  \"userVisibleActions\": [\n    \"API requests to OpenAI and Claude are automatically throttled to stay within rate limits\",\n    \"Requests that would exceed rate limits are prevented from being sent\",\n    \"Different rate limits are enforced for different LLM providers (OpenAI: 60/min, Claude: 50/min)\"\n  ],\n  \"developerVisibleActions\": [\n    \"Check if an API request is allowed before making it using canMakeRequest()\",\n    \"Record successful API requests with recordRequest() to track usage\",\n    \"Configure custom rate limits for providers using configure()\",\n    \"Track request history per provider automatically\",\n    \"Get automatic cleanup of old request timestamps outside the time window\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"constructor\",\n      \"desc\": \"Initializes rate limiter with default limits for OpenAI (60/min) and Claude (50/min)\",\n      \"inputs\": \"none\",\n      \"outputs\": \"RateLimiter instance\"\n    },\n    {\n      \"name\": \"configure\",\n      \"desc\": \"Sets custom rate limit configuration for a specific LLM provider\",\n      \"inputs\": \"provider (LLMProvider), config (RateLimitConfig with maxRequests and windowMs)\",\n      \"outputs\": \"void\"\n    },\n    {\n      \"name\": \"canMakeRequest\",\n      \"desc\": \"Checks if a new request is allowed based on current rate limit status\",\n      \"inputs\": \"provider (LLMProvider)\",\n      \"outputs\": \"boolean - true if request can be made, false if rate limit would be exceeded\"\n    },\n    {\n      \"name\": \"recordRequest\",\n      \"desc\": \"Records a request timestamp for rate limiting tracking\",\n      \"inputs\": \"provider (LLMProvider)\",\n      \"outputs\": \"void\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"This file exists to protect against exceeding API rate limits from LLM providers (OpenAI and Claude) by tracking request frequency and enforcing configurable throttling windows, preventing API errors and potential service disruptions\"\n}\n```"
    },
    {
      "file": "src/ai/llmResponseParser.ts",
      "role": "Core Logic",
      "purpose": "Parses and extracts structured data from LLM text responses into typed objects for different analysis types (file summaries, module summaries, insights, and product documentation).",
      "userVisibleActions": [
        "Converts raw AI-generated text responses into organized documentation",
        "Extracts file purposes and actions from AI analysis",
        "Generates structured module and product documentation from AI responses",
        "Provides fallback text parsing when AI returns non-JSON responses"
      ],
      "developerVisibleActions": [
        "Call parseFileSummary() to convert LLM response into FileSummary object",
        "Call parseModuleSummary() to extract module-level documentation from LLM text",
        "Call parseInsights() to structure AI-generated insights about codebase",
        "Call parseProductDocumentation() to build comprehensive product docs from LLM output",
        "Call parsePurposeAnalysis() to extract product purpose analysis",
        "Handles both JSON and plain text LLM response formats automatically",
        "Returns typed TypeScript objects with fallback values for missing data"
      ],
      "keyFunctions": [
        {
          "name": "parseFileSummary",
          "desc": "Converts LLM response text into a FileSummary object with purpose, actions, and dependencies",
          "inputs": "content (LLM response text), filePath (string), role (string)",
          "outputs": "FileSummary object"
        },
        {
          "name": "parseModuleSummary",
          "desc": "Extracts module-level documentation including purpose, features, and structure from LLM response",
          "inputs": "content (LLM response text), moduleName (string)",
          "outputs": "ModuleSummary object"
        },
        {
          "name": "parseInsights",
          "desc": "Structures AI insights about the codebase into categories like architecture patterns and improvement areas",
          "inputs": "content (LLM response text), context (AnalysisContext)",
          "outputs": "LLMInsights object"
        },
        {
          "name": "parseProductDocumentation",
          "desc": "Builds comprehensive product documentation from LLM analysis including overview, features, and use cases",
          "inputs": "content (LLM response text)",
          "outputs": "EnhancedProductDocumentation object"
        },
        {
          "name": "parsePurposeAnalysis",
          "desc": "Extracts product purpose analysis including main purpose, target audience, and value proposition",
          "inputs": "content (LLM response text)",
          "outputs": "ProductPurposeAnalysis object"
        },
        {
          "name": "extractSection",
          "desc": "Helper that pulls out a specific labeled section from unstructured text",
          "inputs": "content (text), sectionName (string)",
          "outputs": "Extracted section text or empty string"
        },
        {
          "name": "extractListSection",
          "desc": "Helper that extracts bullet points or list items from a text section",
          "inputs": "content (text), sectionName (string)",
          "outputs": "Array of list items"
        }
      ],
      "dependencies": [
        "../fileDocumentation (FileSummary, ModuleSummary, EnhancedProductDocumentation)",
        "../llmService (LLMInsights, ProductPurposeAnalysis, AnalysisContext)"
      ],
      "intent": "This file exists to bridge the gap between unstructured LLM text responses and the structured TypeScript types needed by the application. It handles the unpredictability of AI responses by supporting both JSON and plain text formats, ensuring the rest of the application always receives properly typed data objects regardless of how the LLM formatted its response.",
      "rawContent": "```json\n{\n  \"purpose\": \"Parses and extracts structured data from LLM text responses into typed objects for different analysis types (file summaries, module summaries, insights, and product documentation).\",\n  \"userVisibleActions\": [\n    \"Converts raw AI-generated text responses into organized documentation\",\n    \"Extracts file purposes and actions from AI analysis\",\n    \"Generates structured module and product documentation from AI responses\",\n    \"Provides fallback text parsing when AI returns non-JSON responses\"\n  ],\n  \"developerVisibleActions\": [\n    \"Call parseFileSummary() to convert LLM response into FileSummary object\",\n    \"Call parseModuleSummary() to extract module-level documentation from LLM text\",\n    \"Call parseInsights() to structure AI-generated insights about codebase\",\n    \"Call parseProductDocumentation() to build comprehensive product docs from LLM output\",\n    \"Call parsePurposeAnalysis() to extract product purpose analysis\",\n    \"Handles both JSON and plain text LLM response formats automatically\",\n    \"Returns typed TypeScript objects with fallback values for missing data\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"parseFileSummary\",\n      \"desc\": \"Converts LLM response text into a FileSummary object with purpose, actions, and dependencies\",\n      \"inputs\": \"content (LLM response text), filePath (string), role (string)\",\n      \"outputs\": \"FileSummary object\"\n    },\n    {\n      \"name\": \"parseModuleSummary\",\n      \"desc\": \"Extracts module-level documentation including purpose, features, and structure from LLM response\",\n      \"inputs\": \"content (LLM response text), moduleName (string)\",\n      \"outputs\": \"ModuleSummary object\"\n    },\n    {\n      \"name\": \"parseInsights\",\n      \"desc\": \"Structures AI insights about the codebase into categories like architecture patterns and improvement areas\",\n      \"inputs\": \"content (LLM response text), context (AnalysisContext)\",\n      \"outputs\": \"LLMInsights object\"\n    },\n    {\n      \"name\": \"parseProductDocumentation\",\n      \"desc\": \"Builds comprehensive product documentation from LLM analysis including overview, features, and use cases\",\n      \"inputs\": \"content (LLM response text)\",\n      \"outputs\": \"EnhancedProductDocumentation object\"\n    },\n    {\n      \"name\": \"parsePurposeAnalysis\",\n      \"desc\": \"Extracts product purpose analysis including main purpose, target audience, and value proposition\",\n      \"inputs\": \"content (LLM response text)\",\n      \"outputs\": \"ProductPurposeAnalysis object\"\n    },\n    {\n      \"name\": \"extractSection\",\n      \"desc\": \"Helper that pulls out a specific labeled section from unstructured text\",\n      \"inputs\": \"content (text), sectionName (string)\",\n      \"outputs\": \"Extracted section text or empty string\"\n    },\n    {\n      \"name\": \"extractListSection\",\n      \"desc\": \"Helper that extracts bullet points or list items from a text section\",\n      \"inputs\": \"content (text), sectionName (string)\",\n      \"outputs\": \"Array of list items\"\n    }\n  ],\n  \"dependencies\": [\n    \"../fileDocumentation (FileSummary, ModuleSummary, EnhancedProductDocumentation)\",\n    \"../llmService (LLMInsights, ProductPurposeAnalysis, AnalysisContext)\"\n  ],\n  \"intent\": \"This file exists to bridge the gap between unstructured LLM text responses and the structured TypeScript types needed by the application. It handles the unpredictability of AI responses by supporting both JSON and plain text formats, ensuring the rest of the application always receives properly typed data objects regardless of how the LLM formatted its response.\"\n}\n```"
    },
    {
      "file": "src/ai/llmRetryHandler.ts",
      "role": "Core Logic",
      "purpose": "Handles automatic retry logic for LLM API requests with exponential backoff when requests fail due to rate limits, timeouts, or temporary errors",
      "userVisibleActions": [
        "Automatic retry when AI requests fail due to rate limits or temporary issues",
        "Delayed retry attempts that increase wait time between retries",
        "Transparent error recovery without user intervention for recoverable errors",
        "Final error display only after all retry attempts are exhausted"
      ],
      "developerVisibleActions": [
        "Configure maximum number of retry attempts for LLM requests",
        "Set initial delay and maximum delay between retries",
        "Define which error types should trigger automatic retries",
        "Receive callbacks on each retry attempt for logging or monitoring",
        "Execute any async LLM operation with automatic retry handling",
        "Get result with attempt count after successful retry",
        "Handle non-retryable errors immediately without retry attempts"
      ],
      "keyFunctions": [
        {
          "name": "executeWithRetry",
          "desc": "Executes an LLM operation with automatic retry on failure",
          "inputs": "operation (async function to execute), options (retry configuration including maxRetries, delays, retryable error types, callback)",
          "outputs": "Promise resolving to the operation result, or throws error after all retries exhausted"
        },
        {
          "name": "isRetryableError",
          "desc": "Determines if an error should trigger a retry attempt",
          "inputs": "error object, list of retryable error patterns",
          "outputs": "Boolean indicating whether the error is retryable"
        }
      ],
      "dependencies": [],
      "intent": "Provides resilient LLM API communication by automatically handling transient failures like rate limits, network issues, and temporary service outages, preventing user-facing errors for recoverable problems while using exponential backoff to avoid overwhelming services",
      "rawContent": "```json\n{\n  \"purpose\": \"Handles automatic retry logic for LLM API requests with exponential backoff when requests fail due to rate limits, timeouts, or temporary errors\",\n  \"userVisibleActions\": [\n    \"Automatic retry when AI requests fail due to rate limits or temporary issues\",\n    \"Delayed retry attempts that increase wait time between retries\",\n    \"Transparent error recovery without user intervention for recoverable errors\",\n    \"Final error display only after all retry attempts are exhausted\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure maximum number of retry attempts for LLM requests\",\n    \"Set initial delay and maximum delay between retries\",\n    \"Define which error types should trigger automatic retries\",\n    \"Receive callbacks on each retry attempt for logging or monitoring\",\n    \"Execute any async LLM operation with automatic retry handling\",\n    \"Get result with attempt count after successful retry\",\n    \"Handle non-retryable errors immediately without retry attempts\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"executeWithRetry\",\n      \"desc\": \"Executes an LLM operation with automatic retry on failure\",\n      \"inputs\": \"operation (async function to execute), options (retry configuration including maxRetries, delays, retryable error types, callback)\",\n      \"outputs\": \"Promise resolving to the operation result, or throws error after all retries exhausted\"\n    },\n    {\n      \"name\": \"isRetryableError\",\n      \"desc\": \"Determines if an error should trigger a retry attempt\",\n      \"inputs\": \"error object, list of retryable error patterns\",\n      \"outputs\": \"Boolean indicating whether the error is retryable\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Provides resilient LLM API communication by automatically handling transient failures like rate limits, network issues, and temporary service outages, preventing user-facing errors for recoverable problems while using exponential backoff to avoid overwhelming services\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-20T06:37:52.766Z"
  }
}