{
  "module": "src/domain/services/testing",
  "moduleType": "tests",
  "capabilities": [
    "Automatically generates AI-powered tests for code functions in the workspace",
    "Creates prioritized test plans that determine which functions need testing and in what order",
    "Detects existing test framework configuration (Jest, pytest, JUnit, etc.) and generates appropriate test setups",
    "Executes tests and captures detailed results including pass/fail status, error messages, and coverage metrics",
    "Automatically fixes failing tests through iterative LLM-powered refinement",
    "Provides real-time progress updates during test generation, execution, and validation",
    "Generates comprehensive test reports with statistics on coverage, success rates, and identified issues"
  ],
  "summary": "This module provides an AI-powered automated testing system that generates, executes, and validates tests for code in the workspace. It analyzes the codebase to create intelligent test plans, prioritizing which functions need testing based on complexity, risk, and existing coverage. The system automatically detects the test framework being used (Jest, Mocha, pytest, JUnit, etc.) and generates tests that follow the project's conventions and structure.\n\nThe testing workflow operates in stages: first analyzing functions to determine testability, then generating tests in small batches for incremental validation, executing those tests to capture results, and automatically fixing any failures through LLM-powered iteration. Users receive continuous progress updates showing which function is being tested (e.g., '3 of 10: validateUser'), along with real-time pass/fail results and detailed error messages for any failures.\n\nThe module handles the complete testing lifecycle from setup to reporting, including detecting missing test infrastructure, generating appropriate test directory structures, executing test suites with timeout handling, and producing comprehensive coverage reports. It supports multiple testing frameworks and programming languages, adapting its test generation strategy to match the project's existing patterns and conventions. Failed tests are automatically refined through multiple fix attempts, with each iteration learning from previous failures to improve test quality.",
  "files": [
    {
      "file": "src/domain/services/testing/llmTestGenerationService.ts",
      "role": "Core Logic",
      "purpose": "Generates automated tests for code functions using AI/LLM in small batches, executing and validating them incrementally.",
      "userVisibleActions": [
        "Tests are generated automatically for selected functions in the codebase",
        "Progress updates show which function is currently being tested (e.g., '3 of 10: validateUser')",
        "Test results display pass/fail status for each generated test",
        "Coverage reports show which functions have tests and which need them",
        "Failed tests show error messages and execution details"
      ],
      "developerVisibleActions": [
        "Developer triggers test generation for a batch of functions",
        "System reads function source code and existing mocks automatically",
        "LLM generates test code based on function signatures and implementation",
        "Generated tests are written to the test directory structure",
        "Tests are executed immediately to verify they work",
        "Progress callback provides real-time updates during generation",
        "Results include test code, execution status, and any errors encountered"
      ],
      "keyFunctions": [
        {
          "name": "generateTestBatch",
          "desc": "Generates tests for multiple functions in sequence with progress tracking",
          "inputs": "functions array, workspace root path, LLM service, optional progress callback",
          "outputs": "Map of function names to test generation results (code, execution status, errors)"
        },
        {
          "name": "extractFunctionSource",
          "desc": "Retrieves the source code for a specific function from the file system",
          "inputs": "function metadata, workspace root",
          "outputs": "Source code string"
        },
        {
          "name": "buildGenerationPrompt",
          "desc": "Constructs the AI prompt with function details, source code, and existing mocks",
          "inputs": "function metadata, source code, test framework type, existing mock code",
          "outputs": "Formatted prompt string for LLM"
        },
        {
          "name": "generateTestForFunction",
          "desc": "Calls LLM service to generate test code for a single function",
          "inputs": "Prompt string",
          "outputs": "Test generation result with code and metadata"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestableFunction",
        "TestGenerationState",
        "TestGenerationResult",
        "buildGenerationPrompt",
        "TestExecutionService",
        "SWLogger"
      ],
      "intent": "This service solves the problem of manually writing tests by automating test generation using AI. It processes functions in manageable batches to avoid overwhelming the system, provides real-time progress feedback, and validates generated tests by executing them immediately. This enables developers to quickly achieve test coverage across their codebase.",
      "rawContent": "```json\n{\n  \"purpose\": \"Generates automated tests for code functions using AI/LLM in small batches, executing and validating them incrementally.\",\n  \"userVisibleActions\": [\n    \"Tests are generated automatically for selected functions in the codebase\",\n    \"Progress updates show which function is currently being tested (e.g., '3 of 10: validateUser')\",\n    \"Test results display pass/fail status for each generated test\",\n    \"Coverage reports show which functions have tests and which need them\",\n    \"Failed tests show error messages and execution details\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer triggers test generation for a batch of functions\",\n    \"System reads function source code and existing mocks automatically\",\n    \"LLM generates test code based on function signatures and implementation\",\n    \"Generated tests are written to the test directory structure\",\n    \"Tests are executed immediately to verify they work\",\n    \"Progress callback provides real-time updates during generation\",\n    \"Results include test code, execution status, and any errors encountered\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"generateTestBatch\",\n      \"desc\": \"Generates tests for multiple functions in sequence with progress tracking\",\n      \"inputs\": \"functions array, workspace root path, LLM service, optional progress callback\",\n      \"outputs\": \"Map of function names to test generation results (code, execution status, errors)\"\n    },\n    {\n      \"name\": \"extractFunctionSource\",\n      \"desc\": \"Retrieves the source code for a specific function from the file system\",\n      \"inputs\": \"function metadata, workspace root\",\n      \"outputs\": \"Source code string\"\n    },\n    {\n      \"name\": \"buildGenerationPrompt\",\n      \"desc\": \"Constructs the AI prompt with function details, source code, and existing mocks\",\n      \"inputs\": \"function metadata, source code, test framework type, existing mock code\",\n      \"outputs\": \"Formatted prompt string for LLM\"\n    },\n    {\n      \"name\": \"generateTestForFunction\",\n      \"desc\": \"Calls LLM service to generate test code for a single function\",\n      \"inputs\": \"Prompt string\",\n      \"outputs\": \"Test generation result with code and metadata\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestableFunction\",\n    \"TestGenerationState\",\n    \"TestGenerationResult\",\n    \"buildGenerationPrompt\",\n    \"TestExecutionService\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This service solves the problem of manually writing tests by automating test generation using AI. It processes functions in manageable batches to avoid overwhelming the system, provides real-time progress feedback, and validates generated tests by executing them immediately. This enables developers to quickly achieve test coverage across their codebase.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestPlanningService.ts",
      "role": "Core Logic",
      "purpose": "Creates AI-powered prioritized test plans by analyzing code functions and determining which ones need testing and in what order.",
      "userVisibleActions": [
        "System analyzes code functions and determines which ones can be tested",
        "System generates a prioritized test plan showing which functions to test first",
        "System explains why certain functions are testable or not testable",
        "Test plan is saved to disk for review and implementation"
      ],
      "developerVisibleActions": [
        "Developer triggers test plan creation for analyzed code",
        "System analyzes function complexity, parameters, and return types",
        "LLM evaluates functions against product documentation and architecture insights",
        "System categorizes functions into priority groups (high, medium, low priority)",
        "Test plan with function groups and testing recommendations is generated",
        "Plan is saved as JSON file in output directory for developer review"
      ],
      "keyFunctions": [
        {
          "name": "analyzeFunctions",
          "desc": "Extracts function metadata from code analysis including name, location, complexity, and signature",
          "inputs": "codeAnalysis object containing function data",
          "outputs": "Array of function metadata objects"
        },
        {
          "name": "createTestPlan",
          "desc": "Uses LLM to generate prioritized test strategy based on functions, documentation, and architecture",
          "inputs": "context, functions array, llmService, optional productDocs and architectureInsights",
          "outputs": "TestPlan object with function groups and testing recommendations"
        },
        {
          "name": "saveTestPlan",
          "desc": "Persists the generated test plan to disk as a JSON file",
          "inputs": "testPlan object, output directory path",
          "outputs": "File path where plan was saved"
        }
      ],
      "dependencies": [
        "fs (file system operations)",
        "path (file path handling)",
        "TestPlan and TestableFunction types",
        "buildPlanningPrompt from testPrompts",
        "AnalysisContext from analyzer",
        "SWLogger for logging",
        "LLM service for AI-powered analysis"
      ],
      "intent": "This file solves the problem of determining which code functions should be tested and in what order. Instead of manually reviewing code to decide testing priorities, it uses AI to analyze function complexity, dependencies, and business importance to create an intelligent, prioritized test plan that guides developers on where to focus testing efforts first.",
      "rawContent": "```json\n{\n  \"purpose\": \"Creates AI-powered prioritized test plans by analyzing code functions and determining which ones need testing and in what order.\",\n  \"userVisibleActions\": [\n    \"System analyzes code functions and determines which ones can be tested\",\n    \"System generates a prioritized test plan showing which functions to test first\",\n    \"System explains why certain functions are testable or not testable\",\n    \"Test plan is saved to disk for review and implementation\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer triggers test plan creation for analyzed code\",\n    \"System analyzes function complexity, parameters, and return types\",\n    \"LLM evaluates functions against product documentation and architecture insights\",\n    \"System categorizes functions into priority groups (high, medium, low priority)\",\n    \"Test plan with function groups and testing recommendations is generated\",\n    \"Plan is saved as JSON file in output directory for developer review\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"analyzeFunctions\",\n      \"desc\": \"Extracts function metadata from code analysis including name, location, complexity, and signature\",\n      \"inputs\": \"codeAnalysis object containing function data\",\n      \"outputs\": \"Array of function metadata objects\"\n    },\n    {\n      \"name\": \"createTestPlan\",\n      \"desc\": \"Uses LLM to generate prioritized test strategy based on functions, documentation, and architecture\",\n      \"inputs\": \"context, functions array, llmService, optional productDocs and architectureInsights\",\n      \"outputs\": \"TestPlan object with function groups and testing recommendations\"\n    },\n    {\n      \"name\": \"saveTestPlan\",\n      \"desc\": \"Persists the generated test plan to disk as a JSON file\",\n      \"inputs\": \"testPlan object, output directory path\",\n      \"outputs\": \"File path where plan was saved\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs (file system operations)\",\n    \"path (file path handling)\",\n    \"TestPlan and TestableFunction types\",\n    \"buildPlanningPrompt from testPrompts\",\n    \"AnalysisContext from analyzer\",\n    \"SWLogger for logging\",\n    \"LLM service for AI-powered analysis\"\n  ],\n  \"intent\": \"This file solves the problem of determining which code functions should be tested and in what order. Instead of manually reviewing code to decide testing priorities, it uses AI to analyze function complexity, dependencies, and business importance to create an intelligent, prioritized test plan that guides developers on where to focus testing efforts first.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestSetupService.ts",
      "role": "Core Logic",
      "purpose": "Detects test environment configuration and generates test setup plans using LLM analysis of the codebase structure",
      "userVisibleActions": [
        "Automatic detection of test framework (Jest, pytest, JUnit, etc.) in the workspace",
        "Identification of primary programming language used in the project",
        "Generation of test setup configuration based on existing project structure",
        "Detection of missing test infrastructure components",
        "Automated test directory structure recognition"
      ],
      "developerVisibleActions": [
        "Scans workspace for package.json, tsconfig.json, jest.config.js and test directories",
        "Analyzes file extensions to determine primary language (TypeScript, JavaScript, Python, Java, C++)",
        "Counts file types across the workspace to identify language distribution",
        "Detects installed testing frameworks and dependencies from package.json",
        "Builds prompts for LLM to generate test setup recommendations",
        "Returns TestEnvironment object containing detected configuration",
        "Provides TestSetupPlan with recommended setup steps"
      ],
      "keyFunctions": [
        {
          "name": "detectTestEnvironment",
          "desc": "Analyzes workspace to identify testing framework, language, and existing test infrastructure",
          "inputs": "workspaceRoot: string (path to project root)",
          "outputs": "TestEnvironment object with detected configuration details"
        },
        {
          "name": "getAllFiles",
          "desc": "Recursively scans workspace to get all file paths for analysis",
          "inputs": "workspaceRoot: string",
          "outputs": "Array of file paths"
        },
        {
          "name": "buildSetupPrompt",
          "desc": "Creates LLM prompt for generating test setup recommendations",
          "inputs": "TestEnvironment data",
          "outputs": "Formatted prompt string for LLM"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "child_process",
        "testSetupTypes",
        "testPrompts",
        "SWLogger"
      ],
      "intent": "This service exists to automate the initial test setup process by intelligently detecting the current project's testing environment and generating appropriate configuration recommendations, eliminating manual setup work and reducing configuration errors for developers starting with testing.",
      "rawContent": "```json\n{\n  \"purpose\": \"Detects test environment configuration and generates test setup plans using LLM analysis of the codebase structure\",\n  \"userVisibleActions\": [\n    \"Automatic detection of test framework (Jest, pytest, JUnit, etc.) in the workspace\",\n    \"Identification of primary programming language used in the project\",\n    \"Generation of test setup configuration based on existing project structure\",\n    \"Detection of missing test infrastructure components\",\n    \"Automated test directory structure recognition\"\n  ],\n  \"developerVisibleActions\": [\n    \"Scans workspace for package.json, tsconfig.json, jest.config.js and test directories\",\n    \"Analyzes file extensions to determine primary language (TypeScript, JavaScript, Python, Java, C++)\",\n    \"Counts file types across the workspace to identify language distribution\",\n    \"Detects installed testing frameworks and dependencies from package.json\",\n    \"Builds prompts for LLM to generate test setup recommendations\",\n    \"Returns TestEnvironment object containing detected configuration\",\n    \"Provides TestSetupPlan with recommended setup steps\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"detectTestEnvironment\",\n      \"desc\": \"Analyzes workspace to identify testing framework, language, and existing test infrastructure\",\n      \"inputs\": \"workspaceRoot: string (path to project root)\",\n      \"outputs\": \"TestEnvironment object with detected configuration details\"\n    },\n    {\n      \"name\": \"getAllFiles\",\n      \"desc\": \"Recursively scans workspace to get all file paths for analysis\",\n      \"inputs\": \"workspaceRoot: string\",\n      \"outputs\": \"Array of file paths\"\n    },\n    {\n      \"name\": \"buildSetupPrompt\",\n      \"desc\": \"Creates LLM prompt for generating test setup recommendations\",\n      \"inputs\": \"TestEnvironment data\",\n      \"outputs\": \"Formatted prompt string for LLM\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"child_process\",\n    \"testSetupTypes\",\n    \"testPrompts\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This service exists to automate the initial test setup process by intelligently detecting the current project's testing environment and generating appropriate configuration recommendations, eliminating manual setup work and reducing configuration errors for developers starting with testing.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestValidationService.ts",
      "role": "Core Logic",
      "purpose": "Validates tests by running them, detecting failures, and automatically fixing failing tests using an LLM.",
      "userVisibleActions": [
        "Tests are automatically executed in the workspace",
        "Test results are displayed showing passed, failed, and total test counts",
        "Failing tests are automatically fixed through multiple attempts",
        "Progress updates are shown for each fix attempt",
        "Final test validation reports are generated with success/failure statistics"
      ],
      "developerVisibleActions": [
        "Trigger test execution for entire workspace or specific test files",
        "Initiate automatic fixing of failing tests with configurable retry attempts",
        "Receive structured test execution results with pass/fail counts per file",
        "Get detailed test reports including failure messages and stack traces",
        "Access test report summaries with overall statistics",
        "Monitor fix attempt progress through logged messages"
      ],
      "keyFunctions": [
        {
          "name": "runTests",
          "desc": "Executes all tests or tests in a specific file and captures results",
          "inputs": "workspaceRoot (string), optional testFile (string)",
          "outputs": "Promise<TestExecutionResult[]> - array of test results per file"
        },
        {
          "name": "fixFailingTest",
          "desc": "Attempts to automatically fix a failing test using LLM with retry logic",
          "inputs": "testFilePath (string), executionResult (TestExecutionResult), workspaceRoot (string), llmService (any), optional maxAttempts (number, default 3)",
          "outputs": "Promise<{success: boolean, attempts: number, finalError?: string}>"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestExecutionService",
        "testResultTypes (TestExecutionResult, TestReport, TestReportSummary)",
        "testPrompts (buildFixPrompt)",
        "SWLogger"
      ],
      "intent": "This file exists to automate the test validation workflow by running tests, identifying failures, and using LLM capabilities to automatically fix broken tests with intelligent retry logic. It solves the problem of manual test debugging and fixing by providing an automated validation and repair pipeline for generated or existing tests.",
      "rawContent": "```json\n{\n  \"purpose\": \"Validates tests by running them, detecting failures, and automatically fixing failing tests using an LLM.\",\n  \"userVisibleActions\": [\n    \"Tests are automatically executed in the workspace\",\n    \"Test results are displayed showing passed, failed, and total test counts\",\n    \"Failing tests are automatically fixed through multiple attempts\",\n    \"Progress updates are shown for each fix attempt\",\n    \"Final test validation reports are generated with success/failure statistics\"\n  ],\n  \"developerVisibleActions\": [\n    \"Trigger test execution for entire workspace or specific test files\",\n    \"Initiate automatic fixing of failing tests with configurable retry attempts\",\n    \"Receive structured test execution results with pass/fail counts per file\",\n    \"Get detailed test reports including failure messages and stack traces\",\n    \"Access test report summaries with overall statistics\",\n    \"Monitor fix attempt progress through logged messages\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"runTests\",\n      \"desc\": \"Executes all tests or tests in a specific file and captures results\",\n      \"inputs\": \"workspaceRoot (string), optional testFile (string)\",\n      \"outputs\": \"Promise<TestExecutionResult[]> - array of test results per file\"\n    },\n    {\n      \"name\": \"fixFailingTest\",\n      \"desc\": \"Attempts to automatically fix a failing test using LLM with retry logic\",\n      \"inputs\": \"testFilePath (string), executionResult (TestExecutionResult), workspaceRoot (string), llmService (any), optional maxAttempts (number, default 3)\",\n      \"outputs\": \"Promise<{success: boolean, attempts: number, finalError?: string}>\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestExecutionService\",\n    \"testResultTypes (TestExecutionResult, TestReport, TestReportSummary)\",\n    \"testPrompts (buildFixPrompt)\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This file exists to automate the test validation workflow by running tests, identifying failures, and using LLM capabilities to automatically fix broken tests with intelligent retry logic. It solves the problem of manual test debugging and fixing by providing an automated validation and repair pipeline for generated or existing tests.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/testExecutionService.ts",
      "role": "Core Logic",
      "purpose": "Executes test suites (Jest, Mocha, Pytest) and captures their results for analysis and reporting",
      "userVisibleActions": [
        "Runs tests for a specific file or all tests in the workspace",
        "Shows test execution progress with status updates",
        "Displays test results including passed, failed, and error counts",
        "Shows test execution duration for each test suite",
        "Reports detailed error messages and stack traces for failed tests",
        "Indicates when tests timeout or fail to execute"
      ],
      "developerVisibleActions": [
        "Calls runJest() to execute Jest tests with optional file parameter",
        "Calls runMocha() to execute Mocha tests with optional file parameter",
        "Calls runPytest() to execute Python tests with optional file parameter",
        "Receives TestExecutionResult[] containing pass/fail status, counts, duration, and error details",
        "Handles test execution errors and timeouts gracefully",
        "Works with workspace root path to locate test files",
        "Parses test framework output (JSON format) into structured results",
        "Provides detailed error information including test name, message, and stack trace"
      ],
      "keyFunctions": [
        {
          "name": "runJest",
          "desc": "Executes Jest test suite for a specific file or all tests",
          "inputs": "workspaceRoot: string, testFile?: string",
          "outputs": "Promise<TestExecutionResult[]>"
        },
        {
          "name": "runMocha",
          "desc": "Executes Mocha test suite for a specific file or all tests",
          "inputs": "workspaceRoot: string, testFile?: string",
          "outputs": "Promise<TestExecutionResult[]>"
        },
        {
          "name": "runPytest",
          "desc": "Executes Pytest test suite for a specific file or all tests",
          "inputs": "workspaceRoot: string, testFile?: string",
          "outputs": "Promise<TestExecutionResult[]>"
        },
        {
          "name": "parseJestOutput",
          "desc": "Parses Jest JSON output into structured test results",
          "inputs": "stdout: string, stderr: string",
          "outputs": "TestExecutionResult[]"
        },
        {
          "name": "parseMochaOutput",
          "desc": "Parses Mocha JSON output into structured test results",
          "inputs": "stdout: string, stderr: string",
          "outputs": "TestExecutionResult[]"
        },
        {
          "name": "parsePytestOutput",
          "desc": "Parses Pytest JSON output into structured test results",
          "inputs": "stdout: string, stderr: string",
          "outputs": "TestExecutionResult[]"
        }
      ],
      "dependencies": [
        "child_process",
        "path",
        "./types/testResultTypes"
      ],
      "intent": "This service solves the problem of running different test frameworks (Jest, Mocha, Pytest) in a unified way and translating their diverse output formats into a consistent structured format that can be analyzed, reported, and acted upon. It abstracts away the complexity of executing shell commands, parsing framework-specific output, and handling errors.",
      "rawContent": "```json\n{\n  \"purpose\": \"Executes test suites (Jest, Mocha, Pytest) and captures their results for analysis and reporting\",\n  \"userVisibleActions\": [\n    \"Runs tests for a specific file or all tests in the workspace\",\n    \"Shows test execution progress with status updates\",\n    \"Displays test results including passed, failed, and error counts\",\n    \"Shows test execution duration for each test suite\",\n    \"Reports detailed error messages and stack traces for failed tests\",\n    \"Indicates when tests timeout or fail to execute\"\n  ],\n  \"developerVisibleActions\": [\n    \"Calls runJest() to execute Jest tests with optional file parameter\",\n    \"Calls runMocha() to execute Mocha tests with optional file parameter\",\n    \"Calls runPytest() to execute Python tests with optional file parameter\",\n    \"Receives TestExecutionResult[] containing pass/fail status, counts, duration, and error details\",\n    \"Handles test execution errors and timeouts gracefully\",\n    \"Works with workspace root path to locate test files\",\n    \"Parses test framework output (JSON format) into structured results\",\n    \"Provides detailed error information including test name, message, and stack trace\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"runJest\",\n      \"desc\": \"Executes Jest test suite for a specific file or all tests\",\n      \"inputs\": \"workspaceRoot: string, testFile?: string\",\n      \"outputs\": \"Promise<TestExecutionResult[]>\"\n    },\n    {\n      \"name\": \"runMocha\",\n      \"desc\": \"Executes Mocha test suite for a specific file or all tests\",\n      \"inputs\": \"workspaceRoot: string, testFile?: string\",\n      \"outputs\": \"Promise<TestExecutionResult[]>\"\n    },\n    {\n      \"name\": \"runPytest\",\n      \"desc\": \"Executes Pytest test suite for a specific file or all tests\",\n      \"inputs\": \"workspaceRoot: string, testFile?: string\",\n      \"outputs\": \"Promise<TestExecutionResult[]>\"\n    },\n    {\n      \"name\": \"parseJestOutput\",\n      \"desc\": \"Parses Jest JSON output into structured test results\",\n      \"inputs\": \"stdout: string, stderr: string\",\n      \"outputs\": \"TestExecutionResult[]\"\n    },\n    {\n      \"name\": \"parseMochaOutput\",\n      \"desc\": \"Parses Mocha JSON output into structured test results\",\n      \"inputs\": \"stdout: string, stderr: string\",\n      \"outputs\": \"TestExecutionResult[]\"\n    },\n    {\n      \"name\": \"parsePytestOutput\",\n      \"desc\": \"Parses Pytest JSON output into structured test results\",\n      \"inputs\": \"stdout: string, stderr: string\",\n      \"outputs\": \"TestExecutionResult[]\"\n    }\n  ],\n  \"dependencies\": [\n    \"child_process\",\n    \"path\",\n    \"./types/testResultTypes\"\n  ],\n  \"intent\": \"This service solves the problem of running different test frameworks (Jest, Mocha, Pytest) in a unified way and translating their diverse output formats into a consistent structured format that can be analyzed, reported, and acted upon. It abstracts away the complexity of executing shell commands, parsing framework-specific output, and handling errors.\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-20T06:39:56.238Z"
  }
}