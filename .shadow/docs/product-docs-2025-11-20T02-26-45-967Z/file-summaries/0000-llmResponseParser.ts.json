{
  "file": "src/ai/llmResponseParser.ts",
  "role": "Core Logic",
  "purpose": "Extracts structured data from LLM text responses and converts them into typed objects for file summaries, module summaries, and product documentation.",
  "userVisibleActions": [
    "Receives parsed insights about code files including their purpose and key functions",
    "Gets product-level documentation with clear explanations of what the codebase does",
    "Views structured information about user-facing actions and developer-facing actions extracted from AI responses"
  ],
  "developerVisibleActions": [
    "Parses LLM JSON or text responses into FileSummary objects with file metadata",
    "Converts LLM responses into ModuleSummary objects with module-level insights",
    "Transforms LLM output into EnhancedProductDocumentation with product purpose and architecture",
    "Extracts ProductPurposeAnalysis from LLM responses for high-level product understanding",
    "Falls back to text parsing when JSON parsing fails",
    "Handles malformed LLM responses gracefully with default values"
  ],
  "keyFunctions": [
    {
      "name": "parseFileSummary",
      "desc": "Converts LLM response text into a FileSummary object containing purpose, actions, and dependencies",
      "inputs": "content (string), filePath (string), role (string)",
      "outputs": "FileSummary object"
    },
    {
      "name": "extractSection",
      "desc": "Extracts a named section from text response",
      "inputs": "content (string), sectionName (string)",
      "outputs": "Extracted text string"
    },
    {
      "name": "extractListSection",
      "desc": "Extracts a list/array section from text response",
      "inputs": "content (string), sectionName (string)",
      "outputs": "Array of strings"
    }
  ],
  "dependencies": [
    "../fileDocumentation",
    "../llmService"
  ],
  "intent": "This file exists to bridge the gap between raw LLM text responses and the typed data structures needed by the application. It solves the problem of handling unpredictable LLM output formats (JSON or text) and ensuring the application always receives valid structured data, even when the LLM response is malformed or incomplete.",
  "rawContent": "```json\n{\n  \"purpose\": \"Extracts structured data from LLM text responses and converts them into typed objects for file summaries, module summaries, and product documentation.\",\n  \"userVisibleActions\": [\n    \"Receives parsed insights about code files including their purpose and key functions\",\n    \"Gets product-level documentation with clear explanations of what the codebase does\",\n    \"Views structured information about user-facing actions and developer-facing actions extracted from AI responses\"\n  ],\n  \"developerVisibleActions\": [\n    \"Parses LLM JSON or text responses into FileSummary objects with file metadata\",\n    \"Converts LLM responses into ModuleSummary objects with module-level insights\",\n    \"Transforms LLM output into EnhancedProductDocumentation with product purpose and architecture\",\n    \"Extracts ProductPurposeAnalysis from LLM responses for high-level product understanding\",\n    \"Falls back to text parsing when JSON parsing fails\",\n    \"Handles malformed LLM responses gracefully with default values\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"parseFileSummary\",\n      \"desc\": \"Converts LLM response text into a FileSummary object containing purpose, actions, and dependencies\",\n      \"inputs\": \"content (string), filePath (string), role (string)\",\n      \"outputs\": \"FileSummary object\"\n    },\n    {\n      \"name\": \"extractSection\",\n      \"desc\": \"Extracts a named section from text response\",\n      \"inputs\": \"content (string), sectionName (string)\",\n      \"outputs\": \"Extracted text string\"\n    },\n    {\n      \"name\": \"extractListSection\",\n      \"desc\": \"Extracts a list/array section from text response\",\n      \"inputs\": \"content (string), sectionName (string)\",\n      \"outputs\": \"Array of strings\"\n    }\n  ],\n  \"dependencies\": [\n    \"../fileDocumentation\",\n    \"../llmService\"\n  ],\n  \"intent\": \"This file exists to bridge the gap between raw LLM text responses and the typed data structures needed by the application. It solves the problem of handling unpredictable LLM output formats (JSON or text) and ensuring the application always receives valid structured data, even when the LLM response is malformed or incomplete.\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-20T02:35:38.134Z"
  }
}