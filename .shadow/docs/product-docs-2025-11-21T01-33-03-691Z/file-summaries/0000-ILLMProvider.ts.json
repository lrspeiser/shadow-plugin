{
  "file": "src/ai/providers/ILLMProvider.ts",
  "role": "Core Logic",
  "purpose": "Defines the standard interface for integrating different AI language model providers (OpenAI, Claude, etc.) into the application",
  "userVisibleActions": [
    "User can interact with different AI providers (OpenAI, Claude, custom) transparently without knowing which one is being used",
    "User receives text responses from AI models",
    "User receives structured JSON data responses from AI models",
    "User can send messages with conversation history to AI models",
    "User can request file content or search patterns from AI models"
  ],
  "developerVisibleActions": [
    "Developer implements this interface to add new AI provider support",
    "Developer checks if a provider is properly configured before use",
    "Developer sends text-based requests to any AI provider using a unified format",
    "Developer sends structured JSON requests with optional schemas",
    "Developer receives parsed JSON responses with optional file/grep requests",
    "Developer configures model parameters (temperature, max tokens, response format)",
    "Developer builds conversation flows with system, user, and assistant messages",
    "Developer gets provider name for logging or display purposes"
  ],
  "keyFunctions": [
    {
      "name": "isConfigured",
      "desc": "Verifies if the AI provider has valid API keys and is ready to use",
      "inputs": "none",
      "outputs": "boolean indicating if provider is ready"
    },
    {
      "name": "sendRequest",
      "desc": "Sends a conversation to the AI model and receives a text response",
      "inputs": "LLMRequestOptions (model, messages, temperature, maxTokens, responseFormat)",
      "outputs": "Promise<LLMResponse> with content, finish reason, and model info"
    },
    {
      "name": "sendStructuredRequest",
      "desc": "Sends a request expecting structured JSON output, optionally validated against a schema",
      "inputs": "LLMRequestOptions and optional schema",
      "outputs": "Promise<StructuredOutputResponse<T>> with parsed data and optional file/grep requests"
    },
    {
      "name": "getName",
      "desc": "Returns the name identifier of the AI provider",
      "inputs": "none",
      "outputs": "string with provider name"
    }
  ],
  "dependencies": [],
  "intent": "This interface exists to provide a unified abstraction layer over multiple AI providers, allowing the application to switch between different LLM services (OpenAI, Anthropic Claude, custom providers) without changing application code. It solves the problem of vendor lock-in and enables flexible AI provider configuration while maintaining consistent behavior across the application.",
  "rawContent": "```json\n{\n  \"purpose\": \"Defines the standard interface for integrating different AI language model providers (OpenAI, Claude, etc.) into the application\",\n  \"userVisibleActions\": [\n    \"User can interact with different AI providers (OpenAI, Claude, custom) transparently without knowing which one is being used\",\n    \"User receives text responses from AI models\",\n    \"User receives structured JSON data responses from AI models\",\n    \"User can send messages with conversation history to AI models\",\n    \"User can request file content or search patterns from AI models\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer implements this interface to add new AI provider support\",\n    \"Developer checks if a provider is properly configured before use\",\n    \"Developer sends text-based requests to any AI provider using a unified format\",\n    \"Developer sends structured JSON requests with optional schemas\",\n    \"Developer receives parsed JSON responses with optional file/grep requests\",\n    \"Developer configures model parameters (temperature, max tokens, response format)\",\n    \"Developer builds conversation flows with system, user, and assistant messages\",\n    \"Developer gets provider name for logging or display purposes\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Verifies if the AI provider has valid API keys and is ready to use\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating if provider is ready\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a conversation to the AI model and receives a text response\",\n      \"inputs\": \"LLMRequestOptions (model, messages, temperature, maxTokens, responseFormat)\",\n      \"outputs\": \"Promise<LLMResponse> with content, finish reason, and model info\"\n    },\n    {\n      \"name\": \"sendStructuredRequest\",\n      \"desc\": \"Sends a request expecting structured JSON output, optionally validated against a schema\",\n      \"inputs\": \"LLMRequestOptions and optional schema\",\n      \"outputs\": \"Promise<StructuredOutputResponse<T>> with parsed data and optional file/grep requests\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the name identifier of the AI provider\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string with provider name\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"This interface exists to provide a unified abstraction layer over multiple AI providers, allowing the application to switch between different LLM services (OpenAI, Anthropic Claude, custom providers) without changing application code. It solves the problem of vendor lock-in and enables flexible AI provider configuration while maintaining consistent behavior across the application.\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T01:33:52.556Z"
  }
}