{
  "file": "src/ai/llmRateLimiter.ts",
  "role": "Core Logic",
  "purpose": "Prevents LLM API requests from exceeding rate limits by tracking and throttling requests per provider",
  "userVisibleActions": [
    "API requests are automatically throttled to prevent hitting rate limits",
    "Requests may be delayed or rejected when rate limits are approached",
    "Different LLM providers (OpenAI, Claude) have different rate limits applied"
  ],
  "developerVisibleActions": [
    "Configure custom rate limits for OpenAI and Claude providers",
    "Check if a request can be made before calling the API",
    "Record successful API requests to track usage against limits",
    "Get information about remaining requests available",
    "Wait for rate limit windows to reset when limits are reached"
  ],
  "keyFunctions": [
    {
      "name": "constructor",
      "desc": "Initializes rate limiter with default limits (OpenAI: 60 req/min, Claude: 50 req/min)",
      "inputs": "none",
      "outputs": "RateLimiter instance"
    },
    {
      "name": "configure",
      "desc": "Sets custom rate limit configuration for a specific LLM provider",
      "inputs": "provider (openai or claude), config (maxRequests, windowMs)",
      "outputs": "void"
    },
    {
      "name": "canMakeRequest",
      "desc": "Checks if a new request can be made without exceeding rate limits",
      "inputs": "provider (openai or claude)",
      "outputs": "boolean - true if request is allowed"
    },
    {
      "name": "recordRequest",
      "desc": "Records a request timestamp to track usage against rate limits",
      "inputs": "provider (openai or claude)",
      "outputs": "void"
    }
  ],
  "dependencies": [],
  "intent": "Prevents API rate limit errors by tracking and enforcing request quotas per LLM provider, ensuring the application stays within API usage limits and avoids service disruptions or additional costs from exceeding quotas",
  "rawContent": "```json\n{\n  \"purpose\": \"Prevents LLM API requests from exceeding rate limits by tracking and throttling requests per provider\",\n  \"userVisibleActions\": [\n    \"API requests are automatically throttled to prevent hitting rate limits\",\n    \"Requests may be delayed or rejected when rate limits are approached\",\n    \"Different LLM providers (OpenAI, Claude) have different rate limits applied\"\n  ],\n  \"developerVisibleActions\": [\n    \"Configure custom rate limits for OpenAI and Claude providers\",\n    \"Check if a request can be made before calling the API\",\n    \"Record successful API requests to track usage against limits\",\n    \"Get information about remaining requests available\",\n    \"Wait for rate limit windows to reset when limits are reached\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"constructor\",\n      \"desc\": \"Initializes rate limiter with default limits (OpenAI: 60 req/min, Claude: 50 req/min)\",\n      \"inputs\": \"none\",\n      \"outputs\": \"RateLimiter instance\"\n    },\n    {\n      \"name\": \"configure\",\n      \"desc\": \"Sets custom rate limit configuration for a specific LLM provider\",\n      \"inputs\": \"provider (openai or claude), config (maxRequests, windowMs)\",\n      \"outputs\": \"void\"\n    },\n    {\n      \"name\": \"canMakeRequest\",\n      \"desc\": \"Checks if a new request can be made without exceeding rate limits\",\n      \"inputs\": \"provider (openai or claude)\",\n      \"outputs\": \"boolean - true if request is allowed\"\n    },\n    {\n      \"name\": \"recordRequest\",\n      \"desc\": \"Records a request timestamp to track usage against rate limits\",\n      \"inputs\": \"provider (openai or claude)\",\n      \"outputs\": \"void\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"Prevents API rate limit errors by tracking and enforcing request quotas per LLM provider, ensuring the application stays within API usage limits and avoids service disruptions or additional costs from exceeding quotas\"\n}\n```",
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T01:33:13.148Z"
  }
}