{
  "module": "src/ai/providers",
  "moduleType": "other",
  "capabilities": [
    "Connect to multiple AI providers (OpenAI GPT models and Anthropic Claude models) through a unified interface",
    "Automatically select and initialize the appropriate AI provider based on user configuration and available API keys",
    "Send conversational messages with full conversation history to AI models for context-aware responses",
    "Receive both plain text responses and structured JSON data from AI models",
    "Request file content or search patterns from AI models during conversations",
    "Validate API keys automatically before making requests to prevent errors",
    "Switch between different AI providers transparently without changing application behavior",
    "Configure provider-specific settings including model selection and API keys"
  ],
  "summary": "This module provides a unified AI provider abstraction layer that enables users to interact with multiple AI language models (OpenAI GPT and Anthropic Claude) through a single consistent interface. Users can configure their preferred AI provider and API key in the extension settings, and the module automatically handles provider initialization, API key validation, and request routing.\n\nThe module supports both conversational interactions with full message history and structured data requests. Users can send prompts to AI models and receive either natural language responses or formatted JSON data depending on their needs. The provider factory automatically selects the appropriate AI service based on user configuration and validates that required API keys are present before attempting connections.\n\nAll AI interactions flow through a standardized interface that abstracts away provider-specific implementation details. This allows users to switch between OpenAI and Claude seamlessly, with the application handling differences in API formats, authentication methods, and response structures. The module ensures consistent behavior regardless of which underlying AI provider is being used, while still allowing access to provider-specific features like different model variants (GPT-4, Claude Sonnet, Opus, Haiku).",
  "files": [
    {
      "file": "src/ai/providers/ILLMProvider.ts",
      "role": "Core Logic",
      "purpose": "Defines the standard interface for integrating different AI language model providers (OpenAI, Claude, etc.) into the application",
      "userVisibleActions": [
        "User can interact with different AI providers (OpenAI, Claude, custom) transparently without knowing which one is being used",
        "User receives text responses from AI models",
        "User receives structured JSON data responses from AI models",
        "User can send messages with conversation history to AI models",
        "User can request file content or search patterns from AI models"
      ],
      "developerVisibleActions": [
        "Developer implements this interface to add new AI provider support",
        "Developer checks if a provider is properly configured before use",
        "Developer sends text-based requests to any AI provider using a unified format",
        "Developer sends structured JSON requests with optional schemas",
        "Developer receives parsed JSON responses with optional file/grep requests",
        "Developer configures model parameters (temperature, max tokens, response format)",
        "Developer builds conversation flows with system, user, and assistant messages",
        "Developer gets provider name for logging or display purposes"
      ],
      "keyFunctions": [
        {
          "name": "isConfigured",
          "desc": "Verifies if the AI provider has valid API keys and is ready to use",
          "inputs": "none",
          "outputs": "boolean indicating if provider is ready"
        },
        {
          "name": "sendRequest",
          "desc": "Sends a conversation to the AI model and receives a text response",
          "inputs": "LLMRequestOptions (model, messages, temperature, maxTokens, responseFormat)",
          "outputs": "Promise<LLMResponse> with content, finish reason, and model info"
        },
        {
          "name": "sendStructuredRequest",
          "desc": "Sends a request expecting structured JSON output, optionally validated against a schema",
          "inputs": "LLMRequestOptions and optional schema",
          "outputs": "Promise<StructuredOutputResponse<T>> with parsed data and optional file/grep requests"
        },
        {
          "name": "getName",
          "desc": "Returns the name identifier of the AI provider",
          "inputs": "none",
          "outputs": "string with provider name"
        }
      ],
      "dependencies": [],
      "intent": "This interface exists to provide a unified abstraction layer over multiple AI providers, allowing the application to switch between different LLM services (OpenAI, Anthropic Claude, custom providers) without changing application code. It solves the problem of vendor lock-in and enables flexible AI provider configuration while maintaining consistent behavior across the application.",
      "rawContent": "```json\n{\n  \"purpose\": \"Defines the standard interface for integrating different AI language model providers (OpenAI, Claude, etc.) into the application\",\n  \"userVisibleActions\": [\n    \"User can interact with different AI providers (OpenAI, Claude, custom) transparently without knowing which one is being used\",\n    \"User receives text responses from AI models\",\n    \"User receives structured JSON data responses from AI models\",\n    \"User can send messages with conversation history to AI models\",\n    \"User can request file content or search patterns from AI models\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer implements this interface to add new AI provider support\",\n    \"Developer checks if a provider is properly configured before use\",\n    \"Developer sends text-based requests to any AI provider using a unified format\",\n    \"Developer sends structured JSON requests with optional schemas\",\n    \"Developer receives parsed JSON responses with optional file/grep requests\",\n    \"Developer configures model parameters (temperature, max tokens, response format)\",\n    \"Developer builds conversation flows with system, user, and assistant messages\",\n    \"Developer gets provider name for logging or display purposes\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Verifies if the AI provider has valid API keys and is ready to use\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating if provider is ready\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a conversation to the AI model and receives a text response\",\n      \"inputs\": \"LLMRequestOptions (model, messages, temperature, maxTokens, responseFormat)\",\n      \"outputs\": \"Promise<LLMResponse> with content, finish reason, and model info\"\n    },\n    {\n      \"name\": \"sendStructuredRequest\",\n      \"desc\": \"Sends a request expecting structured JSON output, optionally validated against a schema\",\n      \"inputs\": \"LLMRequestOptions and optional schema\",\n      \"outputs\": \"Promise<StructuredOutputResponse<T>> with parsed data and optional file/grep requests\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the name identifier of the AI provider\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string with provider name\"\n    }\n  ],\n  \"dependencies\": [],\n  \"intent\": \"This interface exists to provide a unified abstraction layer over multiple AI providers, allowing the application to switch between different LLM services (OpenAI, Anthropic Claude, custom providers) without changing application code. It solves the problem of vendor lock-in and enables flexible AI provider configuration while maintaining consistent behavior across the application.\"\n}\n```"
    },
    {
      "file": "src/ai/providers/anthropicProvider.ts",
      "role": "Core Logic",
      "purpose": "Provides integration with Anthropic's Claude AI models for generating text responses and structured outputs",
      "userVisibleActions": [
        "User can interact with Claude AI models through the extension",
        "User receives AI-generated responses from Claude models (Sonnet, Opus, Haiku)",
        "User can configure Claude API key in settings to enable Claude provider",
        "User sees error messages when Claude API key is not configured",
        "User experiences AI conversations with system prompts and message history"
      ],
      "developerVisibleActions": [
        "Developer can send text generation requests to Claude models",
        "Developer can request structured JSON outputs with automatic validation",
        "Developer can specify system prompts to guide Claude's behavior",
        "Developer can provide conversation history with user and assistant messages",
        "Developer receives parsed JSON responses from Claude with validation",
        "Developer can choose between different Claude model variants (Sonnet, Opus, Haiku)",
        "Developer gets automatic retry with fallback when JSON parsing fails",
        "Developer can set custom token limits and timeouts for requests"
      ],
      "keyFunctions": [
        {
          "name": "isConfigured",
          "desc": "Checks if Claude API key is configured and provider is ready to use",
          "inputs": "none",
          "outputs": "boolean indicating if provider is configured"
        },
        {
          "name": "getName",
          "desc": "Returns the identifier name for this provider",
          "inputs": "none",
          "outputs": "string 'claude'"
        },
        {
          "name": "sendRequest",
          "desc": "Sends a text generation request to Claude and returns the response",
          "inputs": "LLMRequestOptions with model, messages, system prompt, max tokens",
          "outputs": "LLMResponse with generated text content and token usage"
        },
        {
          "name": "sendStructuredOutputRequest",
          "desc": "Requests structured JSON output from Claude with validation and retry logic",
          "inputs": "LLMRequestOptions with JSON schema requirements",
          "outputs": "StructuredOutputResponse with parsed JSON data or validation error"
        },
        {
          "name": "initialize",
          "desc": "Sets up the Claude client with API key from configuration",
          "inputs": "none (reads from config)",
          "outputs": "none (initializes internal client)"
        }
      ],
      "dependencies": [
        "@anthropic-ai/sdk",
        "../../config/configurationManager",
        "../../utils/jsonExtractor",
        "./ILLMProvider"
      ],
      "intent": "This file exists to provide a standardized interface for integrating Anthropic's Claude AI models into the extension. It solves the problem of abstracting away Claude-specific API details, handling message format conversion, managing API authentication, parsing structured outputs, and providing consistent error handling for Claude interactions.",
      "rawContent": "```json\n{\n  \"purpose\": \"Provides integration with Anthropic's Claude AI models for generating text responses and structured outputs\",\n  \"userVisibleActions\": [\n    \"User can interact with Claude AI models through the extension\",\n    \"User receives AI-generated responses from Claude models (Sonnet, Opus, Haiku)\",\n    \"User can configure Claude API key in settings to enable Claude provider\",\n    \"User sees error messages when Claude API key is not configured\",\n    \"User experiences AI conversations with system prompts and message history\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer can send text generation requests to Claude models\",\n    \"Developer can request structured JSON outputs with automatic validation\",\n    \"Developer can specify system prompts to guide Claude's behavior\",\n    \"Developer can provide conversation history with user and assistant messages\",\n    \"Developer receives parsed JSON responses from Claude with validation\",\n    \"Developer can choose between different Claude model variants (Sonnet, Opus, Haiku)\",\n    \"Developer gets automatic retry with fallback when JSON parsing fails\",\n    \"Developer can set custom token limits and timeouts for requests\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Checks if Claude API key is configured and provider is ready to use\",\n      \"inputs\": \"none\",\n      \"outputs\": \"boolean indicating if provider is configured\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the identifier name for this provider\",\n      \"inputs\": \"none\",\n      \"outputs\": \"string 'claude'\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a text generation request to Claude and returns the response\",\n      \"inputs\": \"LLMRequestOptions with model, messages, system prompt, max tokens\",\n      \"outputs\": \"LLMResponse with generated text content and token usage\"\n    },\n    {\n      \"name\": \"sendStructuredOutputRequest\",\n      \"desc\": \"Requests structured JSON output from Claude with validation and retry logic\",\n      \"inputs\": \"LLMRequestOptions with JSON schema requirements\",\n      \"outputs\": \"StructuredOutputResponse with parsed JSON data or validation error\"\n    },\n    {\n      \"name\": \"initialize\",\n      \"desc\": \"Sets up the Claude client with API key from configuration\",\n      \"inputs\": \"none (reads from config)\",\n      \"outputs\": \"none (initializes internal client)\"\n    }\n  ],\n  \"dependencies\": [\n    \"@anthropic-ai/sdk\",\n    \"../../config/configurationManager\",\n    \"../../utils/jsonExtractor\",\n    \"./ILLMProvider\"\n  ],\n  \"intent\": \"This file exists to provide a standardized interface for integrating Anthropic's Claude AI models into the extension. It solves the problem of abstracting away Claude-specific API details, handling message format conversion, managing API authentication, parsing structured outputs, and providing consistent error handling for Claude interactions.\"\n}\n```"
    },
    {
      "file": "src/ai/providers/openAIProvider.ts",
      "role": "Core Logic",
      "purpose": "Provides OpenAI integration for making LLM requests with support for both standard and structured JSON responses",
      "userVisibleActions": [
        "User's prompts are sent to OpenAI's GPT models for AI-powered responses",
        "User receives AI-generated text responses based on their input",
        "User can get structured JSON responses when requesting specific data formats",
        "User experiences automatic API key validation before making requests"
      ],
      "developerVisibleActions": [
        "Developer configures OpenAI API key through configuration manager",
        "Developer sends chat completion requests with custom models, prompts, and messages",
        "Developer receives parsed JSON responses when using structured output format",
        "Developer gets error messages when API key is not configured or requests fail",
        "Developer can check if provider is properly configured before use",
        "Developer specifies system prompts, conversation history, and response format options"
      ],
      "keyFunctions": [
        {
          "name": "initialize",
          "desc": "Sets up the OpenAI client with API key from configuration",
          "inputs": "None (reads from config manager)",
          "outputs": "void (initializes client)"
        },
        {
          "name": "isConfigured",
          "desc": "Checks if the provider has a valid API key and is ready to use",
          "inputs": "None",
          "outputs": "boolean indicating configuration status"
        },
        {
          "name": "getName",
          "desc": "Returns the provider identifier",
          "inputs": "None",
          "outputs": "string 'openai'"
        },
        {
          "name": "sendRequest",
          "desc": "Sends a chat completion request to OpenAI with messages and optional system prompt",
          "inputs": "LLMRequestOptions (model, messages, systemPrompt, responseFormat)",
          "outputs": "Promise<LLMResponse> with content and finish reason"
        },
        {
          "name": "sendStructuredOutputRequest",
          "desc": "Sends a request expecting structured JSON output and parses the response",
          "inputs": "LLMRequestOptions with expected JSON format",
          "outputs": "Promise<StructuredOutputResponse> with parsed JSON data"
        }
      ],
      "dependencies": [
        "openai",
        "../../config/configurationManager",
        "../../utils/jsonExtractor",
        "./ILLMProvider"
      ],
      "intent": "This file exists to encapsulate all OpenAI-specific API interactions, providing a consistent interface for the application to communicate with OpenAI's GPT models while handling configuration, error cases, and both standard text and structured JSON responses.",
      "rawContent": "```json\n{\n  \"purpose\": \"Provides OpenAI integration for making LLM requests with support for both standard and structured JSON responses\",\n  \"userVisibleActions\": [\n    \"User's prompts are sent to OpenAI's GPT models for AI-powered responses\",\n    \"User receives AI-generated text responses based on their input\",\n    \"User can get structured JSON responses when requesting specific data formats\",\n    \"User experiences automatic API key validation before making requests\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer configures OpenAI API key through configuration manager\",\n    \"Developer sends chat completion requests with custom models, prompts, and messages\",\n    \"Developer receives parsed JSON responses when using structured output format\",\n    \"Developer gets error messages when API key is not configured or requests fail\",\n    \"Developer can check if provider is properly configured before use\",\n    \"Developer specifies system prompts, conversation history, and response format options\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"initialize\",\n      \"desc\": \"Sets up the OpenAI client with API key from configuration\",\n      \"inputs\": \"None (reads from config manager)\",\n      \"outputs\": \"void (initializes client)\"\n    },\n    {\n      \"name\": \"isConfigured\",\n      \"desc\": \"Checks if the provider has a valid API key and is ready to use\",\n      \"inputs\": \"None\",\n      \"outputs\": \"boolean indicating configuration status\"\n    },\n    {\n      \"name\": \"getName\",\n      \"desc\": \"Returns the provider identifier\",\n      \"inputs\": \"None\",\n      \"outputs\": \"string 'openai'\"\n    },\n    {\n      \"name\": \"sendRequest\",\n      \"desc\": \"Sends a chat completion request to OpenAI with messages and optional system prompt\",\n      \"inputs\": \"LLMRequestOptions (model, messages, systemPrompt, responseFormat)\",\n      \"outputs\": \"Promise<LLMResponse> with content and finish reason\"\n    },\n    {\n      \"name\": \"sendStructuredOutputRequest\",\n      \"desc\": \"Sends a request expecting structured JSON output and parses the response\",\n      \"inputs\": \"LLMRequestOptions with expected JSON format\",\n      \"outputs\": \"Promise<StructuredOutputResponse> with parsed JSON data\"\n    }\n  ],\n  \"dependencies\": [\n    \"openai\",\n    \"../../config/configurationManager\",\n    \"../../utils/jsonExtractor\",\n    \"./ILLMProvider\"\n  ],\n  \"intent\": \"This file exists to encapsulate all OpenAI-specific API interactions, providing a consistent interface for the application to communicate with OpenAI's GPT models while handling configuration, error cases, and both standard text and structured JSON responses.\"\n}\n```"
    },
    {
      "file": "src/ai/providers/providerFactory.ts",
      "role": "Core Logic",
      "purpose": "Creates and manages AI provider instances (OpenAI or Claude) based on user configuration",
      "userVisibleActions": [
        "Automatically connects to the AI provider selected in settings (OpenAI or Claude)",
        "Only shows available AI providers that have valid API keys configured",
        "Uses the default AI provider from configuration when generating responses"
      ],
      "developerVisibleActions": [
        "Provides a centralized factory for getting AI provider instances",
        "Caches provider instances to avoid recreating them",
        "Exposes methods to check which providers are properly configured",
        "Returns the currently active provider based on user settings",
        "Throws error when requesting an unknown provider type"
      ],
      "keyFunctions": [
        {
          "name": "getProvider",
          "desc": "Returns a specific AI provider instance (OpenAI or Claude)",
          "inputs": "provider: 'openai' | 'claude'",
          "outputs": "ILLMProvider instance"
        },
        {
          "name": "getCurrentProvider",
          "desc": "Returns the AI provider that is currently active in user settings",
          "inputs": "none",
          "outputs": "ILLMProvider instance"
        },
        {
          "name": "isProviderConfigured",
          "desc": "Checks if a provider has valid configuration (API key, etc.)",
          "inputs": "provider: 'openai' | 'claude'",
          "outputs": "boolean"
        },
        {
          "name": "getConfiguredProviders",
          "desc": "Returns list of all providers that have valid configuration",
          "inputs": "none",
          "outputs": "Array of provider names"
        }
      ],
      "dependencies": [
        "ILLMProvider",
        "OpenAIProvider",
        "AnthropicProvider",
        "configurationManager"
      ],
      "intent": "Provides a single point of control for creating and accessing AI providers, ensuring only one instance of each provider exists and automatically selecting the provider based on user configuration",
      "rawContent": "```json\n{\n  \"purpose\": \"Creates and manages AI provider instances (OpenAI or Claude) based on user configuration\",\n  \"userVisibleActions\": [\n    \"Automatically connects to the AI provider selected in settings (OpenAI or Claude)\",\n    \"Only shows available AI providers that have valid API keys configured\",\n    \"Uses the default AI provider from configuration when generating responses\"\n  ],\n  \"developerVisibleActions\": [\n    \"Provides a centralized factory for getting AI provider instances\",\n    \"Caches provider instances to avoid recreating them\",\n    \"Exposes methods to check which providers are properly configured\",\n    \"Returns the currently active provider based on user settings\",\n    \"Throws error when requesting an unknown provider type\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"getProvider\",\n      \"desc\": \"Returns a specific AI provider instance (OpenAI or Claude)\",\n      \"inputs\": \"provider: 'openai' | 'claude'\",\n      \"outputs\": \"ILLMProvider instance\"\n    },\n    {\n      \"name\": \"getCurrentProvider\",\n      \"desc\": \"Returns the AI provider that is currently active in user settings\",\n      \"inputs\": \"none\",\n      \"outputs\": \"ILLMProvider instance\"\n    },\n    {\n      \"name\": \"isProviderConfigured\",\n      \"desc\": \"Checks if a provider has valid configuration (API key, etc.)\",\n      \"inputs\": \"provider: 'openai' | 'claude'\",\n      \"outputs\": \"boolean\"\n    },\n    {\n      \"name\": \"getConfiguredProviders\",\n      \"desc\": \"Returns list of all providers that have valid configuration\",\n      \"inputs\": \"none\",\n      \"outputs\": \"Array of provider names\"\n    }\n  ],\n  \"dependencies\": [\n    \"ILLMProvider\",\n    \"OpenAIProvider\",\n    \"AnthropicProvider\",\n    \"configurationManager\"\n  ],\n  \"intent\": \"Provides a single point of control for creating and accessing AI providers, ensuring only one instance of each provider exists and automatically selecting the provider based on user configuration\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T01:44:26.046Z"
  }
}