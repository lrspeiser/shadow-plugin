{
  "module": "src/domain/services/testing",
  "moduleType": "tests",
  "capabilities": [
    "Automatically analyze codebases to identify testable functions and create prioritized test plans",
    "Generate unit tests incrementally using AI, creating test code for functions in small batches",
    "Detect and configure test environments including frameworks, directories, and configuration files",
    "Execute test suites (Jest, Pytest) and capture detailed results with pass/fail statistics",
    "Automatically validate and fix failing tests using AI-powered analysis and code regeneration",
    "Provide real-time progress updates during test generation, execution, and validation",
    "Save test plans and recommendations to the workspace for future reference"
  ],
  "summary": "This module provides comprehensive AI-powered test automation capabilities for software projects. It enables users to automatically generate, execute, and maintain unit tests without manual intervention. The workflow begins with test planning, where the system analyzes code to identify testable functions and prioritize them based on complexity and importance. Users receive a detailed test plan showing which functions need testing and strategic recommendations based on their codebase architecture.\n\nOnce planning is complete, the module can automatically generate test code using AI, processing functions in small batches and providing progress updates throughout the generation process. The system intelligently detects the project's test environment, including programming language, testing framework (Jest, Pytest), and configuration files, ensuring generated tests match the project's conventions. After generation, tests are automatically executed with detailed reporting of passed, failed, and error counts.\n\nThe module includes a self-healing capability that automatically validates and fixes failing tests. When tests fail, the system uses AI analysis to understand the failure and regenerate corrected test code, repeating this process until tests pass or a maximum number of attempts is reached. Users receive comprehensive reports throughout the entire lifecycle, from initial planning through final validation, with all progress, results, and recommendations displayed in real-time. Test plans and configurations are persisted in the .skyway/testing directory for ongoing reference and iteration.",
  "files": [
    {
      "file": "src/domain/services/testing/llmTestGenerationService.ts",
      "role": "Core Logic",
      "purpose": "Generates test code incrementally in small batches using an LLM service to create tests for testable functions",
      "userVisibleActions": [
        "Progress updates shown while tests are being generated for each function",
        "Test generation results displayed for batches of functions",
        "Error messages shown if test generation fails for specific functions"
      ],
      "developerVisibleActions": [
        "Triggers batch generation of tests for multiple functions at once",
        "Receives progress callbacks with current function being tested and completion percentage",
        "Gets back a map of test generation results keyed by function name",
        "Can provide custom progress handlers to track generation status",
        "Accesses generated test code and execution results for each function"
      ],
      "keyFunctions": [
        {
          "name": "generateTestBatch",
          "desc": "Generates tests for multiple functions in a batch, processing them one at a time with progress tracking",
          "inputs": "functions array, workspace root path, LLM service instance, optional progress callback",
          "outputs": "Map of function names to TestGenerationResult objects"
        },
        {
          "name": "extractFunctionSource",
          "desc": "Extracts the source code for a specific function from the workspace",
          "inputs": "TestableFunction object, workspace root path",
          "outputs": "Source code string for the function"
        },
        {
          "name": "buildGenerationPrompt",
          "desc": "Creates an LLM prompt for generating tests based on function details and existing code",
          "inputs": "function object, source code, test framework type, existing mock code",
          "outputs": "Formatted prompt string for the LLM"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestableFunction",
        "TestGenerationState",
        "TestGenerationResult",
        "buildGenerationPrompt",
        "TestExecutionService",
        "SWLogger"
      ],
      "intent": "This file exists to orchestrate the automated generation of unit tests using an LLM in manageable batches, allowing developers to generate test suites for multiple functions incrementally with progress tracking and result collection, solving the problem of manually writing repetitive test code.",
      "rawContent": "```json\n{\n  \"purpose\": \"Generates test code incrementally in small batches using an LLM service to create tests for testable functions\",\n  \"userVisibleActions\": [\n    \"Progress updates shown while tests are being generated for each function\",\n    \"Test generation results displayed for batches of functions\",\n    \"Error messages shown if test generation fails for specific functions\"\n  ],\n  \"developerVisibleActions\": [\n    \"Triggers batch generation of tests for multiple functions at once\",\n    \"Receives progress callbacks with current function being tested and completion percentage\",\n    \"Gets back a map of test generation results keyed by function name\",\n    \"Can provide custom progress handlers to track generation status\",\n    \"Accesses generated test code and execution results for each function\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"generateTestBatch\",\n      \"desc\": \"Generates tests for multiple functions in a batch, processing them one at a time with progress tracking\",\n      \"inputs\": \"functions array, workspace root path, LLM service instance, optional progress callback\",\n      \"outputs\": \"Map of function names to TestGenerationResult objects\"\n    },\n    {\n      \"name\": \"extractFunctionSource\",\n      \"desc\": \"Extracts the source code for a specific function from the workspace\",\n      \"inputs\": \"TestableFunction object, workspace root path\",\n      \"outputs\": \"Source code string for the function\"\n    },\n    {\n      \"name\": \"buildGenerationPrompt\",\n      \"desc\": \"Creates an LLM prompt for generating tests based on function details and existing code\",\n      \"inputs\": \"function object, source code, test framework type, existing mock code\",\n      \"outputs\": \"Formatted prompt string for the LLM\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestableFunction\",\n    \"TestGenerationState\",\n    \"TestGenerationResult\",\n    \"buildGenerationPrompt\",\n    \"TestExecutionService\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This file exists to orchestrate the automated generation of unit tests using an LLM in manageable batches, allowing developers to generate test suites for multiple functions incrementally with progress tracking and result collection, solving the problem of manually writing repetitive test code.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestPlanningService.ts",
      "role": "Core Logic",
      "purpose": "Creates AI-powered test plans by analyzing code functions and prioritizing them for testing based on complexity and importance",
      "userVisibleActions": [
        "Receives a prioritized test plan showing which functions need testing",
        "Sees analysis of how many functions are testable vs total functions",
        "Gets test strategy recommendations based on code complexity and architecture",
        "Views saved test plans in the .skyway/testing directory"
      ],
      "developerVisibleActions": [
        "Calls analyzeFunctions() to extract function metadata from code analysis",
        "Invokes createTestPlan() to generate an AI-powered test strategy",
        "Saves test plans to disk using saveTestPlan() for future reference",
        "Receives structured TestPlan objects containing function groups and priorities",
        "Gets logging output showing progress of test plan creation",
        "Can provide product documentation and architecture insights to improve test planning"
      ],
      "keyFunctions": [
        {
          "name": "analyzeFunctions",
          "desc": "Extracts and structures function metadata from code analysis results",
          "inputs": "codeAnalysis object containing function information",
          "outputs": "Array of function objects with name, file, lines, complexity, parameters, and return type"
        },
        {
          "name": "createTestPlan",
          "desc": "Generates a prioritized test plan using AI by analyzing functions and context",
          "inputs": "CodeAnalysis context, functions array, LLM service, optional product docs and architecture insights",
          "outputs": "TestPlan object containing function groups, testable function count, and testing strategy"
        },
        {
          "name": "saveTestPlan",
          "desc": "Persists the generated test plan to disk in JSON format",
          "inputs": "TestPlan object and output directory path",
          "outputs": "File path where test plan was saved"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestPlan and TestableFunction types",
        "testPrompts (buildPlanningPrompt)",
        "CodeAnalysis from analyzer",
        "SWLogger"
      ],
      "intent": "This file exists to automate test planning by leveraging AI to analyze code complexity, function relationships, and architecture to determine which functions should be tested and in what priority order, eliminating manual test planning effort and ensuring critical code paths are tested first",
      "rawContent": "```json\n{\n  \"purpose\": \"Creates AI-powered test plans by analyzing code functions and prioritizing them for testing based on complexity and importance\",\n  \"userVisibleActions\": [\n    \"Receives a prioritized test plan showing which functions need testing\",\n    \"Sees analysis of how many functions are testable vs total functions\",\n    \"Gets test strategy recommendations based on code complexity and architecture\",\n    \"Views saved test plans in the .skyway/testing directory\"\n  ],\n  \"developerVisibleActions\": [\n    \"Calls analyzeFunctions() to extract function metadata from code analysis\",\n    \"Invokes createTestPlan() to generate an AI-powered test strategy\",\n    \"Saves test plans to disk using saveTestPlan() for future reference\",\n    \"Receives structured TestPlan objects containing function groups and priorities\",\n    \"Gets logging output showing progress of test plan creation\",\n    \"Can provide product documentation and architecture insights to improve test planning\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"analyzeFunctions\",\n      \"desc\": \"Extracts and structures function metadata from code analysis results\",\n      \"inputs\": \"codeAnalysis object containing function information\",\n      \"outputs\": \"Array of function objects with name, file, lines, complexity, parameters, and return type\"\n    },\n    {\n      \"name\": \"createTestPlan\",\n      \"desc\": \"Generates a prioritized test plan using AI by analyzing functions and context\",\n      \"inputs\": \"CodeAnalysis context, functions array, LLM service, optional product docs and architecture insights\",\n      \"outputs\": \"TestPlan object containing function groups, testable function count, and testing strategy\"\n    },\n    {\n      \"name\": \"saveTestPlan\",\n      \"desc\": \"Persists the generated test plan to disk in JSON format\",\n      \"inputs\": \"TestPlan object and output directory path\",\n      \"outputs\": \"File path where test plan was saved\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestPlan and TestableFunction types\",\n    \"testPrompts (buildPlanningPrompt)\",\n    \"CodeAnalysis from analyzer\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This file exists to automate test planning by leveraging AI to analyze code complexity, function relationships, and architecture to determine which functions should be tested and in what priority order, eliminating manual test planning effort and ensuring critical code paths are tested first\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestSetupService.ts",
      "role": "Core Logic",
      "purpose": "Automatically detects the test environment and configuration in a workspace to prepare for generating unit tests",
      "userVisibleActions": [
        "Analyzes workspace to determine programming language and testing framework",
        "Detects existing test configuration files (package.json, jest.config.js, tsconfig.json)",
        "Identifies test directories and file patterns in the project",
        "Generates test setup recommendations based on detected environment",
        "Reports whether test environment is properly configured or needs setup"
      ],
      "developerVisibleActions": [
        "Scans workspace directory structure to identify project type",
        "Counts files by extension to determine primary programming language",
        "Checks for presence of configuration files (package.json, jest.config, tsconfig)",
        "Reads and parses package.json to detect testing frameworks (Jest, Mocha, Vitest, etc.)",
        "Identifies existing test directories (UnitTests, tests, __tests__, spec)",
        "Returns structured test environment information including language, framework, and configuration status",
        "Provides setup plan when test environment is incomplete or missing"
      ],
      "keyFunctions": [
        {
          "name": "detectTestEnvironment",
          "desc": "Analyzes workspace to determine programming language, testing framework, and configuration status",
          "inputs": "workspaceRoot (string path to workspace)",
          "outputs": "TestEnvironment object containing language, framework, config files, and test directories"
        },
        {
          "name": "getAllFiles",
          "desc": "Recursively collects all files in workspace directory for analysis",
          "inputs": "directory path (string)",
          "outputs": "Array of file paths (string[])"
        },
        {
          "name": "buildSetupPrompt",
          "desc": "Creates LLM prompt for generating test setup configuration based on detected environment",
          "inputs": "TestEnvironment object",
          "outputs": "Formatted prompt string for LLM"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "child_process",
        "testSetupTypes",
        "testPrompts",
        "SWLogger"
      ],
      "intent": "This file exists to solve the problem of automatically setting up test environments for different programming languages and frameworks. It eliminates manual configuration by detecting what testing tools are available, what language the project uses, and what configuration is needed to enable automated unit test generation. This is Phase 1 of the test setup process, focusing on environment detection and configuration generation before actual test file creation.",
      "rawContent": "```json\n{\n  \"purpose\": \"Automatically detects the test environment and configuration in a workspace to prepare for generating unit tests\",\n  \"userVisibleActions\": [\n    \"Analyzes workspace to determine programming language and testing framework\",\n    \"Detects existing test configuration files (package.json, jest.config.js, tsconfig.json)\",\n    \"Identifies test directories and file patterns in the project\",\n    \"Generates test setup recommendations based on detected environment\",\n    \"Reports whether test environment is properly configured or needs setup\"\n  ],\n  \"developerVisibleActions\": [\n    \"Scans workspace directory structure to identify project type\",\n    \"Counts files by extension to determine primary programming language\",\n    \"Checks for presence of configuration files (package.json, jest.config, tsconfig)\",\n    \"Reads and parses package.json to detect testing frameworks (Jest, Mocha, Vitest, etc.)\",\n    \"Identifies existing test directories (UnitTests, tests, __tests__, spec)\",\n    \"Returns structured test environment information including language, framework, and configuration status\",\n    \"Provides setup plan when test environment is incomplete or missing\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"detectTestEnvironment\",\n      \"desc\": \"Analyzes workspace to determine programming language, testing framework, and configuration status\",\n      \"inputs\": \"workspaceRoot (string path to workspace)\",\n      \"outputs\": \"TestEnvironment object containing language, framework, config files, and test directories\"\n    },\n    {\n      \"name\": \"getAllFiles\",\n      \"desc\": \"Recursively collects all files in workspace directory for analysis\",\n      \"inputs\": \"directory path (string)\",\n      \"outputs\": \"Array of file paths (string[])\"\n    },\n    {\n      \"name\": \"buildSetupPrompt\",\n      \"desc\": \"Creates LLM prompt for generating test setup configuration based on detected environment\",\n      \"inputs\": \"TestEnvironment object\",\n      \"outputs\": \"Formatted prompt string for LLM\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"child_process\",\n    \"testSetupTypes\",\n    \"testPrompts\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This file exists to solve the problem of automatically setting up test environments for different programming languages and frameworks. It eliminates manual configuration by detecting what testing tools are available, what language the project uses, and what configuration is needed to enable automated unit test generation. This is Phase 1 of the test setup process, focusing on environment detection and configuration generation before actual test file creation.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/llmTestValidationService.ts",
      "role": "Core Logic",
      "purpose": "Validates and automatically fixes failing tests using LLM-powered analysis and code generation",
      "userVisibleActions": [
        "Tests are automatically run and results are displayed showing passed/failed counts",
        "Failing tests are automatically analyzed and fixed without manual intervention",
        "Test validation progress is logged showing attempt numbers and success/failure status",
        "Final test report is generated showing overall test health and remaining failures"
      ],
      "developerVisibleActions": [
        "Developer triggers test validation which runs Jest tests and captures results",
        "Developer receives structured test execution results with pass/fail counts per test file",
        "Developer can specify a single test file or run all tests in workspace",
        "Developer gets iterative fix attempts (up to 3 by default) for each failing test",
        "Developer receives a comprehensive test report with summary statistics",
        "Developer sees logged progress for test execution and fix attempts"
      ],
      "keyFunctions": [
        {
          "name": "runTests",
          "desc": "Executes all tests or a specific test file and returns aggregated results",
          "inputs": "workspaceRoot: string, testFile?: string",
          "outputs": "Promise<TestExecutionResult[]> - array of test results with pass/fail counts"
        },
        {
          "name": "fixFailingTest",
          "desc": "Attempts to automatically fix a failing test using LLM with multiple retry attempts",
          "inputs": "testFilePath: string, executionResult: TestExecutionResult, workspaceRoot: string, llmService: any, maxAttempts?: number",
          "outputs": "Promise<{success: boolean, attempts: number, finalError?: string}> - fix outcome"
        },
        {
          "name": "validateAndFix",
          "desc": "Orchestrates the complete test validation and auto-fix workflow for all failing tests",
          "inputs": "workspaceRoot: string, llmService: any, testFile?: string, maxAttempts?: number",
          "outputs": "Promise<TestReport> - comprehensive report of test execution and fix attempts"
        },
        {
          "name": "generateTestReport",
          "desc": "Creates a structured summary report of all test results and fix outcomes",
          "inputs": "results: TestExecutionResult[], fixes: Map<string, {success: boolean, attempts: number}>",
          "outputs": "TestReport - formatted report with statistics and per-file details"
        }
      ],
      "dependencies": [
        "fs",
        "path",
        "TestExecutionService",
        "TestExecutionResult, TestReport, TestReportSummary (types)",
        "buildFixPrompt (prompts)",
        "SWLogger"
      ],
      "intent": "This file exists to automate the test validation and fixing workflow by running tests, detecting failures, and using LLM to intelligently repair broken tests. It solves the problem of manual test maintenance by providing automated analysis and fixes for failing tests, reducing developer time spent debugging test failures.",
      "rawContent": "```json\n{\n  \"purpose\": \"Validates and automatically fixes failing tests using LLM-powered analysis and code generation\",\n  \"userVisibleActions\": [\n    \"Tests are automatically run and results are displayed showing passed/failed counts\",\n    \"Failing tests are automatically analyzed and fixed without manual intervention\",\n    \"Test validation progress is logged showing attempt numbers and success/failure status\",\n    \"Final test report is generated showing overall test health and remaining failures\"\n  ],\n  \"developerVisibleActions\": [\n    \"Developer triggers test validation which runs Jest tests and captures results\",\n    \"Developer receives structured test execution results with pass/fail counts per test file\",\n    \"Developer can specify a single test file or run all tests in workspace\",\n    \"Developer gets iterative fix attempts (up to 3 by default) for each failing test\",\n    \"Developer receives a comprehensive test report with summary statistics\",\n    \"Developer sees logged progress for test execution and fix attempts\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"runTests\",\n      \"desc\": \"Executes all tests or a specific test file and returns aggregated results\",\n      \"inputs\": \"workspaceRoot: string, testFile?: string\",\n      \"outputs\": \"Promise<TestExecutionResult[]> - array of test results with pass/fail counts\"\n    },\n    {\n      \"name\": \"fixFailingTest\",\n      \"desc\": \"Attempts to automatically fix a failing test using LLM with multiple retry attempts\",\n      \"inputs\": \"testFilePath: string, executionResult: TestExecutionResult, workspaceRoot: string, llmService: any, maxAttempts?: number\",\n      \"outputs\": \"Promise<{success: boolean, attempts: number, finalError?: string}> - fix outcome\"\n    },\n    {\n      \"name\": \"validateAndFix\",\n      \"desc\": \"Orchestrates the complete test validation and auto-fix workflow for all failing tests\",\n      \"inputs\": \"workspaceRoot: string, llmService: any, testFile?: string, maxAttempts?: number\",\n      \"outputs\": \"Promise<TestReport> - comprehensive report of test execution and fix attempts\"\n    },\n    {\n      \"name\": \"generateTestReport\",\n      \"desc\": \"Creates a structured summary report of all test results and fix outcomes\",\n      \"inputs\": \"results: TestExecutionResult[], fixes: Map<string, {success: boolean, attempts: number}>\",\n      \"outputs\": \"TestReport - formatted report with statistics and per-file details\"\n    }\n  ],\n  \"dependencies\": [\n    \"fs\",\n    \"path\",\n    \"TestExecutionService\",\n    \"TestExecutionResult, TestReport, TestReportSummary (types)\",\n    \"buildFixPrompt (prompts)\",\n    \"SWLogger\"\n  ],\n  \"intent\": \"This file exists to automate the test validation and fixing workflow by running tests, detecting failures, and using LLM to intelligently repair broken tests. It solves the problem of manual test maintenance by providing automated analysis and fixes for failing tests, reducing developer time spent debugging test failures.\"\n}\n```"
    },
    {
      "file": "src/domain/services/testing/testExecutionService.ts",
      "role": "Core Logic",
      "purpose": "Executes test suites (Jest, Pytest) and captures their results for display to users and developers",
      "userVisibleActions": [
        "Tests run when user triggers test execution",
        "Test results appear showing passed, failed, and error counts",
        "Test execution status displays (running, success, error)",
        "Individual test failures show with error messages and stack traces",
        "Test execution progress shows duration/timing information"
      ],
      "developerVisibleActions": [
        "Triggers Jest tests for specific files or entire test suite",
        "Triggers Pytest tests for specific files or entire test suite",
        "Parses test framework output to extract structured results",
        "Handles test execution errors and timeouts",
        "Returns standardized test results regardless of testing framework",
        "Provides detailed error information including stack traces"
      ],
      "keyFunctions": [
        {
          "name": "runJest",
          "desc": "Executes Jest tests and returns structured results",
          "inputs": "workspaceRoot (string), optional testFile (string)",
          "outputs": "Promise<TestExecutionResult[]> containing test outcomes"
        },
        {
          "name": "runPytest",
          "desc": "Executes Pytest tests and returns structured results",
          "inputs": "workspaceRoot (string), optional testFile (string)",
          "outputs": "Promise<TestExecutionResult[]> containing test outcomes"
        },
        {
          "name": "parseJestOutput",
          "desc": "Converts Jest JSON output into standardized test result format",
          "inputs": "stdout (string), stderr (string)",
          "outputs": "TestExecutionResult[] with parsed test data"
        },
        {
          "name": "parsePytestOutput",
          "desc": "Converts Pytest JSON output into standardized test result format",
          "inputs": "stdout (string), stderr (string)",
          "outputs": "TestExecutionResult[] with parsed test data"
        }
      ],
      "dependencies": [
        "child_process",
        "path",
        "./types/testResultTypes"
      ],
      "intent": "Provides a unified interface for executing different testing frameworks (Jest for JavaScript/TypeScript, Pytest for Python) and normalizing their outputs into a consistent format that can be displayed to users and processed by other services, handling execution failures gracefully and providing detailed error information.",
      "rawContent": "```json\n{\n  \"purpose\": \"Executes test suites (Jest, Pytest) and captures their results for display to users and developers\",\n  \"userVisibleActions\": [\n    \"Tests run when user triggers test execution\",\n    \"Test results appear showing passed, failed, and error counts\",\n    \"Test execution status displays (running, success, error)\",\n    \"Individual test failures show with error messages and stack traces\",\n    \"Test execution progress shows duration/timing information\"\n  ],\n  \"developerVisibleActions\": [\n    \"Triggers Jest tests for specific files or entire test suite\",\n    \"Triggers Pytest tests for specific files or entire test suite\",\n    \"Parses test framework output to extract structured results\",\n    \"Handles test execution errors and timeouts\",\n    \"Returns standardized test results regardless of testing framework\",\n    \"Provides detailed error information including stack traces\"\n  ],\n  \"keyFunctions\": [\n    {\n      \"name\": \"runJest\",\n      \"desc\": \"Executes Jest tests and returns structured results\",\n      \"inputs\": \"workspaceRoot (string), optional testFile (string)\",\n      \"outputs\": \"Promise<TestExecutionResult[]> containing test outcomes\"\n    },\n    {\n      \"name\": \"runPytest\",\n      \"desc\": \"Executes Pytest tests and returns structured results\",\n      \"inputs\": \"workspaceRoot (string), optional testFile (string)\",\n      \"outputs\": \"Promise<TestExecutionResult[]> containing test outcomes\"\n    },\n    {\n      \"name\": \"parseJestOutput\",\n      \"desc\": \"Converts Jest JSON output into standardized test result format\",\n      \"inputs\": \"stdout (string), stderr (string)\",\n      \"outputs\": \"TestExecutionResult[] with parsed test data\"\n    },\n    {\n      \"name\": \"parsePytestOutput\",\n      \"desc\": \"Converts Pytest JSON output into standardized test result format\",\n      \"inputs\": \"stdout (string), stderr (string)\",\n      \"outputs\": \"TestExecutionResult[] with parsed test data\"\n    }\n  ],\n  \"dependencies\": [\n    \"child_process\",\n    \"path\",\n    \"./types/testResultTypes\"\n  ],\n  \"intent\": \"Provides a unified interface for executing different testing frameworks (Jest for JavaScript/TypeScript, Pytest for Python) and normalizing their outputs into a consistent format that can be displayed to users and processed by other services, handling execution failures gracefully and providing detailed error information.\"\n}\n```"
    }
  ],
  "endpoints": [],
  "commands": [],
  "workers": [],
  "_metadata": {
    "index": 0,
    "total": 0,
    "savedAt": "2025-11-21T01:46:15.309Z"
  }
}