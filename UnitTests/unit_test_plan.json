{
  "rationale": "These unit tests provide comprehensive coverage of Shadow Watch's critical functionality including: (1) Core analysis functions that detect architecture issues like god objects, circular dependencies, and complexity - these are the primary value proposition and must be thoroughly tested. (2) LLM integration layer that communicates with AI providers - this is mission-critical and failures here directly impact user experience. (3) Provider abstraction that enables multi-model support - essential for the product's flexibility goals. (4) Caching and storage systems that optimize performance - failures here cause performance degradation. (5) Error handling that ensures reliable operation - critical for production quality. (6) Configuration management that controls extension behavior - errors here cause feature failures. (7) Extension activation and commands that integrate with VS Code - the entry point must work reliably. Tests focus on both success paths and failure scenarios including edge cases, null handling, and error propagation. The test plan prioritizes high-risk areas identified in the architecture analysis (God Object services, duplicate analyzers, circular dependencies) and uses mocking to isolate units for fast, reliable test execution. All tests include executable TypeScript code using Jest framework with proper imports, setup, mocks, and assertions ready to run immediately.",
  "aggregated_plan": {
    "unit_test_plan": {
      "strategy": "Test TypeScript codebase using Jest with focus on unit testing individual functions in isolation. Mock external dependencies including file system, VSCode API, network calls, and LLM providers. Test core analysis functions, LLM integration, UI components, and error handling paths. Prioritize testing God Object services (llmService.ts, llmIntegration.ts) and critical analysis logic (analyzer.ts, enhancedAnalyzer.ts). Use dependency injection where possible to enable mocking. Test both success and failure paths including edge cases, null handling, and error propagation.",
      "testing_framework": "jest",
      "mocking_approach": "Use Jest's built-in mocking (jest.fn(), jest.mock()) for functions and modules. Mock VSCode API with jest.mock('vscode'). Mock file system operations with jest.mock('fs'). Use jest.spyOn() for partial mocks. Create mock implementations of interfaces (ILLMProvider, AnalysisResult) for dependency injection. Mock LLM API responses with predefined JSON fixtures. Use jest.clearAllMocks() in beforeEach to ensure test isolation.",
      "isolation_strategy": "Test functions in complete isolation by mocking all external dependencies. For pure functions (formatters, parsers), test without mocks. For service classes, mock all injected dependencies. For UI components, mock VSCode APIs and data services. For analyzers, mock file system and parser outputs. Test orchestration functions by mocking all called services and verifying interaction patterns."
    },
    "test_suites": [
      {
        "id": "analyzer-suite",
        "name": "Analyzer Core Tests",
        "description": "Tests for core code analysis functionality including god object detection, circular dependencies, complexity calculation, and health scoring",
        "test_file_path": "src/test/analyzer.test.ts",
        "source_files": [
          "src/analyzer.ts"
        ],
        "test_cases": [
          {
            "id": "test-detect-god-objects",
            "name": "test_detectGodObjects_identifies_large_files",
            "description": "Verifies detectGodObjects correctly identifies files exceeding line count threshold",
            "target_function": "detectGodObjects",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "file exceeds threshold",
              "file below threshold",
              "empty file",
              "multiple files mixed"
            ],
            "mocks": [
              "file system reads"
            ],
            "assertions": [
              "returns array of god objects",
              "includes file path and line count",
              "empty array when no god objects"
            ],
            "priority": "high",
            "test_code": "import { Analyzer } from '../analyzer';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('Analyzer.detectGodObjects', () => {\n  let analyzer: Analyzer;\n  const mockFs = fs as jest.Mocked;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    analyzer = new Analyzer('');\n  });\n\n  it('should identify files exceeding line count threshold', () => {\n    const largeFileContent = 'line\\n'.repeat(600);\n    mockFs.readFileSync.mockReturnValue(largeFileContent);\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.statSync.mockReturnValue({ isFile: () => true, isDirectory: () => false } as any);\n\n    const result = analyzer.detectGodObjects(['src/large.ts'], 500);\n\n    expect(result).toHaveLength(1);\n    expect(result[0]).toHaveProperty('file');\n    expect(result[0]).toHaveProperty('lines');\n    expect(result[0].lines).toBeGreaterThan(500);\n  });\n\n  it('should return empty array when no files exceed threshold', () => {\n    const smallFileContent = 'line\\n'.repeat(100);\n    mockFs.readFileSync.mockReturnValue(smallFileContent);\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.statSync.mockReturnValue({ isFile: () => true, isDirectory: () => false } as any);\n\n    const result = analyzer.detectGodObjects(['src/small.ts'], 500);\n\n    expect(result).toHaveLength(0);\n  });\n\n  it('should handle empty files', () => {\n    mockFs.readFileSync.mockReturnValue('');\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.statSync.mockReturnValue({ isFile: () => true, isDirectory: () => false } as any);\n\n    const result = analyzer.detectGodObjects(['src/empty.ts'], 500);\n\n    expect(result).toHaveLength(0);\n  });\n\n  it('should handle multiple files with mixed sizes', () => {\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.statSync.mockReturnValue({ isFile: () => true, isDirectory: () => false } as any);\n    mockFs.readFileSync.mockImplementation((path: any) => {\n      if (path.includes('large')) return 'line\\n'.repeat(600);\n      return 'line\\n'.repeat(100);\n    });\n\n    const result = analyzer.detectGodObjects(['src/large.ts', 'src/small.ts'], 500);\n\n    expect(result).toHaveLength(1);\n    expect(result[0].file).toContain('large');\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t test_detectGodObjects_identifies_large_files"
          },
          {
            "id": "test-circular-dependencies",
            "name": "test_findCircularDependencies_detects_cycles",
            "description": "Verifies findCircularDependencies correctly identifies circular import chains",
            "target_function": "findCircularDependencies",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "simple A->B->A cycle",
              "complex A->B->C->A cycle",
              "no cycles present",
              "self-reference"
            ],
            "mocks": [
              "file system reads",
              "import parsing"
            ],
            "assertions": [
              "returns array of circular dependency chains",
              "includes all files in cycle",
              "empty array when no cycles"
            ],
            "priority": "high",
            "test_code": "import { Analyzer } from '../analyzer';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('Analyzer.findCircularDependencies', () => {\n  let analyzer: Analyzer;\n  const mockFs = fs as jest.Mocked;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    analyzer = new Analyzer('');\n  });\n\n  it('should detect simple A->B->A cycle', () => {\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.statSync.mockReturnValue({ isFile: () => true, isDirectory: () => false } as any);\n    mockFs.readFileSync.mockImplementation((path: any) => {\n      if (path.includes('fileA.ts')) return \"import { b } from './fileB';\";\n      if (path.includes('fileB.ts')) return \"import { a } from './fileA';\";\n      return '';\n    });\n\n    const result = analyzer.findCircularDependencies(['src/fileA.ts', 'src/fileB.ts']);\n\n    expect(result.length).toBeGreaterThan(0);\n    expect(result[0]).toContain('fileA');\n    expect(result[0]).toContain('fileB');\n  });\n\n  it('should detect complex A->B->C->A cycle', () => {\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.statSync.mockReturnValue({ isFile: () => true, isDirectory: () => false } as any);\n    mockFs.readFileSync.mockImplementation((path: any) => {\n      if (path.includes('fileA.ts')) return \"import { b } from './fileB';\";\n      if (path.includes('fileB.ts')) return \"import { c } from './fileC';\";\n      if (path.includes('fileC.ts')) return \"import { a } from './fileA';\";\n      return '';\n    });\n\n    const result = analyzer.findCircularDependencies(['src/fileA.ts', 'src/fileB.ts', 'src/fileC.ts']);\n\n    expect(result.length).toBeGreaterThan(0);\n    expect(result[0]).toContain('fileA');\n    expect(result[0]).toContain('fileB');\n    expect(result[0]).toContain('fileC');\n  });\n\n  it('should return empty array when no cycles present', () => {\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.statSync.mockReturnValue({ isFile: () => true, isDirectory: () => false } as any);\n    mockFs.readFileSync.mockImplementation((path: any) => {\n      if (path.includes('fileA.ts')) return \"import { b } from './fileB';\";\n      if (path.includes('fileB.ts')) return \"import { c } from './fileC';\";\n      if (path.includes('fileC.ts')) return \"const c = 1;\";\n      return '';\n    });\n\n    const result = analyzer.findCircularDependencies(['src/fileA.ts', 'src/fileB.ts', 'src/fileC.ts']);\n\n    expect(result).toHaveLength(0);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t test_findCircularDependencies_detects_cycles"
          },
          {
            "id": "test-calculate-complexity",
            "name": "test_calculateComplexity_measures_cyclomatic_complexity",
            "description": "Verifies calculateComplexity correctly measures function cyclomatic complexity",
            "target_function": "calculateComplexity",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "simple linear function",
              "function with if statements",
              "function with loops",
              "nested conditionals"
            ],
            "mocks": [
              "AST parsing"
            ],
            "assertions": [
              "returns complexity score",
              "higher score for more branches",
              "complexity 1 for linear code"
            ],
            "priority": "high",
            "test_code": "import { Analyzer } from '../analyzer';\n\ndescribe('Analyzer.calculateComplexity', () => {\n  let analyzer: Analyzer;\n\n  beforeEach(() => {\n    analyzer = new Analyzer('');\n  });\n\n  it('should return complexity 1 for linear code', () => {\n    const simpleCode = `\n      function simple() {\n        const a = 1;\n        const b = 2;\n        return a + b;\n      }\n    `;\n\n    const result = analyzer.calculateComplexity(simpleCode);\n\n    expect(result).toBe(1);\n  });\n\n  it('should increase complexity for if statements', () => {\n    const codeWithIf = `\n      function withIf(x: number) {\n        if (x > 0) {\n          return 1;\n        }\n        return 0;\n      }\n    `;\n\n    const result = analyzer.calculateComplexity(codeWithIf);\n\n    expect(result).toBeGreaterThan(1);\n  });\n\n  it('should increase complexity for loops', () => {\n    const codeWithLoop = `\n      function withLoop(arr: number[]) {\n        for (let i = 0; i  0) {\n            return arr[i];\n          }\n        }\n        return 0;\n      }\n    `;\n\n    const result = analyzer.calculateComplexity(codeWithLoop);\n\n    expect(result).toBeGreaterThan(2);\n  });\n\n  it('should handle nested conditionals', () => {\n    const nestedCode = `\n      function nested(x: number, y: number) {\n        if (x > 0) {\n          if (y > 0) {\n            return 1;\n          }\n          return 2;\n        }\n        return 0;\n      }\n    `;\n\n    const result = analyzer.calculateComplexity(nestedCode);\n\n    expect(result).toBeGreaterThan(2);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t test_calculateComplexity_measures_cyclomatic_complexity"
          },
          {
            "id": "test-calculate-health-score",
            "name": "test_calculateHealthScore_computes_percentage",
            "description": "Verifies calculateHealthScore correctly computes overall codebase health percentage based on issues",
            "target_function": "calculateHealthScore",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "no issues returns 100%",
              "errors decrease score significantly",
              "warnings decrease score moderately",
              "info issues minimal impact"
            ],
            "mocks": [
              "none - pure function"
            ],
            "assertions": [
              "returns value between 0-100",
              "100% for zero issues",
              "decreases with issue count",
              "errors weighted more than warnings"
            ],
            "priority": "high",
            "test_code": "import { Analyzer } from '../analyzer';\n\ndescribe('Analyzer.calculateHealthScore', () => {\n  let analyzer: Analyzer;\n\n  beforeEach(() => {\n    analyzer = new Analyzer('');\n  });\n\n  it('should return 100% for no issues', () => {\n    const score = analyzer.calculateHealthScore([]);\n\n    expect(score).toBe(100);\n  });\n\n  it('should return value between 0-100', () => {\n    const issues = [\n      { severity: 'error', category: 'complexity', description: 'test', file: 'test.ts' },\n      { severity: 'warning', category: 'style', description: 'test', file: 'test.ts' }\n    ];\n\n    const score = analyzer.calculateHealthScore(issues);\n\n    expect(score).toBeGreaterThanOrEqual(0);\n    expect(score).toBeLessThanOrEqual(100);\n  });\n\n  it('should decrease score with issue count', () => {\n    const fewIssues = [{ severity: 'warning', category: 'style', description: 'test', file: 'test.ts' }];\n    const manyIssues = [\n      { severity: 'warning', category: 'style', description: 'test1', file: 'test.ts' },\n      { severity: 'warning', category: 'style', description: 'test2', file: 'test.ts' },\n      { severity: 'warning', category: 'style', description: 'test3', file: 'test.ts' }\n    ];\n\n    const scoreFew = analyzer.calculateHealthScore(fewIssues);\n    const scoreMany = analyzer.calculateHealthScore(manyIssues);\n\n    expect(scoreMany).toBeLessThan(scoreFew);\n  });\n\n  it('should weight errors more than warnings', () => {\n    const withErrors = [\n      { severity: 'error', category: 'complexity', description: 'test', file: 'test.ts' },\n      { severity: 'error', category: 'complexity', description: 'test', file: 'test.ts' }\n    ];\n    const withWarnings = [\n      { severity: 'warning', category: 'style', description: 'test', file: 'test.ts' },\n      { severity: 'warning', category: 'style', description: 'test', file: 'test.ts' }\n    ];\n\n    const scoreErrors = analyzer.calculateHealthScore(withErrors);\n    const scoreWarnings = analyzer.calculateHealthScore(withWarnings);\n\n    expect(scoreErrors).toBeLessThan(scoreWarnings);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t test_calculateHealthScore_computes_percentage"
          }
        ]
      },
      {
        "id": "llm-service-suite",
        "name": "LLM Service Tests",
        "description": "Tests for LLM service including API communication, response parsing, rate limiting, retry logic, and error handling",
        "test_file_path": "src/test/llmService.test.ts",
        "source_files": [
          "src/llmService.ts"
        ],
        "test_cases": [
          {
            "id": "test-llm-api-call",
            "name": "test_callLLMAPI_sends_request_successfully",
            "description": "Verifies LLM API call sends request with correct parameters and returns response",
            "target_function": "callLLMAPI",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "successful API call",
              "API returns valid JSON",
              "request includes prompt",
              "request includes model config"
            ],
            "mocks": [
              "HTTP client",
              "provider factory",
              "configuration manager"
            ],
            "assertions": [
              "returns parsed response",
              "calls provider with correct params",
              "handles valid JSON response"
            ],
            "priority": "high",
            "test_code": "import { LLMService } from '../llmService';\nimport { ProviderFactory } from '../ai/providers/providerFactory';\nimport { ConfigurationManager } from '../config/configurationManager';\n\njest.mock('../ai/providers/providerFactory');\njest.mock('../config/configurationManager');\n\ndescribe('LLMService.callLLMAPI', () => {\n  let llmService: LLMService;\n  let mockProvider: any;\n  let mockConfig: any;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockProvider = {\n      sendRequest: jest.fn().mockResolvedValue({ content: 'test response', usage: { tokens: 100 } })\n    };\n    mockConfig = {\n      getProvider: jest.fn().mockReturnValue('openai'),\n      getModel: jest.fn().mockReturnValue('gpt-4'),\n      getApiKey: jest.fn().mockReturnValue('test-key')\n    };\n    (ProviderFactory.createProvider as jest.Mock).mockReturnValue(mockProvider);\n    (ConfigurationManager.getInstance as jest.Mock).mockReturnValue(mockConfig);\n    llmService = new LLMService();\n  });\n\n  it('should send request with correct parameters', async () => {\n    const prompt = 'test prompt';\n    const systemPrompt = 'system prompt';\n\n    await llmService.callLLMAPI(prompt, systemPrompt);\n\n    expect(mockProvider.sendRequest).toHaveBeenCalledWith(\n      expect.objectContaining({\n        messages: expect.arrayContaining([\n          expect.objectContaining({ role: 'system', content: systemPrompt }),\n          expect.objectContaining({ role: 'user', content: prompt })\n        ])\n      })\n    );\n  });\n\n  it('should return parsed response', async () => {\n    const prompt = 'test prompt';\n    const expected = { content: 'test response', usage: { tokens: 100 } };\n\n    const result = await llmService.callLLMAPI(prompt, 'system');\n\n    expect(result).toEqual(expected);\n  });\n\n  it('should handle valid JSON response', async () => {\n    const jsonResponse = { data: { key: 'value' } };\n    mockProvider.sendRequest.mockResolvedValue({ content: JSON.stringify(jsonResponse), usage: { tokens: 100 } });\n\n    const result = await llmService.callLLMAPI('test', 'system');\n\n    expect(result.content).toBe(JSON.stringify(jsonResponse));\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t test_callLLMAPI_sends_request_successfully"
          },
          {
            "id": "test-llm-parse-response",
            "name": "test_parseResponse_validates_json_schema",
            "description": "Verifies parseResponse correctly validates LLM response against JSON schema",
            "target_function": "parseResponse",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "valid JSON matching schema",
              "invalid JSON syntax",
              "missing required fields",
              "extra fields allowed"
            ],
            "mocks": [
              "schema validator"
            ],
            "assertions": [
              "returns parsed object",
              "throws error for invalid JSON",
              "throws error for schema violations",
              "preserves valid data"
            ],
            "priority": "high",
            "test_code": "import { LLMService } from '../llmService';\n\ndescribe('LLMService.parseResponse', () => {\n  let llmService: LLMService;\n\n  beforeEach(() => {\n    llmService = new LLMService();\n  });\n\n  it('should parse valid JSON matching schema', () => {\n    const validJson = JSON.stringify({\n      insights: [{ type: 'architecture', description: 'test', severity: 'info' }],\n      summary: 'test summary'\n    });\n\n    const result = llmService.parseResponse(validJson, 'insights');\n\n    expect(result).toHaveProperty('insights');\n    expect(result).toHaveProperty('summary');\n    expect(Array.isArray(result.insights)).toBe(true);\n  });\n\n  it('should throw error for invalid JSON syntax', () => {\n    const invalidJson = '{ invalid json';\n\n    expect(() => llmService.parseResponse(invalidJson, 'insights')).toThrow();\n  });\n\n  it('should throw error for missing required fields', () => {\n    const missingFields = JSON.stringify({ summary: 'test' });\n\n    expect(() => llmService.parseResponse(missingFields, 'insights')).toThrow();\n  });\n\n  it('should preserve valid data structure', () => {\n    const validJson = JSON.stringify({\n      insights: [\n        { type: 'architecture', description: 'test insight', severity: 'warning', file: 'test.ts' }\n      ],\n      summary: 'test summary',\n      metadata: { analyzed_at: '2024-01-01' }\n    });\n\n    const result = llmService.parseResponse(validJson, 'insights');\n\n    expect(result.insights[0].type).toBe('architecture');\n    expect(result.insights[0].description).toBe('test insight');\n    expect(result.summary).toBe('test summary');\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t test_parseResponse_validates_json_schema"
          },
          {
            "id": "test-llm-rate-limiting",
            "name": "test_rateLimiter_throttles_requests",
            "description": "Verifies rate limiter correctly throttles requests to configured rate",
            "target_function": "rateLimiter",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "requests within limit pass through",
              "requests exceeding limit queued",
              "queue processes after delay",
              "multiple concurrent requests"
            ],
            "mocks": [
              "timer functions"
            ],
            "assertions": [
              "allows requests within limit",
              "queues excess requests",
              "processes queue in order",
              "respects rate limit"
            ],
            "priority": "high",
            "test_code": "import { LLMRateLimiter } from '../ai/llmRateLimiter';\n\njest.useFakeTimers();\n\ndescribe('LLMRateLimiter.throttle', () => {\n  let rateLimiter: LLMRateLimiter;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    rateLimiter = new LLMRateLimiter(2, 1000);\n  });\n\n  afterEach(() => {\n    jest.clearAllTimers();\n  });\n\n  it('should allow requests within limit', async () => {\n    const fn = jest.fn().mockResolvedValue('result');\n\n    const promise1 = rateLimiter.throttle(fn);\n    const promise2 = rateLimiter.throttle(fn);\n\n    await Promise.all([promise1, promise2]);\n\n    expect(fn).toHaveBeenCalledTimes(2);\n  });\n\n  it('should queue requests exceeding limit', async () => {\n    const fn = jest.fn().mockResolvedValue('result');\n\n    rateLimiter.throttle(fn);\n    rateLimiter.throttle(fn);\n    const promise3 = rateLimiter.throttle(fn);\n\n    expect(fn).toHaveBeenCalledTimes(2);\n\n    jest.advanceTimersByTime(1000);\n    await promise3;\n\n    expect(fn).toHaveBeenCalledTimes(3);\n  });\n\n  it('should process queue in order', async () => {\n    const results: number[] = [];\n    const fn = (id: number) => {\n      results.push(id);\n      return Promise.resolve(id);\n    };\n\n    rateLimiter.throttle(() => fn(1));\n    rateLimiter.throttle(() => fn(2));\n    rateLimiter.throttle(() => fn(3));\n\n    jest.advanceTimersByTime(2000);\n    await Promise.resolve();\n\n    expect(results).toEqual([1, 2, 3]);\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t test_rateLimiter_throttles_requests"
          },
          {
            "id": "test-llm-retry-logic",
            "name": "test_retryHandler_retries_failed_requests",
            "description": "Verifies retry handler retries failed requests with exponential backoff",
            "target_function": "retryHandler",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "success on first try",
              "success after retry",
              "max retries exceeded",
              "non-retryable error"
            ],
            "mocks": [
              "API call function",
              "timer functions"
            ],
            "assertions": [
              "succeeds without retry",
              "retries on transient failure",
              "gives up after max retries",
              "immediate failure for non-retryable"
            ],
            "priority": "high",
            "test_code": "import { LLMRetryHandler } from '../ai/llmRetryHandler';\n\njest.useFakeTimers();\n\ndescribe('LLMRetryHandler.retry', () => {\n  let retryHandler: LLMRetryHandler;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    retryHandler = new LLMRetryHandler(3, 1000);\n  });\n\n  afterEach(() => {\n    jest.clearAllTimers();\n  });\n\n  it('should succeed on first try without retry', async () => {\n    const fn = jest.fn().mockResolvedValue('success');\n\n    const result = await retryHandler.retry(fn);\n\n    expect(result).toBe('success');\n    expect(fn).toHaveBeenCalledTimes(1);\n  });\n\n  it('should retry on transient failure and succeed', async () => {\n    const fn = jest.fn()\n      .mockRejectedValueOnce(new Error('Network error'))\n      .mockResolvedValueOnce('success');\n\n    const promise = retryHandler.retry(fn);\n    jest.advanceTimersByTime(1000);\n    const result = await promise;\n\n    expect(result).toBe('success');\n    expect(fn).toHaveBeenCalledTimes(2);\n  });\n\n  it('should give up after max retries', async () => {\n    const fn = jest.fn().mockRejectedValue(new Error('Persistent error'));\n\n    const promise = retryHandler.retry(fn);\n    jest.advanceTimersByTime(10000);\n\n    await expect(promise).rejects.toThrow('Persistent error');\n    expect(fn).toHaveBeenCalledTimes(4);\n  });\n\n  it('should fail immediately for non-retryable errors', async () => {\n    const authError = new Error('Authentication failed');\n    authError.name = 'AuthenticationError';\n    const fn = jest.fn().mockRejectedValue(authError);\n\n    await expect(retryHandler.retry(fn)).rejects.toThrow('Authentication failed');\n    expect(fn).toHaveBeenCalledTimes(1);\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t test_retryHandler_retries_failed_requests"
          }
        ]
      },
      {
        "id": "llm-integration-suite",
        "name": "LLM Integration Tests",
        "description": "Tests for high-level LLM integration including product docs generation, insights generation, and unit test generation",
        "test_file_path": "src/test/llmIntegration.test.ts",
        "source_files": [
          "src/llmIntegration.ts"
        ],
        "test_cases": [
          {
            "id": "test-generate-product-docs",
            "name": "test_generateProductDocs_creates_documentation",
            "description": "Verifies generateProductDocs creates comprehensive product documentation from codebase",
            "target_function": "generateProductDocs",
            "target_file": "src/llmIntegration.ts",
            "scenarios": [
              "successful generation",
              "handles API failure",
              "caches results",
              "validates response format"
            ],
            "mocks": [
              "LLM service",
              "file system",
              "codebase analyzer"
            ],
            "assertions": [
              "returns documentation object",
              "includes all sections",
              "saves to file system",
              "validates schema"
            ],
            "priority": "high",
            "test_code": "import { generateProductDocs } from '../llmIntegration';\nimport { LLMService } from '../llmService';\nimport * as fs from 'fs';\n\njest.mock('../llmService');\njest.mock('fs');\n\ndescribe('generateProductDocs', () => {\n  let mockLLMService: jest.Mocked;\n  const mockFs = fs as jest.Mocked;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockLLMService = {\n      callLLMAPI: jest.fn().mockResolvedValue({\n        content: JSON.stringify({\n          product_overview: 'Test product overview',\n          key_features: ['Feature 1', 'Feature 2'],\n          architecture_summary: 'Architecture summary',\n          user_workflows: ['Workflow 1']\n        })\n      })\n    } as any;\n    mockFs.writeFileSync.mockImplementation(() => {});\n    mockFs.mkdirSync.mockImplementation(() => {});\n  });\n\n  it('should create documentation with all sections', async () => {\n    const result = await generateProductDocs(mockLLMService, '/test/workspace');\n\n    expect(result).toHaveProperty('product_overview');\n    expect(result).toHaveProperty('key_features');\n    expect(result).toHaveProperty('architecture_summary');\n    expect(result).toHaveProperty('user_workflows');\n  });\n\n  it('should save documentation to file system', async () => {\n    await generateProductDocs(mockLLMService, '/test/workspace');\n\n    expect(mockFs.writeFileSync).toHaveBeenCalled();\n    expect(mockFs.writeFileSync).toHaveBeenCalledWith(\n      expect.stringContaining('.shadow'),\n      expect.any(String)\n    );\n  });\n\n  it('should handle API failure gracefully', async () => {\n    mockLLMService.callLLMAPI.mockRejectedValue(new Error('API Error'));\n\n    await expect(generateProductDocs(mockLLMService, '/test/workspace')).rejects.toThrow('API Error');\n  });\n\n  it('should validate response format', async () => {\n    mockLLMService.callLLMAPI.mockResolvedValue({\n      content: JSON.stringify({ invalid: 'format' })\n    });\n\n    await expect(generateProductDocs(mockLLMService, '/test/workspace')).rejects.toThrow();\n  });\n});",
            "run_instructions": "npm test -- llmIntegration.test.ts -t test_generateProductDocs_creates_documentation"
          },
          {
            "id": "test-generate-llm-insights",
            "name": "test_generateLLMInsights_creates_architectural_insights",
            "description": "Verifies generateLLMInsights creates architectural insights from code analysis",
            "target_function": "generateLLMInsights",
            "target_file": "src/llmIntegration.ts",
            "scenarios": [
              "successful generation",
              "multiple insight types",
              "handles empty codebase",
              "validates insight structure"
            ],
            "mocks": [
              "LLM service",
              "analyzer",
              "file system"
            ],
            "assertions": [
              "returns insights array",
              "includes insight metadata",
              "categorizes by type",
              "saves results"
            ],
            "priority": "high",
            "test_code": "import { generateLLMInsights } from '../llmIntegration';\nimport { LLMService } from '../llmService';\nimport { Analyzer } from '../analyzer';\n\njest.mock('../llmService');\njest.mock('../analyzer');\n\ndescribe('generateLLMInsights', () => {\n  let mockLLMService: jest.Mocked;\n  let mockAnalyzer: jest.Mocked;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockLLMService = {\n      callLLMAPI: jest.fn().mockResolvedValue({\n        content: JSON.stringify({\n          insights: [\n            { type: 'architecture', description: 'Insight 1', severity: 'info', file: 'test.ts' },\n            { type: 'performance', description: 'Insight 2', severity: 'warning', file: 'test.ts' }\n          ],\n          summary: 'Analysis summary'\n        })\n      })\n    } as any;\n    mockAnalyzer = {\n      analyzeWorkspace: jest.fn().mockResolvedValue({\n        healthScore: 85,\n        issues: [],\n        files: ['test.ts']\n      })\n    } as any;\n  });\n\n  it('should return insights array with metadata', async () => {\n    const result = await generateLLMInsights(mockLLMService, mockAnalyzer, '/test/workspace');\n\n    expect(Array.isArray(result.insights)).toBe(true);\n    expect(result.insights).toHaveLength(2);\n    expect(result).toHaveProperty('summary');\n  });\n\n  it('should categorize insights by type', async () => {\n    const result = await generateLLMInsights(mockLLMService, mockAnalyzer, '/test/workspace');\n\n    const types = result.insights.map((i: any) => i.type);\n    expect(types).toContain('architecture');\n    expect(types).toContain('performance');\n  });\n\n  it('should handle empty codebase', async () => {\n    mockAnalyzer.analyzeWorkspace.mockResolvedValue({ healthScore: 100, issues: [], files: [] });\n    mockLLMService.callLLMAPI.mockResolvedValue({\n      content: JSON.stringify({ insights: [], summary: 'No insights' })\n    });\n\n    const result = await generateLLMInsights(mockLLMService, mockAnalyzer, '/test/workspace');\n\n    expect(result.insights).toHaveLength(0);\n    expect(result.summary).toBe('No insights');\n  });\n});",
            "run_instructions": "npm test -- llmIntegration.test.ts -t test_generateLLMInsights_creates_architectural_insights"
          },
          {
            "id": "test-generate-unit-tests",
            "name": "test_generateUnitTests_creates_test_plan",
            "description": "Verifies generateUnitTests creates comprehensive unit test plan from codebase",
            "target_function": "generateUnitTests",
            "target_file": "src/llmIntegration.ts",
            "scenarios": [
              "successful test plan generation",
              "includes test suites",
              "includes test cases",
              "validates test code syntax"
            ],
            "mocks": [
              "LLM service",
              "analyzer",
              "file system"
            ],
            "assertions": [
              "returns test plan object",
              "includes test suites array",
              "test cases have executable code",
              "test cases have run instructions"
            ],
            "priority": "high",
            "test_code": "import { generateUnitTests } from '../llmIntegration';\nimport { LLMService } from '../llmService';\nimport { Analyzer } from '../analyzer';\n\njest.mock('../llmService');\njest.mock('../analyzer');\n\ndescribe('generateUnitTests', () => {\n  let mockLLMService: jest.Mocked;\n  let mockAnalyzer: jest.Mocked;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockLLMService = {\n      callLLMAPI: jest.fn().mockResolvedValue({\n        content: JSON.stringify({\n          unit_test_strategy: {\n            overall_approach: 'Test approach',\n            testing_frameworks: ['jest'],\n            mocking_strategy: 'Mock strategy',\n            isolation_level: 'Unit level'\n          },\n          test_suites: [\n            {\n              id: 'suite-1',\n              name: 'Test Suite 1',\n              description: 'Test suite description',\n              test_file_path: 'src/test/test.test.ts',\n              source_files: ['src/test.ts'],\n              test_cases: [\n                {\n                  id: 'test-1',\n                  name: 'test_function',\n                  description: 'Test description',\n                  target_function: 'targetFunction',\n                  target_file: 'src/test.ts',\n                  priority: 'high',\n                  test_code: 'import { test } from \"src/test\"; test();',\n                  run_instructions: 'npm test -- test.test.ts'\n                }\n              ]\n            }\n          ],\n          rationale: 'Test rationale'\n        })\n      })\n    } as any;\n    mockAnalyzer = {\n      analyzeWorkspace: jest.fn().mockResolvedValue({ files: ['test.ts'], functions: ['testFn'] })\n    } as any;\n  });\n\n  it('should return test plan with test suites', async () => {\n    const result = await generateUnitTests(mockLLMService, mockAnalyzer, '/test/workspace');\n\n    expect(result).toHaveProperty('unit_test_strategy');\n    expect(result).toHaveProperty('test_suites');\n    expect(Array.isArray(result.test_suites)).toBe(true);\n    expect(result.test_suites).toHaveLength(1);\n  });\n\n  it('should include executable test code', async () => {\n    const result = await generateUnitTests(mockLLMService, mockAnalyzer, '/test/workspace');\n\n    const testCase = result.test_suites[0].test_cases[0];\n    expect(testCase.test_code).toBeTruthy();\n    expect(testCase.test_code.length).toBeGreaterThan(0);\n    expect(testCase.test_code).toContain('import');\n  });\n\n  it('should include run instructions', async () => {\n    const result = await generateUnitTests(mockLLMService, mockAnalyzer, '/test/workspace');\n\n    const testCase = result.test_suites[0].test_cases[0];\n    expect(testCase.run_instructions).toBeTruthy();\n    expect(testCase.run_instructions).toContain('npm test');\n  });\n\n  it('should validate test strategy structure', async () => {\n    const result = await generateUnitTests(mockLLMService, mockAnalyzer, '/test/workspace');\n\n    expect(result.unit_test_strategy).toHaveProperty('overall_approach');\n    expect(result.unit_test_strategy).toHaveProperty('testing_frameworks');\n    expect(result.unit_test_strategy).toHaveProperty('mocking_strategy');\n    expect(result.unit_test_strategy).toHaveProperty('isolation_level');\n  });\n});",
            "run_instructions": "npm test -- llmIntegration.test.ts -t test_generateUnitTests_creates_test_plan"
          }
        ]
      },
      {
        "id": "formatter-suite",
        "name": "LLM Formatter Tests",
        "description": "Tests for formatting architecture issues into LLM-specific prompts (Cursor, ChatGPT, generic)",
        "test_file_path": "src/test/llmFormatter.test.ts",
        "source_files": [
          "src/llmFormatter.ts"
        ],
        "test_cases": [
          {
            "id": "test-format-cursor",
            "name": "test_formatForCursor_creates_cursor_prompt",
            "description": "Verifies formatForCursor creates properly structured prompt for Cursor AI",
            "target_function": "formatForCursor",
            "target_file": "src/llmFormatter.ts",
            "scenarios": [
              "formats issues correctly",
              "includes file context",
              "prioritizes by severity",
              "includes code snippets"
            ],
            "mocks": [
              "none - pure function"
            ],
            "assertions": [
              "returns string",
              "includes issue descriptions",
              "formatted as markdown",
              "includes file paths"
            ],
            "priority": "medium",
            "test_code": "import { formatForCursor } from '../llmFormatter';\n\ndescribe('formatForCursor', () => {\n  const sampleIssues = [\n    {\n      severity: 'error',\n      category: 'complexity',\n      description: 'Function too complex',\n      file: 'src/test.ts',\n      line: 10,\n      suggestion: 'Break into smaller functions'\n    },\n    {\n      severity: 'warning',\n      category: 'style',\n      description: 'Inconsistent naming',\n      file: 'src/test.ts',\n      line: 20,\n      suggestion: 'Use camelCase'\n    }\n  ];\n\n  it('should return formatted string', () => {\n    const result = formatForCursor(sampleIssues);\n\n    expect(typeof result).toBe('string');\n    expect(result.length).toBeGreaterThan(0);\n  });\n\n  it('should include issue descriptions', () => {\n    const result = formatForCursor(sampleIssues);\n\n    expect(result).toContain('Function too complex');\n    expect(result).toContain('Inconsistent naming');\n  });\n\n  it('should format as markdown', () => {\n    const result = formatForCursor(sampleIssues);\n\n    expect(result).toContain('#');\n    expect(result).toContain('**');\n    expect(result).toMatch(/\\n/);\n  });\n\n  it('should include file paths', () => {\n    const result = formatForCursor(sampleIssues);\n\n    expect(result).toContain('src/test.ts');\n  });\n\n  it('should prioritize errors over warnings', () => {\n    const result = formatForCursor(sampleIssues);\n\n    const errorIndex = result.indexOf('Function too complex');\n    const warningIndex = result.indexOf('Inconsistent naming');\n\n    expect(errorIndex).toBeLessThan(warningIndex);\n  });\n});",
            "run_instructions": "npm test -- llmFormatter.test.ts -t test_formatForCursor_creates_cursor_prompt"
          },
          {
            "id": "test-format-chatgpt",
            "name": "test_formatForChatGPT_creates_chatgpt_prompt",
            "description": "Verifies formatForChatGPT creates verbose prompt optimized for ChatGPT",
            "target_function": "formatForChatGPT",
            "target_file": "src/llmFormatter.ts",
            "scenarios": [
              "includes detailed context",
              "verbose descriptions",
              "structured sections",
              "includes examples"
            ],
            "mocks": [
              "none - pure function"
            ],
            "assertions": [
              "returns string",
              "more verbose than cursor format",
              "includes context sections",
              "structured with headers"
            ],
            "priority": "medium",
            "test_code": "import { formatForChatGPT, formatForCursor } from '../llmFormatter';\n\ndescribe('formatForChatGPT', () => {\n  const sampleIssues = [\n    {\n      severity: 'error',\n      category: 'complexity',\n      description: 'Function too complex',\n      file: 'src/test.ts',\n      line: 10,\n      suggestion: 'Break into smaller functions'\n    }\n  ];\n\n  it('should return formatted string', () => {\n    const result = formatForChatGPT(sampleIssues);\n\n    expect(typeof result).toBe('string');\n    expect(result.length).toBeGreaterThan(0);\n  });\n\n  it('should be more verbose than cursor format', () => {\n    const chatgptResult = formatForChatGPT(sampleIssues);\n    const cursorResult = formatForCursor(sampleIssues);\n\n    expect(chatgptResult.length).toBeGreaterThan(cursorResult.length);\n  });\n\n  it('should include context sections', () => {\n    const result = formatForChatGPT(sampleIssues);\n\n    expect(result).toContain('Context');\n    expect(result).toContain('Issue');\n    expect(result).toContain('Suggestion');\n  });\n\n  it('should structure with headers', () => {\n    const result = formatForChatGPT(sampleIssues);\n\n    expect(result).toMatch(/##\\s+/);\n    expect(result).toMatch(/###\\s+/);\n  });\n});",
            "run_instructions": "npm test -- llmFormatter.test.ts -t test_formatForChatGPT_creates_chatgpt_prompt"
          },
          {
            "id": "test-format-generic",
            "name": "test_formatGeneric_creates_standard_markdown",
            "description": "Verifies formatGeneric creates standard markdown format for any AI assistant",
            "target_function": "formatGeneric",
            "target_file": "src/llmFormatter.ts",
            "scenarios": [
              "standard markdown format",
              "includes all issue data",
              "readable structure",
              "no assistant-specific optimizations"
            ],
            "mocks": [
              "none - pure function"
            ],
            "assertions": [
              "returns markdown string",
              "includes severity levels",
              "includes file locations",
              "human-readable format"
            ],
            "priority": "medium",
            "test_code": "import { formatGeneric } from '../llmFormatter';\n\ndescribe('formatGeneric', () => {\n  const sampleIssues = [\n    {\n      severity: 'error',\n      category: 'complexity',\n      description: 'Function too complex',\n      file: 'src/test.ts',\n      line: 10,\n      suggestion: 'Break into smaller functions'\n    },\n    {\n      severity: 'warning',\n      category: 'style',\n      description: 'Inconsistent naming',\n      file: 'src/utils.ts',\n      line: 5,\n      suggestion: 'Use camelCase'\n    }\n  ];\n\n  it('should return markdown string', () => {\n    const result = formatGeneric(sampleIssues);\n\n    expect(typeof result).toBe('string');\n    expect(result).toContain('#');\n  });\n\n  it('should include severity levels', () => {\n    const result = formatGeneric(sampleIssues);\n\n    expect(result).toContain('error');\n    expect(result).toContain('warning');\n  });\n\n  it('should include file locations', () => {\n    const result = formatGeneric(sampleIssues);\n\n    expect(result).toContain('src/test.ts');\n    expect(result).toContain('src/utils.ts');\n  });\n\n  it('should be human-readable', () => {\n    const result = formatGeneric(sampleIssues);\n\n    expect(result).toContain('Function too complex');\n    expect(result).toContain('Break into smaller functions');\n    expect(result).toMatch(/\\n\\n/);\n  });\n\n  it('should include line numbers', () => {\n    const result = formatGeneric(sampleIssues);\n\n    expect(result).toContain('10');\n    expect(result).toContain('5');\n  });\n});",
            "run_instructions": "npm test -- llmFormatter.test.ts -t test_formatGeneric_creates_standard_markdown"
          }
        ]
      },
      {
        "id": "provider-suite",
        "name": "AI Provider Tests",
        "description": "Tests for AI provider abstraction including OpenAI and Anthropic implementations",
        "test_file_path": "src/test/providers.test.ts",
        "source_files": [
          "src/ai/providers/openAIProvider.ts",
          "src/ai/providers/anthropicProvider.ts",
          "src/ai/providers/providerFactory.ts"
        ],
        "test_cases": [
          {
            "id": "test-openai-provider",
            "name": "test_OpenAIProvider_sends_requests",
            "description": "Verifies OpenAI provider sends requests with correct format and handles responses",
            "target_function": "sendRequest",
            "target_file": "src/ai/providers/openAIProvider.ts",
            "scenarios": [
              "successful request",
              "handles API errors",
              "validates response format",
              "includes API key"
            ],
            "mocks": [
              "HTTP client",
              "OpenAI SDK"
            ],
            "assertions": [
              "calls OpenAI API",
              "returns formatted response",
              "handles errors",
              "includes auth"
            ],
            "priority": "high",
            "test_code": "import { OpenAIProvider } from '../ai/providers/openAIProvider';\nimport { ConfigurationManager } from '../config/configurationManager';\n\njest.mock('../config/configurationManager');\njest.mock('openai');\n\ndescribe('OpenAIProvider.sendRequest', () => {\n  let provider: OpenAIProvider;\n  let mockConfig: any;\n  let mockOpenAI: any;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockConfig = {\n      getOpenAIApiKey: jest.fn().mockReturnValue('test-key'),\n      getOpenAIModel: jest.fn().mockReturnValue('gpt-4')\n    };\n    mockOpenAI = {\n      chat: {\n        completions: {\n          create: jest.fn().mockResolvedValue({\n            choices: [{ message: { content: 'test response' } }],\n            usage: { total_tokens: 100 }\n          })\n        }\n      }\n    };\n    (ConfigurationManager.getInstance as jest.Mock).mockReturnValue(mockConfig);\n    provider = new OpenAIProvider();\n    (provider as any).client = mockOpenAI;\n  });\n\n  it('should call OpenAI API with correct parameters', async () => {\n    const request = {\n      messages: [{ role: 'user', content: 'test prompt' }],\n      model: 'gpt-4'\n    };\n\n    await provider.sendRequest(request);\n\n    expect(mockOpenAI.chat.completions.create).toHaveBeenCalledWith(\n      expect.objectContaining({\n        model: 'gpt-4',\n        messages: expect.arrayContaining([expect.objectContaining({ role: 'user' })])\n      })\n    );\n  });\n\n  it('should return formatted response', async () => {\n    const request = { messages: [{ role: 'user', content: 'test' }], model: 'gpt-4' };\n\n    const result = await provider.sendRequest(request);\n\n    expect(result).toHaveProperty('content');\n    expect(result).toHaveProperty('usage');\n    expect(result.content).toBe('test response');\n  });\n\n  it('should handle API errors', async () => {\n    mockOpenAI.chat.completions.create.mockRejectedValue(new Error('API Error'));\n    const request = { messages: [{ role: 'user', content: 'test' }], model: 'gpt-4' };\n\n    await expect(provider.sendRequest(request)).rejects.toThrow('API Error');\n  });\n\n  it('should include API key in authentication', async () => {\n    const request = { messages: [{ role: 'user', content: 'test' }], model: 'gpt-4' };\n\n    await provider.sendRequest(request);\n\n    expect(mockConfig.getOpenAIApiKey).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- providers.test.ts -t test_OpenAIProvider_sends_requests"
          },
          {
            "id": "test-anthropic-provider",
            "name": "test_AnthropicProvider_sends_requests",
            "description": "Verifies Anthropic provider sends requests with correct format for Claude API",
            "target_function": "sendRequest",
            "target_file": "src/ai/providers/anthropicProvider.ts",
            "scenarios": [
              "successful request",
              "handles streaming",
              "validates Claude response",
              "includes API key"
            ],
            "mocks": [
              "HTTP client",
              "Anthropic SDK"
            ],
            "assertions": [
              "calls Claude API",
              "returns formatted response",
              "handles streaming",
              "includes auth"
            ],
            "priority": "high",
            "test_code": "import { AnthropicProvider } from '../ai/providers/anthropicProvider';\nimport { ConfigurationManager } from '../config/configurationManager';\n\njest.mock('../config/configurationManager');\njest.mock('@anthropic-ai/sdk');\n\ndescribe('AnthropicProvider.sendRequest', () => {\n  let provider: AnthropicProvider;\n  let mockConfig: any;\n  let mockAnthropic: any;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockConfig = {\n      getClaudeApiKey: jest.fn().mockReturnValue('test-key'),\n      getClaudeModel: jest.fn().mockReturnValue('claude-3-opus-20240229')\n    };\n    mockAnthropic = {\n      messages: {\n        create: jest.fn().mockResolvedValue({\n          content: [{ text: 'test response' }],\n          usage: { input_tokens: 50, output_tokens: 50 }\n        })\n      }\n    };\n    (ConfigurationManager.getInstance as jest.Mock).mockReturnValue(mockConfig);\n    provider = new AnthropicProvider();\n    (provider as any).client = mockAnthropic;\n  });\n\n  it('should call Claude API with correct parameters', async () => {\n    const request = {\n      messages: [{ role: 'user', content: 'test prompt' }],\n      model: 'claude-3-opus-20240229'\n    };\n\n    await provider.sendRequest(request);\n\n    expect(mockAnthropic.messages.create).toHaveBeenCalledWith(\n      expect.objectContaining({\n        model: 'claude-3-opus-20240229',\n        messages: expect.arrayContaining([expect.objectContaining({ role: 'user' })])\n      })\n    );\n  });\n\n  it('should return formatted response', async () => {\n    const request = { messages: [{ role: 'user', content: 'test' }], model: 'claude-3-opus-20240229' };\n\n    const result = await provider.sendRequest(request);\n\n    expect(result).toHaveProperty('content');\n    expect(result).toHaveProperty('usage');\n    expect(result.content).toBe('test response');\n  });\n\n  it('should handle API errors', async () => {\n    mockAnthropic.messages.create.mockRejectedValue(new Error('API Error'));\n    const request = { messages: [{ role: 'user', content: 'test' }], model: 'claude-3-opus-20240229' };\n\n    await expect(provider.sendRequest(request)).rejects.toThrow('API Error');\n  });\n\n  it('should include API key in authentication', async () => {\n    const request = { messages: [{ role: 'user', content: 'test' }], model: 'claude-3-opus-20240229' };\n\n    await provider.sendRequest(request);\n\n    expect(mockConfig.getClaudeApiKey).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- providers.test.ts -t test_AnthropicProvider_sends_requests"
          },
          {
            "id": "test-provider-factory",
            "name": "test_ProviderFactory_creates_correct_provider",
            "description": "Verifies provider factory creates correct provider based on configuration",
            "target_function": "createProvider",
            "target_file": "src/ai/providers/providerFactory.ts",
            "scenarios": [
              "creates OpenAI provider",
              "creates Anthropic provider",
              "handles unknown provider",
              "validates configuration"
            ],
            "mocks": [
              "configuration manager"
            ],
            "assertions": [
              "returns correct provider type",
              "throws for unknown provider",
              "validates config"
            ],
            "priority": "high",
            "test_code": "import { ProviderFactory } from '../ai/providers/providerFactory';\nimport { ConfigurationManager } from '../config/configurationManager';\nimport { OpenAIProvider } from '../ai/providers/openAIProvider';\nimport { AnthropicProvider } from '../ai/providers/anthropicProvider';\n\njest.mock('../config/configurationManager');\n\ndescribe('ProviderFactory.createProvider', () => {\n  let mockConfig: any;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockConfig = {\n      getProvider: jest.fn(),\n      getOpenAIApiKey: jest.fn().mockReturnValue('test-key'),\n      getClaudeApiKey: jest.fn().mockReturnValue('test-key')\n    };\n    (ConfigurationManager.getInstance as jest.Mock).mockReturnValue(mockConfig);\n  });\n\n  it('should create OpenAI provider when configured', () => {\n    mockConfig.getProvider.mockReturnValue('openai');\n\n    const provider = ProviderFactory.createProvider();\n\n    expect(provider).toBeInstanceOf(OpenAIProvider);\n  });\n\n  it('should create Anthropic provider when configured', () => {\n    mockConfig.getProvider.mockReturnValue('anthropic');\n\n    const provider = ProviderFactory.createProvider();\n\n    expect(provider).toBeInstanceOf(AnthropicProvider);\n  });\n\n  it('should throw error for unknown provider', () => {\n    mockConfig.getProvider.mockReturnValue('unknown');\n\n    expect(() => ProviderFactory.createProvider()).toThrow();\n  });\n\n  it('should validate configuration before creating provider', () => {\n    mockConfig.getProvider.mockReturnValue('openai');\n    mockConfig.getOpenAIApiKey.mockReturnValue(undefined);\n\n    expect(() => ProviderFactory.createProvider()).toThrow();\n  });\n});",
            "run_instructions": "npm test -- providers.test.ts -t test_ProviderFactory_creates_correct_provider"
          }
        ]
      },
      {
        "id": "enhanced-analyzer-suite",
        "name": "Enhanced Analyzer Tests",
        "description": "Tests for enhanced analyzer including advanced pattern detection and metrics calculation",
        "test_file_path": "src/test/enhancedAnalyzer.test.ts",
        "source_files": [
          "src/analysis/enhancedAnalyzer.ts"
        ],
        "test_cases": [
          {
            "id": "test-analyze-file",
            "name": "test_analyzeFile_parses_typescript",
            "description": "Verifies analyzeFile correctly parses TypeScript files and extracts metrics",
            "target_function": "analyzeFile",
            "target_file": "src/analysis/enhancedAnalyzer.ts",
            "scenarios": [
              "valid TypeScript file",
              "syntax errors",
              "empty file",
              "large file"
            ],
            "mocks": [
              "file system reads"
            ],
            "assertions": [
              "returns analysis result",
              "includes function count",
              "includes complexity metrics",
              "handles errors"
            ],
            "priority": "high",
            "test_code": "import { EnhancedAnalyzer } from '../analysis/enhancedAnalyzer';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('EnhancedAnalyzer.analyzeFile', () => {\n  let analyzer: EnhancedAnalyzer;\n  const mockFs = fs as jest.Mocked;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    analyzer = new EnhancedAnalyzer();\n  });\n\n  it('should parse valid TypeScript file', () => {\n    const validCode = `\n      function testFunction() {\n        return 42;\n      }\n      export { testFunction };\n    `;\n    mockFs.readFileSync.mockReturnValue(validCode);\n\n    const result = analyzer.analyzeFile('test.ts');\n\n    expect(result).toHaveProperty('functions');\n    expect(result.functions).toHaveLength(1);\n  });\n\n  it('should include function count in result', () => {\n    const code = `\n      function fn1() {}\n      function fn2() {}\n      const fn3 = () => {};\n    `;\n    mockFs.readFileSync.mockReturnValue(code);\n\n    const result = analyzer.analyzeFile('test.ts');\n\n    expect(result.functions.length).toBeGreaterThanOrEqual(2);\n  });\n\n  it('should include complexity metrics', () => {\n    const complexCode = `\n      function complex(x: number) {\n        if (x > 0) {\n          for (let i = 0; i  {\n    const invalidCode = 'function invalid( {';\n    mockFs.readFileSync.mockReturnValue(invalidCode);\n\n    expect(() => analyzer.analyzeFile('test.ts')).toThrow();\n  });\n\n  it('should handle empty files', () => {\n    mockFs.readFileSync.mockReturnValue('');\n\n    const result = analyzer.analyzeFile('test.ts');\n\n    expect(result.functions).toHaveLength(0);\n  });\n});",
            "run_instructions": "npm test -- enhancedAnalyzer.test.ts -t test_analyzeFile_parses_typescript"
          },
          {
            "id": "test-detect-patterns",
            "name": "test_detectPatterns_identifies_design_patterns",
            "description": "Verifies detectPatterns correctly identifies common design patterns in code",
            "target_function": "detectPatterns",
            "target_file": "src/analysis/enhancedAnalyzer.ts",
            "scenarios": [
              "singleton pattern",
              "factory pattern",
              "observer pattern",
              "no patterns present"
            ],
            "mocks": [
              "file content reader"
            ],
            "assertions": [
              "returns pattern list",
              "identifies pattern types",
              "includes pattern locations",
              "empty for no patterns"
            ],
            "priority": "medium",
            "test_code": "import { EnhancedAnalyzer } from '../analysis/enhancedAnalyzer';\n\ndescribe('EnhancedAnalyzer.detectPatterns', () => {\n  let analyzer: EnhancedAnalyzer;\n\n  beforeEach(() => {\n    analyzer = new EnhancedAnalyzer();\n  });\n\n  it('should identify singleton pattern', () => {\n    const singletonCode = `\n      class Singleton {\n        private static instance: Singleton;\n        private constructor() {}\n        static getInstance(): Singleton {\n          if (!Singleton.instance) {\n            Singleton.instance = new Singleton();\n          }\n          return Singleton.instance;\n        }\n      }\n    `;\n\n    const patterns = analyzer.detectPatterns(singletonCode);\n\n    expect(patterns).toContain('singleton');\n  });\n\n  it('should identify factory pattern', () => {\n    const factoryCode = `\n      class Factory {\n        static create(type: string) {\n          if (type === 'A') return new ClassA();\n          if (type === 'B') return new ClassB();\n        }\n      }\n    `;\n\n    const patterns = analyzer.detectPatterns(factoryCode);\n\n    expect(patterns).toContain('factory');\n  });\n\n  it('should return empty array for no patterns', () => {\n    const simpleCode = `\n      function simpleFunction() {\n        return 42;\n      }\n    `;\n\n    const patterns = analyzer.detectPatterns(simpleCode);\n\n    expect(patterns).toHaveLength(0);\n  });\n});",
            "run_instructions": "npm test -- enhancedAnalyzer.test.ts -t test_detectPatterns_identifies_design_patterns"
          }
        ]
      },
      {
        "id": "extension-suite",
        "name": "Extension Activation Tests",
        "description": "Tests for VS Code extension activation, command registration, and lifecycle management",
        "test_file_path": "src/test/extension.test.ts",
        "source_files": [
          "src/extension.ts"
        ],
        "test_cases": [
          {
            "id": "test-activate",
            "name": "test_activate_initializes_extension",
            "description": "Verifies activate function properly initializes extension and registers commands",
            "target_function": "activate",
            "target_file": "src/extension.ts",
            "scenarios": [
              "successful activation",
              "registers all commands",
              "initializes services",
              "handles activation errors"
            ],
            "mocks": [
              "vscode API",
              "service dependencies"
            ],
            "assertions": [
              "returns context",
              "registers commands",
              "initializes analyzers",
              "sets up file watchers"
            ],
            "priority": "high",
            "test_code": "import * as vscode from 'vscode';\nimport { activate } from '../extension';\n\njest.mock('vscode');\n\ndescribe('Extension.activate', () => {\n  let mockContext: vscode.ExtensionContext;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockContext = {\n      subscriptions: [],\n      extensionPath: '/test/path',\n      globalState: {\n        get: jest.fn(),\n        update: jest.fn()\n      },\n      workspaceState: {\n        get: jest.fn(),\n        update: jest.fn()\n      }\n    } as any;\n  });\n\n  it('should initialize extension successfully', async () => {\n    await activate(mockContext);\n\n    expect(mockContext.subscriptions.length).toBeGreaterThan(0);\n  });\n\n  it('should register all commands', async () => {\n    const registerCommand = jest.fn();\n    (vscode.commands.registerCommand as jest.Mock) = registerCommand;\n\n    await activate(mockContext);\n\n    expect(registerCommand).toHaveBeenCalledWith('shadowWatch.analyzeWorkspace', expect.any(Function));\n    expect(registerCommand).toHaveBeenCalledWith('shadowWatch.analyzeCurrentFile', expect.any(Function));\n    expect(registerCommand).toHaveBeenCalledWith('shadowWatch.generateProductDocs', expect.any(Function));\n  });\n\n  it('should initialize analyzers', async () => {\n    await activate(mockContext);\n\n    expect(mockContext.subscriptions).toContainEqual(expect.any(Object));\n  });\n\n  it('should handle activation errors', async () => {\n    const error = new Error('Activation failed');\n    (vscode.window.showErrorMessage as jest.Mock).mockImplementation(() => {});\n    jest.spyOn(console, 'error').mockImplementation(() => {});\n\n    await expect(activate(mockContext)).resolves.not.toThrow();\n  });\n});",
            "run_instructions": "npm test -- extension.test.ts -t test_activate_initializes_extension"
          },
          {
            "id": "test-analyze-workspace-command",
            "name": "test_analyzeWorkspace_command_executes",
            "description": "Verifies analyzeWorkspace command properly triggers workspace analysis",
            "target_function": "analyzeWorkspace",
            "target_file": "src/extension.ts",
            "scenarios": [
              "successful analysis",
              "no workspace open",
              "analysis fails",
              "shows progress"
            ],
            "mocks": [
              "vscode API",
              "analyzer service"
            ],
            "assertions": [
              "triggers analysis",
              "shows results",
              "handles no workspace",
              "displays progress"
            ],
            "priority": "high",
            "test_code": "import * as vscode from 'vscode';\nimport { analyzeWorkspace } from '../extension';\nimport { Analyzer } from '../analyzer';\n\njest.mock('vscode');\njest.mock('../analyzer');\n\ndescribe('Extension.analyzeWorkspace', () => {\n  let mockAnalyzer: jest.Mocked;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockAnalyzer = {\n      analyzeWorkspace: jest.fn().mockResolvedValue({\n        healthScore: 85,\n        issues: [],\n        files: ['test.ts']\n      })\n    } as any;\n    (vscode.workspace.workspaceFolders as any) = [{ uri: { fsPath: '/test/workspace' } }];\n    (vscode.window.withProgress as jest.Mock).mockImplementation((options, task) => task());\n  });\n\n  it('should trigger analysis successfully', async () => {\n    await analyzeWorkspace(mockAnalyzer);\n\n    expect(mockAnalyzer.analyzeWorkspace).toHaveBeenCalled();\n  });\n\n  it('should handle no workspace open', async () => {\n    (vscode.workspace.workspaceFolders as any) = undefined;\n\n    await analyzeWorkspace(mockAnalyzer);\n\n    expect(vscode.window.showErrorMessage).toHaveBeenCalledWith(\n      expect.stringContaining('No workspace')\n    );\n  });\n\n  it('should display progress', async () => {\n    await analyzeWorkspace(mockAnalyzer);\n\n    expect(vscode.window.withProgress).toHaveBeenCalled();\n  });\n\n  it('should handle analysis errors', async () => {\n    mockAnalyzer.analyzeWorkspace.mockRejectedValue(new Error('Analysis failed'));\n\n    await analyzeWorkspace(mockAnalyzer);\n\n    expect(vscode.window.showErrorMessage).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- extension.test.ts -t test_analyzeWorkspace_command_executes"
          }
        ]
      },
      {
        "id": "cache-suite",
        "name": "Cache System Tests",
        "description": "Tests for caching system including cache hits, misses, invalidation, and persistence",
        "test_file_path": "src/test/cache.test.ts",
        "source_files": [
          "src/cache.ts",
          "src/storage/incrementalStorage.ts"
        ],
        "test_cases": [
          {
            "id": "test-cache-get-set",
            "name": "test_cache_get_set_operations",
            "description": "Verifies cache correctly stores and retrieves analysis results",
            "target_function": "get, set",
            "target_file": "src/cache.ts",
            "scenarios": [
              "cache hit returns stored value",
              "cache miss returns undefined",
              "multiple keys work independently",
              "values persist"
            ],
            "mocks": [
              "none - in-memory cache"
            ],
            "assertions": [
              "get returns stored value",
              "get returns undefined for miss",
              "set stores value",
              "multiple keys independent"
            ],
            "priority": "high",
            "test_code": "import { Cache } from '../cache';\n\ndescribe('Cache.get and Cache.set', () => {\n  let cache: Cache;\n\n  beforeEach(() => {\n    cache = new Cache();\n  });\n\n  it('should return stored value on cache hit', () => {\n    const key = 'test-key';\n    const value = { data: 'test data' };\n\n    cache.set(key, value);\n    const result = cache.get(key);\n\n    expect(result).toEqual(value);\n  });\n\n  it('should return undefined on cache miss', () => {\n    const result = cache.get('nonexistent-key');\n\n    expect(result).toBeUndefined();\n  });\n\n  it('should handle multiple keys independently', () => {\n    cache.set('key1', { data: 'value1' });\n    cache.set('key2', { data: 'value2' });\n\n    expect(cache.get('key1')).toEqual({ data: 'value1' });\n    expect(cache.get('key2')).toEqual({ data: 'value2' });\n  });\n\n  it('should persist values until cleared', () => {\n    cache.set('key', { data: 'value' });\n\n    expect(cache.get('key')).toEqual({ data: 'value' });\n    expect(cache.get('key')).toEqual({ data: 'value' });\n  });\n});",
            "run_instructions": "npm test -- cache.test.ts -t test_cache_get_set_operations"
          },
          {
            "id": "test-cache-invalidation",
            "name": "test_cache_invalidation_clears_entries",
            "description": "Verifies cache invalidation correctly removes stale entries",
            "target_function": "invalidate, clear",
            "target_file": "src/cache.ts",
            "scenarios": [
              "invalidate removes specific key",
              "clear removes all entries",
              "invalidated key returns undefined",
              "other keys unaffected"
            ],
            "mocks": [
              "none - in-memory cache"
            ],
            "assertions": [
              "invalidate removes key",
              "clear empties cache",
              "get returns undefined after invalidate",
              "other keys preserved"
            ],
            "priority": "high",
            "test_code": "import { Cache } from '../cache';\n\ndescribe('Cache.invalidate and Cache.clear', () => {\n  let cache: Cache;\n\n  beforeEach(() => {\n    cache = new Cache();\n  });\n\n  it('should remove specific key on invalidate', () => {\n    cache.set('key1', { data: 'value1' });\n    cache.set('key2', { data: 'value2' });\n\n    cache.invalidate('key1');\n\n    expect(cache.get('key1')).toBeUndefined();\n    expect(cache.get('key2')).toEqual({ data: 'value2' });\n  });\n\n  it('should remove all entries on clear', () => {\n    cache.set('key1', { data: 'value1' });\n    cache.set('key2', { data: 'value2' });\n\n    cache.clear();\n\n    expect(cache.get('key1')).toBeUndefined();\n    expect(cache.get('key2')).toBeUndefined();\n  });\n\n  it('should return undefined after invalidation', () => {\n    cache.set('key', { data: 'value' });\n    cache.invalidate('key');\n\n    const result = cache.get('key');\n\n    expect(result).toBeUndefined();\n  });\n\n  it('should preserve other keys during invalidation', () => {\n    cache.set('key1', { data: 'value1' });\n    cache.set('key2', { data: 'value2' });\n    cache.set('key3', { data: 'value3' });\n\n    cache.invalidate('key2');\n\n    expect(cache.get('key1')).toEqual({ data: 'value1' });\n    expect(cache.get('key3')).toEqual({ data: 'value3' });\n  });\n});",
            "run_instructions": "npm test -- cache.test.ts -t test_cache_invalidation_clears_entries"
          },
          {
            "id": "test-incremental-storage",
            "name": "test_incrementalStorage_saves_and_loads",
            "description": "Verifies incremental storage correctly persists analysis results to disk",
            "target_function": "save, load",
            "target_file": "src/storage/incrementalStorage.ts",
            "scenarios": [
              "save writes to disk",
              "load reads from disk",
              "handles missing files",
              "validates loaded data"
            ],
            "mocks": [
              "file system operations"
            ],
            "assertions": [
              "save writes file",
              "load reads file",
              "missing file returns null",
              "validates structure"
            ],
            "priority": "high",
            "test_code": "import { IncrementalStorage } from '../storage/incrementalStorage';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('IncrementalStorage.save and load', () => {\n  let storage: IncrementalStorage;\n  const mockFs = fs as jest.Mocked;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    storage = new IncrementalStorage('/test/path');\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.mkdirSync.mockImplementation(() => {});\n  });\n\n  it('should write to disk on save', () => {\n    const data = { healthScore: 85, issues: [], timestamp: Date.now() };\n    mockFs.writeFileSync.mockImplementation(() => {});\n\n    storage.save('test-key', data);\n\n    expect(mockFs.writeFileSync).toHaveBeenCalledWith(\n      expect.stringContaining('test-key'),\n      expect.any(String)\n    );\n  });\n\n  it('should read from disk on load', () => {\n    const data = { healthScore: 85, issues: [], timestamp: Date.now() };\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.readFileSync.mockReturnValue(JSON.stringify(data));\n\n    const result = storage.load('test-key');\n\n    expect(result).toEqual(data);\n  });\n\n  it('should return null for missing files', () => {\n    mockFs.existsSync.mockReturnValue(false);\n\n    const result = storage.load('nonexistent-key');\n\n    expect(result).toBeNull();\n  });\n\n  it('should validate loaded data structure', () => {\n    const invalidData = { invalid: 'structure' };\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.readFileSync.mockReturnValue(JSON.stringify(invalidData));\n\n    expect(() => storage.load('test-key')).toThrow();\n  });\n});",
            "run_instructions": "npm test -- cache.test.ts -t test_incrementalStorage_saves_and_loads"
          }
        ]
      },
      {
        "id": "error-handling-suite",
        "name": "Error Handling Tests",
        "description": "Tests for error handling utilities and error boundary patterns across the codebase",
        "test_file_path": "src/test/errorHandler.test.ts",
        "source_files": [
          "src/utils/errorHandler.ts"
        ],
        "test_cases": [
          {
            "id": "test-handle-error",
            "name": "test_handleError_transforms_errors",
            "description": "Verifies handleError correctly transforms infrastructure errors to user-friendly messages",
            "target_function": "handleError",
            "target_file": "src/utils/errorHandler.ts",
            "scenarios": [
              "network error transformation",
              "authentication error",
              "file system error",
              "unknown error"
            ],
            "mocks": [
              "vscode window API"
            ],
            "assertions": [
              "transforms error message",
              "displays to user",
              "logs error details",
              "categorizes error type"
            ],
            "priority": "high",
            "test_code": "import { handleError } from '../utils/errorHandler';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('ErrorHandler.handleError', () => {\n  beforeEach(() => {\n    jest.clearAllMocks();\n    (vscode.window.showErrorMessage as jest.Mock).mockResolvedValue(undefined);\n  });\n\n  it('should transform network errors', () => {\n    const networkError = new Error('Network timeout');\n    networkError.name = 'NetworkError';\n\n    handleError(networkError);\n\n    expect(vscode.window.showErrorMessage).toHaveBeenCalledWith(\n      expect.stringContaining('network')\n    );\n  });\n\n  it('should transform authentication errors', () => {\n    const authError = new Error('Invalid API key');\n    authError.name = 'AuthenticationError';\n\n    handleError(authError);\n\n    expect(vscode.window.showErrorMessage).toHaveBeenCalledWith(\n      expect.stringContaining('API key')\n    );\n  });\n\n  it('should transform file system errors', () => {\n    const fsError = new Error('ENOENT: no such file');\n    fsError.name = 'FileSystemError';\n\n    handleError(fsError);\n\n    expect(vscode.window.showErrorMessage).toHaveBeenCalledWith(\n      expect.stringContaining('file')\n    );\n  });\n\n  it('should handle unknown errors', () => {\n    const unknownError = new Error('Something went wrong');\n\n    handleError(unknownError);\n\n    expect(vscode.window.showErrorMessage).toHaveBeenCalled();\n  });\n\n  it('should log error details', () => {\n    const consoleErrorSpy = jest.spyOn(console, 'error').mockImplementation(() => {});\n    const error = new Error('Test error');\n\n    handleError(error);\n\n    expect(consoleErrorSpy).toHaveBeenCalledWith(expect.stringContaining('Test error'));\n    consoleErrorSpy.mockRestore();\n  });\n});",
            "run_instructions": "npm test -- errorHandler.test.ts -t test_handleError_transforms_errors"
          }
        ]
      },
      {
        "id": "config-suite",
        "name": "Configuration Management Tests",
        "description": "Tests for configuration management including loading, validation, and updates",
        "test_file_path": "src/test/configurationManager.test.ts",
        "source_files": [
          "src/config/configurationManager.ts"
        ],
        "test_cases": [
          {
            "id": "test-get-config",
            "name": "test_getConfiguration_returns_values",
            "description": "Verifies getConfiguration correctly retrieves configuration values from VS Code settings",
            "target_function": "getConfiguration",
            "target_file": "src/config/configurationManager.ts",
            "scenarios": [
              "get existing config",
              "get missing config returns default",
              "config type conversion",
              "nested config access"
            ],
            "mocks": [
              "vscode workspace configuration"
            ],
            "assertions": [
              "returns config value",
              "returns default for missing",
              "converts types correctly",
              "accesses nested values"
            ],
            "priority": "medium",
            "test_code": "import { ConfigurationManager } from '../config/configurationManager';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('ConfigurationManager.getConfiguration', () => {\n  let configManager: ConfigurationManager;\n  let mockConfig: any;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockConfig = {\n      get: jest.fn((key: string, defaultValue?: any) => {\n        const configs: any = {\n          'provider': 'openai',\n          'openai.apiKey': 'test-key',\n          'openai.model': 'gpt-4'\n        };\n        return configs[key] || defaultValue;\n      }),\n      update: jest.fn()\n    };\n    (vscode.workspace.getConfiguration as jest.Mock).mockReturnValue(mockConfig);\n    configManager = ConfigurationManager.getInstance();\n  });\n\n  it('should return existing config value', () => {\n    const provider = configManager.getProvider();\n\n    expect(provider).toBe('openai');\n  });\n\n  it('should return default for missing config', () => {\n    mockConfig.get.mockReturnValue(undefined);\n\n    const value = configManager.getConfiguration('nonexistent', 'default');\n\n    expect(value).toBe('default');\n  });\n\n  it('should access nested config values', () => {\n    const apiKey = configManager.getOpenAIApiKey();\n\n    expect(apiKey).toBe('test-key');\n    expect(mockConfig.get).toHaveBeenCalledWith(expect.stringContaining('apiKey'));\n  });\n});",
            "run_instructions": "npm test -- configurationManager.test.ts -t test_getConfiguration_returns_values"
          },
          {
            "id": "test-update-config",
            "name": "test_updateConfiguration_persists_changes",
            "description": "Verifies updateConfiguration correctly updates and persists configuration changes",
            "target_function": "updateConfiguration",
            "target_file": "src/config/configurationManager.ts",
            "scenarios": [
              "update existing value",
              "update creates new value",
              "update validates value",
              "update notifies listeners"
            ],
            "mocks": [
              "vscode workspace configuration"
            ],
            "assertions": [
              "calls update method",
              "persists change",
              "validates before update",
              "triggers change events"
            ],
            "priority": "medium",
            "test_code": "import { ConfigurationManager } from '../config/configurationManager';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('ConfigurationManager.updateConfiguration', () => {\n  let configManager: ConfigurationManager;\n  let mockConfig: any;\n\n  beforeEach(() => {\n    jest.clearAllMocks();\n    mockConfig = {\n      get: jest.fn(),\n      update: jest.fn().mockResolvedValue(undefined)\n    };\n    (vscode.workspace.getConfiguration as jest.Mock).mockReturnValue(mockConfig);\n    configManager = ConfigurationManager.getInstance();\n  });\n\n  it('should call update method with correct parameters', async () => {\n    await configManager.updateConfiguration('provider', 'anthropic');\n\n    expect(mockConfig.update).toHaveBeenCalledWith(\n      expect.stringContaining('provider'),\n      'anthropic',\n      vscode.ConfigurationTarget.Global\n    );\n  });\n\n  it('should persist changes', async () => {\n    await configManager.updateConfiguration('openai.model', 'gpt-4-turbo');\n\n    expect(mockConfig.update).toHaveBeenCalled();\n  });\n\n  it('should validate value before update', async () => {\n    await expect(\n      configManager.updateConfiguration('provider', 'invalid-provider')\n    ).rejects.toThrow();\n  });\n});",
            "run_instructions": "npm test -- configurationManager.test.ts -t test_updateConfiguration_persists_changes"
          }
        ]
      }
    ],
    "read_write_test_suites": [],
    "user_workflow_test_suites": []
  }
}