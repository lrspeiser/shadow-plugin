{
  "rationale": "This comprehensive unit test plan focuses on regression prevention by covering all critical functions identified in the architecture analysis. Tests target high-priority areas including: (1) Core analysis engine (analyzer.ts) with god object detection, circular dependency finding, complexity calculation, and health score computation - these are fundamental to the extension's value proposition; (2) LLM integration layer (llmService.ts, llmIntegration.ts) handling provider communication, request formatting, response parsing, and error handling - the most architecturally complex components requiring thorough testing; (3) Caching and file access (cache.ts, fileAccessHelper.ts) ensuring performance optimizations work correctly; (4) Formatting and parsing utilities (llmFormatter.ts, llmResponseParser.ts, insightGenerator.ts) verifying correct data transformation; (5) Infrastructure components (rate limiting, retry logic, provider factory, configuration management) ensuring robustness and reliability. Each test suite includes executable TypeScript test code using Jest with proper mocking strategies, covering happy paths, edge cases, error conditions, and integration points. Tests are organized by source file and functionality area for maintainability. The plan provides exact run instructions for both individual tests and entire suites, enabling developers to quickly verify changes haven't broken existing functionality. Priority is given to functions mentioned in architecture insights as critical issues or high-priority areas, ensuring tests catch regressions in the most important code paths.",
  "aggregated_plan": {
    "unit_test_plan": {
      "strategy": "Adopt a layered testing strategy for this TypeScript VS Code extension: (1) Unit test core business logic in isolated modules (analyzer, insightGenerator, cache) with mocked dependencies, (2) Integration test LLM service layer with mock providers to verify request/response handling, (3) Test view components with VS Code API mocks to verify UI logic, (4) Focus on regression prevention by covering critical paths identified in architecture insights (god object detection, circular dependency analysis, LLM integration, caching). Use dependency injection where possible to facilitate mocking. Prioritize testing functions mentioned in architecture issues (llmService, llmIntegration, analyzer modules) and entry point flows.",
      "testing_framework": "jest",
      "mocking_approach": "Use jest.mock() for module mocking, jest.fn() for function mocks, and jest.spyOn() for partial mocks. Mock VS Code API with manual mocks in __mocks__/vscode.ts. Mock file system operations using memfs or mock-fs. Mock LLM providers by implementing ILLMProvider interface with test doubles. Use dependency injection to inject mocks into constructors and functions where possible. For complex objects, use factory functions that accept mock dependencies.",
      "isolation_strategy": "Test pure functions (formatters, parsers, utilities) in complete isolation. Test business logic classes (Analyzer, InsightGenerator, Cache) with mocked I/O dependencies (file system, VS Code APIs). Test services (LLMService, LLMIntegration) with mocked providers and repositories. View components require VS Code API mocks but can test logic independently. Entry point (extension.ts) requires integration-style tests with multiple mocks."
    },
    "test_suites": [
      {
        "id": "analyzer-core",
        "name": "Analyzer Core Functionality",
        "description": "Tests core static analysis engine including god object detection, circular dependency detection, dead code identification, and complexity calculation",
        "test_file_path": "src/test/analyzer.test.ts",
        "source_files": [
          "src/analyzer.ts"
        ],
        "test_cases": [
          {
            "id": "analyzer-detect-god-objects",
            "name": "test_detectGodObjects_identifiesLargeFiles",
            "description": "Verifies detectGodObjects correctly identifies files exceeding line count threshold as god objects",
            "target_function": "detectGodObjects",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "File with 1000 lines should be flagged as god object",
              "File with 300 lines should not be flagged",
              "Empty file should not be flagged"
            ],
            "mocks": [
              "File system read operations"
            ],
            "assertions": [
              "Returns array of god objects",
              "God object contains file path and line count",
              "Files under threshold are not included"
            ],
            "priority": "high",
            "test_code": "import { Analyzer } from '../analyzer';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\nconst mockFs = fs as jest.Mocked;\n\ndescribe('Analyzer.detectGodObjects', () => {\n  let analyzer: Analyzer;\n\n  beforeEach(() => {\n    analyzer = new Analyzer();\n    jest.clearAllMocks();\n  });\n\n  test('identifies files exceeding line count threshold', () => {\n    const mockFileAnalysis = [\n      { filePath: '/src/large.ts', lineCount: 1000, functions: [], imports: [], exports: [] },\n      { filePath: '/src/small.ts', lineCount: 300, functions: [], imports: [], exports: [] },\n      { filePath: '/src/empty.ts', lineCount: 0, functions: [], imports: [], exports: [] }\n    ];\n\n    mockFs.existsSync.mockReturnValue(true);\n    mockFs.readFileSync.mockReturnValue('mock content');\n\n    const result = analyzer.detectGodObjects(mockFileAnalysis);\n\n    expect(result).toHaveLength(1);\n    expect(result[0].filePath).toBe('/src/large.ts');\n    expect(result[0].lineCount).toBe(1000);\n    expect(result[0].severity).toBe('error');\n  });\n\n  test('returns empty array when no files exceed threshold', () => {\n    const mockFileAnalysis = [\n      { filePath: '/src/small1.ts', lineCount: 100, functions: [], imports: [], exports: [] },\n      { filePath: '/src/small2.ts', lineCount: 200, functions: [], imports: [], exports: [] }\n    ];\n\n    const result = analyzer.detectGodObjects(mockFileAnalysis);\n\n    expect(result).toHaveLength(0);\n  });\n\n  test('handles empty file analysis array', () => {\n    const result = analyzer.detectGodObjects([]);\n\n    expect(result).toHaveLength(0);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'identifies files exceeding line count threshold'"
          },
          {
            "id": "analyzer-circular-deps",
            "name": "test_findCircularDependencies_detectsCycles",
            "description": "Verifies findCircularDependencies correctly identifies circular import chains",
            "target_function": "findCircularDependencies",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "Simple A->B->A cycle detected",
              "Complex A->B->C->A cycle detected",
              "No cycles returns empty array"
            ],
            "mocks": [
              "File system operations"
            ],
            "assertions": [
              "Returns array of circular dependency chains",
              "Each chain contains complete cycle path",
              "No false positives for valid dependencies"
            ],
            "priority": "high",
            "test_code": "import { Analyzer } from '../analyzer';\n\ndescribe('Analyzer.findCircularDependencies', () => {\n  let analyzer: Analyzer;\n\n  beforeEach(() => {\n    analyzer = new Analyzer();\n  });\n\n  test('detects simple A->B->A circular dependency', () => {\n    const mockFileAnalysis = [\n      { filePath: '/src/a.ts', imports: ['/src/b.ts'], exports: [], functions: [], lineCount: 10 },\n      { filePath: '/src/b.ts', imports: ['/src/a.ts'], exports: [], functions: [], lineCount: 10 }\n    ];\n\n    const result = analyzer.findCircularDependencies(mockFileAnalysis);\n\n    expect(result.length).toBeGreaterThan(0);\n    expect(result[0].cycle).toContain('/src/a.ts');\n    expect(result[0].cycle).toContain('/src/b.ts');\n    expect(result[0].severity).toBe('error');\n  });\n\n  test('detects complex A->B->C->A circular dependency', () => {\n    const mockFileAnalysis = [\n      { filePath: '/src/a.ts', imports: ['/src/b.ts'], exports: [], functions: [], lineCount: 10 },\n      { filePath: '/src/b.ts', imports: ['/src/c.ts'], exports: [], functions: [], lineCount: 10 },\n      { filePath: '/src/c.ts', imports: ['/src/a.ts'], exports: [], functions: [], lineCount: 10 }\n    ];\n\n    const result = analyzer.findCircularDependencies(mockFileAnalysis);\n\n    expect(result.length).toBeGreaterThan(0);\n    const cycle = result[0].cycle;\n    expect(cycle).toContain('/src/a.ts');\n    expect(cycle).toContain('/src/b.ts');\n    expect(cycle).toContain('/src/c.ts');\n  });\n\n  test('returns empty array when no circular dependencies exist', () => {\n    const mockFileAnalysis = [\n      { filePath: '/src/a.ts', imports: ['/src/b.ts'], exports: [], functions: [], lineCount: 10 },\n      { filePath: '/src/b.ts', imports: [], exports: [], functions: [], lineCount: 10 }\n    ];\n\n    const result = analyzer.findCircularDependencies(mockFileAnalysis);\n\n    expect(result).toHaveLength(0);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'detects simple A->B->A circular dependency'"
          },
          {
            "id": "analyzer-complexity",
            "name": "test_calculateComplexity_computesCyclomaticComplexity",
            "description": "Verifies calculateComplexity correctly computes cyclomatic complexity for functions",
            "target_function": "calculateComplexity",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "Simple function returns complexity 1",
              "Function with if statement returns complexity 2",
              "Function with multiple branches returns correct complexity"
            ],
            "mocks": [],
            "assertions": [
              "Returns numeric complexity score",
              "Higher complexity for more branching",
              "Correct calculation for nested conditions"
            ],
            "priority": "high",
            "test_code": "import { Analyzer } from '../analyzer';\n\ndescribe('Analyzer.calculateComplexity', () => {\n  let analyzer: Analyzer;\n\n  beforeEach(() => {\n    analyzer = new Analyzer();\n  });\n\n  test('simple function returns complexity 1', () => {\n    const functionNode = {\n      name: 'simpleFunc',\n      startLine: 1,\n      endLine: 3,\n      body: 'return true;'\n    };\n\n    const complexity = analyzer.calculateComplexity(functionNode);\n\n    expect(complexity).toBe(1);\n  });\n\n  test('function with if statement increases complexity', () => {\n    const functionNode = {\n      name: 'conditionalFunc',\n      startLine: 1,\n      endLine: 5,\n      body: 'if (x > 0) { return x; } return 0;'\n    };\n\n    const complexity = analyzer.calculateComplexity(functionNode);\n\n    expect(complexity).toBeGreaterThanOrEqual(2);\n  });\n\n  test('function with multiple branches returns higher complexity', () => {\n    const functionNode = {\n      name: 'complexFunc',\n      startLine: 1,\n      endLine: 10,\n      body: 'if (a) {} else if (b) {} for (let i = 0; i  {\n    const functionNode = {\n      name: 'emptyFunc',\n      startLine: 1,\n      endLine: 1,\n      body: ''\n    };\n\n    const complexity = analyzer.calculateComplexity(functionNode);\n\n    expect(complexity).toBe(1);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'simple function returns complexity 1'"
          },
          {
            "id": "analyzer-health-score",
            "name": "test_calculateHealthScore_computesAccurateScore",
            "description": "Verifies calculateHealthScore computes accurate health percentage based on issues",
            "target_function": "calculateHealthScore",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "No issues returns 100% health",
              "Critical errors significantly reduce score",
              "Mix of errors and warnings calculated correctly"
            ],
            "mocks": [],
            "assertions": [
              "Returns score between 0 and 100",
              "More severe issues reduce score more",
              "Score formula matches documented algorithm"
            ],
            "priority": "high",
            "test_code": "import { Analyzer } from '../analyzer';\n\ndescribe('Analyzer.calculateHealthScore', () => {\n  let analyzer: Analyzer;\n\n  beforeEach(() => {\n    analyzer = new Analyzer();\n  });\n\n  test('no issues returns 100% health score', () => {\n    const issues = [];\n\n    const score = analyzer.calculateHealthScore(issues);\n\n    expect(score).toBe(100);\n  });\n\n  test('critical errors significantly reduce score', () => {\n    const issues = [\n      { severity: 'error', category: 'god-object', description: 'Large file', file: 'a.ts' },\n      { severity: 'error', category: 'circular-dependency', description: 'Cycle', file: 'b.ts' },\n      { severity: 'error', category: 'dead-code', description: 'Unused', file: 'c.ts' }\n    ];\n\n    const score = analyzer.calculateHealthScore(issues);\n\n    expect(score).toBeLessThan(70);\n    expect(score).toBeGreaterThanOrEqual(0);\n  });\n\n  test('warnings reduce score less than errors', () => {\n    const errorIssues = [\n      { severity: 'error', category: 'complexity', description: 'Complex', file: 'a.ts' }\n    ];\n    const warningIssues = [\n      { severity: 'warning', category: 'complexity', description: 'Complex', file: 'a.ts' }\n    ];\n\n    const errorScore = analyzer.calculateHealthScore(errorIssues);\n    const warningScore = analyzer.calculateHealthScore(warningIssues);\n\n    expect(warningScore).toBeGreaterThan(errorScore);\n  });\n\n  test('mix of errors and warnings calculated correctly', () => {\n    const issues = [\n      { severity: 'error', category: 'god-object', description: 'Large', file: 'a.ts' },\n      { severity: 'warning', category: 'complexity', description: 'Complex', file: 'b.ts' },\n      { severity: 'info', category: 'style', description: 'Format', file: 'c.ts' }\n    ];\n\n    const score = analyzer.calculateHealthScore(issues);\n\n    expect(score).toBeGreaterThan(0);\n    expect(score).toBeLessThan(100);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'no issues returns 100% health score'"
          }
        ]
      },
      {
        "id": "llm-service-core",
        "name": "LLM Service Core Operations",
        "description": "Tests LLM service including provider communication, request formatting, response parsing, and error handling",
        "test_file_path": "src/test/llmService.test.ts",
        "source_files": [
          "src/llmService.ts"
        ],
        "test_cases": [
          {
            "id": "llm-service-send-request",
            "name": "test_sendRequest_formatsAndSendsCorrectly",
            "description": "Verifies sendRequest properly formats prompts and sends to configured provider",
            "target_function": "sendRequest",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "Request sent to OpenAI provider with correct format",
              "Request sent to Anthropic provider with correct format",
              "Request includes token budget and schema"
            ],
            "mocks": [
              "ILLMProvider implementation",
              "ConfigurationManager"
            ],
            "assertions": [
              "Provider called with formatted prompt",
              "Request includes system message and user message",
              "Response parsed correctly"
            ],
            "priority": "high",
            "test_code": "import { LLMService } from '../llmService';\nimport { ILLMProvider } from '../ai/providers/ILLMProvider';\nimport { ConfigurationManager } from '../config/configurationManager';\n\njest.mock('../config/configurationManager');\n\nconst mockProvider: ILLMProvider = {\n  getName: jest.fn().mockReturnValue('openai'),\n  sendRequest: jest.fn().mockResolvedValue({ content: '{\"result\": \"success\"}', usage: { totalTokens: 100 } }),\n  validateConfiguration: jest.fn().mockReturnValue(true)\n};\n\nconst mockConfig = {\n  getCurrentProvider: jest.fn().mockReturnValue('openai'),\n  getOpenAIKey: jest.fn().mockReturnValue('test-key'),\n  getMaxTokens: jest.fn().mockReturnValue(4000)\n} as any;\n\ndescribe('LLMService.sendRequest', () => {\n  let llmService: LLMService;\n\n  beforeEach(() => {\n    (ConfigurationManager.getInstance as jest.Mock).mockReturnValue(mockConfig);\n    llmService = new LLMService(mockProvider);\n  });\n\n  test('formats and sends request to provider correctly', async () => {\n    const prompt = 'Analyze this code';\n    const schema = { type: 'object', properties: { result: { type: 'string' } } };\n\n    const result = await llmService.sendRequest(prompt, schema, 2000);\n\n    expect(mockProvider.sendRequest).toHaveBeenCalled();\n    const callArgs = (mockProvider.sendRequest as jest.Mock).mock.calls[0][0];\n    expect(callArgs.messages).toBeDefined();\n    expect(callArgs.messages.length).toBeGreaterThan(0);\n    expect(result).toBeDefined();\n  });\n\n  test('includes token budget in request', async () => {\n    const prompt = 'Test prompt';\n    const tokenBudget = 3000;\n\n    await llmService.sendRequest(prompt, {}, tokenBudget);\n\n    expect(mockProvider.sendRequest).toHaveBeenCalled();\n    const callArgs = (mockProvider.sendRequest as jest.Mock).mock.calls[0][0];\n    expect(callArgs.maxTokens).toBeLessThanOrEqual(tokenBudget);\n  });\n\n  test('handles provider errors gracefully', async () => {\n    (mockProvider.sendRequest as jest.Mock).mockRejectedValue(new Error('API Error'));\n\n    await expect(llmService.sendRequest('prompt', {}, 1000)).rejects.toThrow();\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t 'formats and sends request to provider correctly'"
          },
          {
            "id": "llm-service-response-parse",
            "name": "test_parseResponse_handlesValidAndInvalidJSON",
            "description": "Verifies response parsing handles valid JSON, malformed JSON, and non-JSON responses",
            "target_function": "parseResponse",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "Valid JSON response parsed correctly",
              "Malformed JSON triggers retry",
              "Non-JSON text handled gracefully"
            ],
            "mocks": [],
            "assertions": [
              "Valid JSON returns parsed object",
              "Invalid JSON throws appropriate error",
              "Error includes helpful context"
            ],
            "priority": "high",
            "test_code": "import { LLMService } from '../llmService';\n\ndescribe('LLMService.parseResponse', () => {\n  let llmService: LLMService;\n\n  beforeEach(() => {\n    const mockProvider = {\n      getName: jest.fn().mockReturnValue('test'),\n      sendRequest: jest.fn(),\n      validateConfiguration: jest.fn().mockReturnValue(true)\n    };\n    llmService = new LLMService(mockProvider as any);\n  });\n\n  test('parses valid JSON response correctly', () => {\n    const validJSON = '{\"analysis\": {\"result\": \"success\"}, \"confidence\": 0.95}';\n\n    const result = llmService.parseResponse(validJSON);\n\n    expect(result).toEqual({ analysis: { result: 'success' }, confidence: 0.95 });\n  });\n\n  test('extracts JSON from markdown code blocks', () => {\n    const markdownJSON = '\\n{\"data\": \"value\"}\\n';\n\n    const result = llmService.parseResponse(markdownJSON);\n\n    expect(result).toEqual({ data: 'value' });\n  });\n\n  test('throws error for malformed JSON', () => {\n    const malformedJSON = '{invalid json';\n\n    expect(() => llmService.parseResponse(malformedJSON)).toThrow();\n  });\n\n  test('handles non-JSON text response', () => {\n    const textResponse = 'This is plain text without JSON';\n\n    expect(() => llmService.parseResponse(textResponse)).toThrow();\n  });\n\n  test('handles empty response', () => {\n    const emptyResponse = '';\n\n    expect(() => llmService.parseResponse(emptyResponse)).toThrow();\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t 'parses valid JSON response correctly'"
          }
        ]
      },
      {
        "id": "llm-integration",
        "name": "LLM Integration Workflows",
        "description": "Tests high-level LLM integration workflows including product docs generation, insights generation, and unit test generation",
        "test_file_path": "src/test/llmIntegration.test.ts",
        "source_files": [
          "src/llmIntegration.ts"
        ],
        "test_cases": [
          {
            "id": "llm-integration-product-docs",
            "name": "test_generateProductDocs_createsComprehensiveDocs",
            "description": "Verifies generateProductDocs orchestrates LLM calls to create complete product documentation",
            "target_function": "generateProductDocs",
            "target_file": "src/llmIntegration.ts",
            "scenarios": [
              "Successful documentation generation",
              "Handles LLM service errors",
              "Saves documentation to storage"
            ],
            "mocks": [
              "LLMService",
              "vscode.window",
              "AnalysisResultRepository"
            ],
            "assertions": [
              "LLM service called with correct prompt",
              "Documentation includes product overview",
              "Documentation saved to file system",
              "User notified of success"
            ],
            "priority": "high",
            "test_code": "import { generateProductDocs } from '../llmIntegration';\nimport { LLMService } from '../llmService';\nimport * as vscode from 'vscode';\n\njest.mock('../llmService');\njest.mock('vscode');\n\nconst mockLLMService = {\n  sendRequest: jest.fn().mockResolvedValue({\n    product_name: 'Test App',\n    product_overview: 'A test application',\n    key_features: ['Feature 1', 'Feature 2']\n  })\n};\n\nconst mockWindow = {\n  showInformationMessage: jest.fn(),\n  showErrorMessage: jest.fn(),\n  withProgress: jest.fn((options, task) => task({ report: jest.fn() }))\n};\n\n(vscode.window as any) = mockWindow;\n\ndescribe('generateProductDocs', () => {\n  beforeEach(() => {\n    jest.clearAllMocks();\n  });\n\n  test('creates comprehensive product documentation', async () => {\n    const mockAnalysis = {\n      files: [{ filePath: 'src/main.ts', lineCount: 100, functions: [] }],\n      statistics: { totalFiles: 1, totalLines: 100 }\n    };\n\n    await generateProductDocs(mockAnalysis as any, mockLLMService as any);\n\n    expect(mockLLMService.sendRequest).toHaveBeenCalled();\n    const promptArg = (mockLLMService.sendRequest as jest.Mock).mock.calls[0][0];\n    expect(promptArg).toContain('product');\n  });\n\n  test('handles LLM service errors gracefully', async () => {\n    mockLLMService.sendRequest.mockRejectedValue(new Error('LLM API Error'));\n\n    const mockAnalysis = {\n      files: [],\n      statistics: { totalFiles: 0, totalLines: 0 }\n    };\n\n    await generateProductDocs(mockAnalysis as any, mockLLMService as any);\n\n    expect(mockWindow.showErrorMessage).toHaveBeenCalled();\n  });\n\n  test('shows progress indicator during generation', async () => {\n    const mockAnalysis = {\n      files: [],\n      statistics: { totalFiles: 0, totalLines: 0 }\n    };\n\n    await generateProductDocs(mockAnalysis as any, mockLLMService as any);\n\n    expect(mockWindow.withProgress).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- llmIntegration.test.ts -t 'creates comprehensive product documentation'"
          },
          {
            "id": "llm-integration-insights",
            "name": "test_generateLLMInsights_producesArchitecturalInsights",
            "description": "Verifies generateLLMInsights creates architectural analysis and design pattern insights",
            "target_function": "generateLLMInsights",
            "target_file": "src/llmIntegration.ts",
            "scenarios": [
              "Insights generated successfully",
              "Includes design patterns",
              "Identifies architectural issues"
            ],
            "mocks": [
              "LLMService",
              "vscode.window"
            ],
            "assertions": [
              "LLM called with analysis context",
              "Response includes architecture assessment",
              "Design patterns identified",
              "Insights stored correctly"
            ],
            "priority": "high",
            "test_code": "import { generateLLMInsights } from '../llmIntegration';\nimport { LLMService } from '../llmService';\nimport * as vscode from 'vscode';\n\njest.mock('../llmService');\njest.mock('vscode');\n\nconst mockLLMService = {\n  sendRequest: jest.fn().mockResolvedValue({\n    overall_assessment: 'Architecture is sound',\n    strengths: ['Good separation', 'Clear modules'],\n    critical_issues: [],\n    design_patterns: ['Factory', 'Observer']\n  })\n};\n\nconst mockWindow = {\n  showInformationMessage: jest.fn(),\n  showErrorMessage: jest.fn(),\n  withProgress: jest.fn((options, task) => task({ report: jest.fn() }))\n};\n\n(vscode.window as any) = mockWindow;\n\ndescribe('generateLLMInsights', () => {\n  beforeEach(() => {\n    jest.clearAllMocks();\n  });\n\n  test('produces comprehensive architectural insights', async () => {\n    const mockAnalysis = {\n      files: [{ filePath: 'src/service.ts', lineCount: 200, functions: [] }],\n      dependencies: [],\n      statistics: { totalFiles: 1, totalLines: 200 }\n    };\n\n    await generateLLMInsights(mockAnalysis as any, mockLLMService as any);\n\n    expect(mockLLMService.sendRequest).toHaveBeenCalled();\n    const promptArg = (mockLLMService.sendRequest as jest.Mock).mock.calls[0][0];\n    expect(promptArg).toContain('architecture');\n  });\n\n  test('identifies design patterns in codebase', async () => {\n    const mockAnalysis = {\n      files: [],\n      statistics: { totalFiles: 0, totalLines: 0 }\n    };\n\n    const result = await generateLLMInsights(mockAnalysis as any, mockLLMService as any);\n\n    expect(mockLLMService.sendRequest).toHaveBeenCalled();\n  });\n\n  test('handles empty codebase analysis', async () => {\n    const emptyAnalysis = {\n      files: [],\n      statistics: { totalFiles: 0, totalLines: 0 }\n    };\n\n    await generateLLMInsights(emptyAnalysis as any, mockLLMService as any);\n\n    expect(mockLLMService.sendRequest).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- llmIntegration.test.ts -t 'produces comprehensive architectural insights'"
          }
        ]
      },
      {
        "id": "cache-operations",
        "name": "Cache Management",
        "description": "Tests cache functionality including storing, retrieving, and invalidating cached analysis results",
        "test_file_path": "src/test/cache.test.ts",
        "source_files": [
          "src/cache.ts"
        ],
        "test_cases": [
          {
            "id": "cache-store-retrieve",
            "name": "test_cache_storesAndRetrievesCorrectly",
            "description": "Verifies cache stores analysis results and retrieves them correctly",
            "target_function": "set, get",
            "target_file": "src/cache.ts",
            "scenarios": [
              "Store and retrieve analysis result",
              "Cache hit returns stored data",
              "Cache miss returns null"
            ],
            "mocks": [],
            "assertions": [
              "Stored data retrieved exactly",
              "Cache key matching works correctly",
              "Expiration handled properly"
            ],
            "priority": "high",
            "test_code": "import { Cache } from '../cache';\n\ndescribe('Cache operations', () => {\n  let cache: Cache;\n\n  beforeEach(() => {\n    cache = new Cache();\n  });\n\n  test('stores and retrieves analysis results correctly', () => {\n    const key = 'src/test.ts';\n    const analysisResult = {\n      filePath: 'src/test.ts',\n      issues: [{ severity: 'warning', description: 'Test issue' }],\n      healthScore: 85\n    };\n\n    cache.set(key, analysisResult);\n    const retrieved = cache.get(key);\n\n    expect(retrieved).toEqual(analysisResult);\n  });\n\n  test('cache miss returns null', () => {\n    const result = cache.get('nonexistent-key');\n\n    expect(result).toBeNull();\n  });\n\n  test('cache hit returns exact stored data', () => {\n    const key = 'src/module.ts';\n    const data = { test: 'data', nested: { value: 42 } };\n\n    cache.set(key, data);\n    const retrieved = cache.get(key);\n\n    expect(retrieved).toEqual(data);\n    expect(retrieved.nested.value).toBe(42);\n  });\n\n  test('clear removes all cached entries', () => {\n    cache.set('key1', { data: 1 });\n    cache.set('key2', { data: 2 });\n\n    cache.clear();\n\n    expect(cache.get('key1')).toBeNull();\n    expect(cache.get('key2')).toBeNull();\n  });\n\n  test('handles cache invalidation by key', () => {\n    cache.set('key1', { data: 1 });\n    cache.set('key2', { data: 2 });\n\n    cache.invalidate('key1');\n\n    expect(cache.get('key1')).toBeNull();\n    expect(cache.get('key2')).toEqual({ data: 2 });\n  });\n});",
            "run_instructions": "npm test -- cache.test.ts -t 'stores and retrieves analysis results correctly'"
          }
        ]
      },
      {
        "id": "insight-generator",
        "name": "Insight Generation",
        "description": "Tests insight generation including architecture insights and issue formatting",
        "test_file_path": "src/test/insightGenerator.test.ts",
        "source_files": [
          "src/insightGenerator.ts"
        ],
        "test_cases": [
          {
            "id": "insight-generator-format",
            "name": "test_generateInsights_formatsCorrectly",
            "description": "Verifies insight generator formats analysis results into human-readable insights",
            "target_function": "generateInsights",
            "target_file": "src/insightGenerator.ts",
            "scenarios": [
              "Formats god object insights",
              "Formats circular dependency insights",
              "Formats complexity insights"
            ],
            "mocks": [],
            "assertions": [
              "Insights have correct structure",
              "Severity levels assigned correctly",
              "Descriptions are clear and actionable"
            ],
            "priority": "medium",
            "test_code": "import { InsightGenerator } from '../insightGenerator';\n\ndescribe('InsightGenerator.generateInsights', () => {\n  let generator: InsightGenerator;\n\n  beforeEach(() => {\n    generator = new InsightGenerator();\n  });\n\n  test('formats god object insights correctly', () => {\n    const issues = [\n      {\n        type: 'god-object',\n        filePath: 'src/large.ts',\n        lineCount: 1500,\n        severity: 'error'\n      }\n    ];\n\n    const insights = generator.generateInsights(issues);\n\n    expect(insights).toHaveLength(1);\n    expect(insights[0].description).toContain('large');\n    expect(insights[0].severity).toBe('error');\n    expect(insights[0].file).toBe('src/large.ts');\n  });\n\n  test('formats circular dependency insights', () => {\n    const issues = [\n      {\n        type: 'circular-dependency',\n        cycle: ['src/a.ts', 'src/b.ts', 'src/a.ts'],\n        severity: 'error'\n      }\n    ];\n\n    const insights = generator.generateInsights(issues);\n\n    expect(insights).toHaveLength(1);\n    expect(insights[0].description).toContain('circular');\n    expect(insights[0].description).toContain('src/a.ts');\n  });\n\n  test('formats complexity insights with actionable suggestions', () => {\n    const issues = [\n      {\n        type: 'complexity',\n        filePath: 'src/complex.ts',\n        functionName: 'processData',\n        complexity: 15,\n        severity: 'warning'\n      }\n    ];\n\n    const insights = generator.generateInsights(issues);\n\n    expect(insights).toHaveLength(1);\n    expect(insights[0].description).toContain('complex');\n    expect(insights[0].severity).toBe('warning');\n  });\n\n  test('handles empty issues array', () => {\n    const insights = generator.generateInsights([]);\n\n    expect(insights).toHaveLength(0);\n  });\n});",
            "run_instructions": "npm test -- insightGenerator.test.ts -t 'formats god object insights correctly'"
          }
        ]
      },
      {
        "id": "llm-formatter",
        "name": "LLM Prompt Formatting",
        "description": "Tests LLM formatter for generating prompts optimized for different AI assistants",
        "test_file_path": "src/test/llmFormatter.test.ts",
        "source_files": [
          "src/llmFormatter.ts"
        ],
        "test_cases": [
          {
            "id": "llm-formatter-cursor",
            "name": "test_formatForCursor_generatesOptimizedPrompt",
            "description": "Verifies formatForCursor generates prompts optimized for Cursor AI assistant",
            "target_function": "formatForCursor",
            "target_file": "src/llmFormatter.ts",
            "scenarios": [
              "Cursor prompt includes context",
              "Cursor prompt includes code samples",
              "Cursor prompt formatted correctly"
            ],
            "mocks": [],
            "assertions": [
              "Output is valid markdown",
              "Includes file paths",
              "Includes issue descriptions",
              "Formatted for Cursor consumption"
            ],
            "priority": "medium",
            "test_code": "import { LLMFormatter } from '../llmFormatter';\n\ndescribe('LLMFormatter.formatForCursor', () => {\n  let formatter: LLMFormatter;\n\n  beforeEach(() => {\n    formatter = new LLMFormatter();\n  });\n\n  test('generates Cursor-optimized prompt with context', () => {\n    const insights = [\n      {\n        severity: 'error',\n        category: 'god-object',\n        description: 'File too large',\n        file: 'src/service.ts',\n        line: 1\n      }\n    ];\n\n    const prompt = formatter.formatForCursor(insights);\n\n    expect(prompt).toContain('src/service.ts');\n    expect(prompt).toContain('god-object');\n    expect(prompt).toContain('File too large');\n  });\n\n  test('includes code context in Cursor prompt', () => {\n    const insights = [\n      {\n        severity: 'warning',\n        category: 'complexity',\n        description: 'High complexity',\n        file: 'src/complex.ts',\n        line: 50,\n        codeSnippet: 'function complex() { ... }'\n      }\n    ];\n\n    const prompt = formatter.formatForCursor(insights);\n\n    expect(prompt).toContain('complex.ts');\n    expect(prompt).toContain('complexity');\n  });\n\n  test('formats multiple issues correctly', () => {\n    const insights = [\n      { severity: 'error', category: 'god-object', description: 'Issue 1', file: 'a.ts' },\n      { severity: 'warning', category: 'complexity', description: 'Issue 2', file: 'b.ts' }\n    ];\n\n    const prompt = formatter.formatForCursor(insights);\n\n    expect(prompt).toContain('a.ts');\n    expect(prompt).toContain('b.ts');\n  });\n\n  test('handles empty insights array', () => {\n    const prompt = formatter.formatForCursor([]);\n\n    expect(prompt).toBeDefined();\n    expect(prompt.length).toBeGreaterThan(0);\n  });\n});",
            "run_instructions": "npm test -- llmFormatter.test.ts -t 'generates Cursor-optimized prompt with context'"
          },
          {
            "id": "llm-formatter-chatgpt",
            "name": "test_formatForChatGPT_generatesVerbosePrompt",
            "description": "Verifies formatForChatGPT generates detailed prompts for ChatGPT",
            "target_function": "formatForChatGPT",
            "target_file": "src/llmFormatter.ts",
            "scenarios": [
              "ChatGPT prompt is verbose",
              "Includes full context",
              "Includes refactoring suggestions"
            ],
            "mocks": [],
            "assertions": [
              "Output is detailed",
              "Includes explanations",
              "Formatted for ChatGPT"
            ],
            "priority": "medium",
            "test_code": "import { LLMFormatter } from '../llmFormatter';\n\ndescribe('LLMFormatter.formatForChatGPT', () => {\n  let formatter: LLMFormatter;\n\n  beforeEach(() => {\n    formatter = new LLMFormatter();\n  });\n\n  test('generates verbose ChatGPT prompt', () => {\n    const insights = [\n      {\n        severity: 'error',\n        category: 'circular-dependency',\n        description: 'Circular import detected',\n        file: 'src/a.ts'\n      }\n    ];\n\n    const prompt = formatter.formatForChatGPT(insights);\n\n    expect(prompt).toContain('circular-dependency');\n    expect(prompt).toContain('src/a.ts');\n    expect(prompt.length).toBeGreaterThan(100);\n  });\n\n  test('includes detailed explanations', () => {\n    const insights = [\n      {\n        severity: 'warning',\n        category: 'dead-code',\n        description: 'Unused function',\n        file: 'src/util.ts'\n      }\n    ];\n\n    const prompt = formatter.formatForChatGPT(insights);\n\n    expect(prompt).toContain('dead-code');\n    expect(prompt).toContain('util.ts');\n  });\n\n  test('handles multiple severity levels', () => {\n    const insights = [\n      { severity: 'error', category: 'god-object', description: 'Large file', file: 'a.ts' },\n      { severity: 'warning', category: 'complexity', description: 'Complex', file: 'b.ts' },\n      { severity: 'info', category: 'style', description: 'Format', file: 'c.ts' }\n    ];\n\n    const prompt = formatter.formatForChatGPT(insights);\n\n    expect(prompt).toContain('error');\n    expect(prompt).toContain('warning');\n    expect(prompt).toContain('info');\n  });\n});",
            "run_instructions": "npm test -- llmFormatter.test.ts -t 'generates verbose ChatGPT prompt'"
          }
        ]
      },
      {
        "id": "file-access-helper",
        "name": "File System Operations",
        "description": "Tests file access helper for reading files and managing file system operations",
        "test_file_path": "src/test/fileAccessHelper.test.ts",
        "source_files": [
          "src/fileAccessHelper.ts"
        ],
        "test_cases": [
          {
            "id": "file-access-search",
            "name": "test_searchDirectory_findsMatchingFiles",
            "description": "Verifies searchDirectory finds files matching criteria",
            "target_function": "searchDirectory",
            "target_file": "src/fileAccessHelper.ts",
            "scenarios": [
              "Finds TypeScript files",
              "Filters by extension",
              "Respects ignore patterns"
            ],
            "mocks": [
              "File system operations"
            ],
            "assertions": [
              "Returns matching files",
              "Excludes ignored directories",
              "Handles nested directories"
            ],
            "priority": "medium",
            "test_code": "import { FileAccessHelper } from '../fileAccessHelper';\nimport * as fs from 'fs';\nimport * as path from 'path';\n\njest.mock('fs');\n\nconst mockFs = fs as jest.Mocked;\n\ndescribe('FileAccessHelper.searchDirectory', () => {\n  let helper: FileAccessHelper;\n\n  beforeEach(() => {\n    helper = new FileAccessHelper();\n    jest.clearAllMocks();\n  });\n\n  test('finds TypeScript files in directory', () => {\n    mockFs.readdirSync.mockReturnValue([\n      'file1.ts',\n      'file2.ts',\n      'file3.js',\n      'readme.md'\n    ] as any);\n    mockFs.statSync.mockReturnValue({ isDirectory: () => false, isFile: () => true } as any);\n\n    const result = helper.searchDirectory('/src', ['.ts']);\n\n    expect(result.length).toBeGreaterThanOrEqual(2);\n    expect(result.some(f => f.endsWith('.ts'))).toBe(true);\n  });\n\n  test('filters files by extension correctly', () => {\n    mockFs.readdirSync.mockReturnValue(['test.ts', 'test.js', 'test.py'] as any);\n    mockFs.statSync.mockReturnValue({ isDirectory: () => false, isFile: () => true } as any);\n\n    const result = helper.searchDirectory('/src', ['.ts']);\n\n    expect(result.every(f => f.endsWith('.ts'))).toBe(true);\n  });\n\n  test('respects ignore patterns', () => {\n    mockFs.readdirSync.mockReturnValue(['node_modules', 'src', 'test'] as any);\n    mockFs.statSync.mockReturnValue({ isDirectory: () => true, isFile: () => false } as any);\n\n    const result = helper.searchDirectory('/', ['.ts'], ['node_modules']);\n\n    expect(result.some(f => f.includes('node_modules'))).toBe(false);\n  });\n\n  test('handles nested directory structure', () => {\n    mockFs.readdirSync\n      .mockReturnValueOnce(['src'] as any)\n      .mockReturnValueOnce(['subdir'] as any)\n      .mockReturnValueOnce(['file.ts'] as any);\n    mockFs.statSync\n      .mockReturnValueOnce({ isDirectory: () => true, isFile: () => false } as any)\n      .mockReturnValueOnce({ isDirectory: () => true, isFile: () => false } as any)\n      .mockReturnValueOnce({ isDirectory: () => false, isFile: () => true } as any);\n\n    const result = helper.searchDirectory('/', ['.ts']);\n\n    expect(result.length).toBeGreaterThan(0);\n  });\n});",
            "run_instructions": "npm test -- fileAccessHelper.test.ts -t 'finds TypeScript files in directory'"
          }
        ]
      },
      {
        "id": "response-parser",
        "name": "LLM Response Parser",
        "description": "Tests parsing and validation of LLM responses against JSON schemas",
        "test_file_path": "src/test/llmResponseParser.test.ts",
        "source_files": [
          "src/ai/llmResponseParser.ts"
        ],
        "test_cases": [
          {
            "id": "response-parser-valid",
            "name": "test_parseResponse_validatesAgainstSchema",
            "description": "Verifies parser validates responses against provided JSON schemas",
            "target_function": "parseResponse",
            "target_file": "src/ai/llmResponseParser.ts",
            "scenarios": [
              "Valid response passes validation",
              "Invalid response rejected",
              "Missing required fields detected"
            ],
            "mocks": [],
            "assertions": [
              "Valid JSON passes schema validation",
              "Invalid JSON throws error",
              "Error messages are descriptive"
            ],
            "priority": "high",
            "test_code": "import { LLMResponseParser } from '../ai/llmResponseParser';\n\ndescribe('LLMResponseParser.parseResponse', () => {\n  let parser: LLMResponseParser;\n\n  beforeEach(() => {\n    parser = new LLMResponseParser();\n  });\n\n  test('validates response against schema successfully', () => {\n    const schema = {\n      type: 'object',\n      properties: {\n        result: { type: 'string' },\n        count: { type: 'number' }\n      },\n      required: ['result']\n    };\n    const response = { result: 'success', count: 42 };\n\n    const parsed = parser.parseResponse(JSON.stringify(response), schema);\n\n    expect(parsed.result).toBe('success');\n    expect(parsed.count).toBe(42);\n  });\n\n  test('rejects response with missing required fields', () => {\n    const schema = {\n      type: 'object',\n      properties: {\n        required_field: { type: 'string' }\n      },\n      required: ['required_field']\n    };\n    const response = { optional_field: 'value' };\n\n    expect(() => parser.parseResponse(JSON.stringify(response), schema)).toThrow();\n  });\n\n  test('rejects response with wrong types', () => {\n    const schema = {\n      type: 'object',\n      properties: {\n        count: { type: 'number' }\n      },\n      required: ['count']\n    };\n    const response = { count: 'not a number' };\n\n    expect(() => parser.parseResponse(JSON.stringify(response), schema)).toThrow();\n  });\n\n  test('handles nested object validation', () => {\n    const schema = {\n      type: 'object',\n      properties: {\n        data: {\n          type: 'object',\n          properties: {\n            nested: { type: 'string' }\n          },\n          required: ['nested']\n        }\n      },\n      required: ['data']\n    };\n    const response = { data: { nested: 'value' } };\n\n    const parsed = parser.parseResponse(JSON.stringify(response), schema);\n\n    expect(parsed.data.nested).toBe('value');\n  });\n});",
            "run_instructions": "npm test -- llmResponseParser.test.ts -t 'validates response against schema successfully'"
          }
        ]
      },
      {
        "id": "rate-limiter",
        "name": "LLM Rate Limiting",
        "description": "Tests rate limiting functionality for LLM API calls",
        "test_file_path": "src/test/llmRateLimiter.test.ts",
        "source_files": [
          "src/ai/llmRateLimiter.ts"
        ],
        "test_cases": [
          {
            "id": "rate-limiter-throttle",
            "name": "test_rateLimiter_throttlesRequests",
            "description": "Verifies rate limiter properly throttles API requests",
            "target_function": "throttle",
            "target_file": "src/ai/llmRateLimiter.ts",
            "scenarios": [
              "Delays requests when limit reached",
              "Allows requests when under limit",
              "Tracks request count correctly"
            ],
            "mocks": [],
            "assertions": [
              "Requests delayed appropriately",
              "Rate limit enforced",
              "Token tracking accurate"
            ],
            "priority": "medium",
            "test_code": "import { LLMRateLimiter } from '../ai/llmRateLimiter';\n\ndescribe('LLMRateLimiter.throttle', () => {\n  let rateLimiter: LLMRateLimiter;\n\n  beforeEach(() => {\n    rateLimiter = new LLMRateLimiter({ requestsPerMinute: 10, tokensPerMinute: 100000 });\n  });\n\n  test('allows requests when under limit', async () => {\n    const start = Date.now();\n\n    await rateLimiter.throttle();\n\n    const duration = Date.now() - start;\n    expect(duration).toBeLessThan(100);\n  });\n\n  test('tracks request count correctly', async () => {\n    for (let i = 0; i  {\n    const limit = 3;\n    const limiter = new LLMRateLimiter({ requestsPerMinute: limit, tokensPerMinute: 100000 });\n\n    for (let i = 0; i  {\n    await rateLimiter.throttle(1000);\n    await rateLimiter.throttle(2000);\n\n    const stats = rateLimiter.getStats();\n    expect(stats.tokenCount).toBe(3000);\n  });\n});",
            "run_instructions": "npm test -- llmRateLimiter.test.ts -t 'allows requests when under limit'"
          }
        ]
      },
      {
        "id": "retry-handler",
        "name": "LLM Retry Logic",
        "description": "Tests retry handler for LLM API failures",
        "test_file_path": "src/test/llmRetryHandler.test.ts",
        "source_files": [
          "src/ai/llmRetryHandler.ts"
        ],
        "test_cases": [
          {
            "id": "retry-handler-transient",
            "name": "test_retryHandler_retriesTransientFailures",
            "description": "Verifies retry handler retries transient failures appropriately",
            "target_function": "executeWithRetry",
            "target_file": "src/ai/llmRetryHandler.ts",
            "scenarios": [
              "Retries on network error",
              "Retries on rate limit",
              "Gives up after max retries",
              "Success on retry"
            ],
            "mocks": [],
            "assertions": [
              "Retries correct number of times",
              "Exponential backoff applied",
              "Success returned on retry",
              "Final failure thrown"
            ],
            "priority": "high",
            "test_code": "import { LLMRetryHandler } from '../ai/llmRetryHandler';\n\ndescribe('LLMRetryHandler.executeWithRetry', () => {\n  let retryHandler: LLMRetryHandler;\n\n  beforeEach(() => {\n    retryHandler = new LLMRetryHandler({ maxRetries: 3, initialDelay: 100 });\n  });\n\n  test('retries on transient network errors', async () => {\n    let attempts = 0;\n    const operation = jest.fn(async () => {\n      attempts++;\n      if (attempts  {\n    const operation = jest.fn(async () => {\n      throw new Error('Persistent error');\n    });\n\n    await expect(retryHandler.executeWithRetry(operation)).rejects.toThrow('Persistent error');\n    expect(operation).toHaveBeenCalledTimes(4);\n  });\n\n  test('succeeds on first attempt without retry', async () => {\n    const operation = jest.fn(async () => 'immediate success');\n\n    const result = await retryHandler.executeWithRetry(operation);\n\n    expect(result).toBe('immediate success');\n    expect(operation).toHaveBeenCalledTimes(1);\n  });\n\n  test('applies exponential backoff between retries', async () => {\n    let attempts = 0;\n    const timestamps: number[] = [];\n    const operation = jest.fn(async () => {\n      timestamps.push(Date.now());\n      attempts++;\n      if (attempts < 3) {\n        throw new Error('Retry me');\n      }\n      return 'success';\n    });\n\n    await retryHandler.executeWithRetry(operation);\n\n    expect(timestamps.length).toBe(3);\n    const delay1 = timestamps[1] - timestamps[0];\n    const delay2 = timestamps[2] - timestamps[1];\n    expect(delay2).toBeGreaterThan(delay1);\n  });\n});",
            "run_instructions": "npm test -- llmRetryHandler.test.ts -t 'retries on transient network errors'"
          }
        ]
      },
      {
        "id": "provider-factory",
        "name": "LLM Provider Factory",
        "description": "Tests provider factory for creating appropriate LLM provider instances",
        "test_file_path": "src/test/providerFactory.test.ts",
        "source_files": [
          "src/ai/providers/providerFactory.ts"
        ],
        "test_cases": [
          {
            "id": "provider-factory-create",
            "name": "test_createProvider_returnsCorrectProvider",
            "description": "Verifies factory creates correct provider based on configuration",
            "target_function": "createProvider",
            "target_file": "src/ai/providers/providerFactory.ts",
            "scenarios": [
              "Creates OpenAI provider",
              "Creates Anthropic provider",
              "Throws for unknown provider"
            ],
            "mocks": [
              "ConfigurationManager"
            ],
            "assertions": [
              "Correct provider instance returned",
              "Provider configured properly",
              "Invalid config throws error"
            ],
            "priority": "medium",
            "test_code": "import { ProviderFactory } from '../ai/providers/providerFactory';\nimport { ConfigurationManager } from '../config/configurationManager';\n\njest.mock('../config/configurationManager');\n\nconst mockConfig = {\n  getCurrentProvider: jest.fn(),\n  getOpenAIKey: jest.fn().mockReturnValue('test-key'),\n  getClaudeKey: jest.fn().mockReturnValue('test-key'),\n  getMaxTokens: jest.fn().mockReturnValue(4000)\n} as any;\n\ndescribe('ProviderFactory.createProvider', () => {\n  beforeEach(() => {\n    (ConfigurationManager.getInstance as jest.Mock).mockReturnValue(mockConfig);\n    jest.clearAllMocks();\n  });\n\n  test('creates OpenAI provider when configured', () => {\n    mockConfig.getCurrentProvider.mockReturnValue('openai');\n\n    const provider = ProviderFactory.createProvider();\n\n    expect(provider).toBeDefined();\n    expect(provider.getName()).toBe('openai');\n  });\n\n  test('creates Anthropic provider when configured', () => {\n    mockConfig.getCurrentProvider.mockReturnValue('anthropic');\n\n    const provider = ProviderFactory.createProvider();\n\n    expect(provider).toBeDefined();\n    expect(provider.getName()).toBe('anthropic');\n  });\n\n  test('throws error for unknown provider', () => {\n    mockConfig.getCurrentProvider.mockReturnValue('unknown-provider');\n\n    expect(() => ProviderFactory.createProvider()).toThrow();\n  });\n\n  test('validates configuration before creating provider', () => {\n    mockConfig.getCurrentProvider.mockReturnValue('openai');\n    mockConfig.getOpenAIKey.mockReturnValue('');\n\n    expect(() => ProviderFactory.createProvider()).toThrow();\n  });\n});",
            "run_instructions": "npm test -- providerFactory.test.ts -t 'creates OpenAI provider when configured'"
          }
        ]
      },
      {
        "id": "config-manager",
        "name": "Configuration Management",
        "description": "Tests configuration manager for accessing and validating extension settings",
        "test_file_path": "src/test/configurationManager.test.ts",
        "source_files": [
          "src/config/configurationManager.ts"
        ],
        "test_cases": [
          {
            "id": "config-manager-get",
            "name": "test_configManager_retrievesSettings",
            "description": "Verifies configuration manager retrieves settings correctly",
            "target_function": "getConfiguration",
            "target_file": "src/config/configurationManager.ts",
            "scenarios": [
              "Retrieves provider setting",
              "Retrieves API keys",
              "Returns defaults when not configured"
            ],
            "mocks": [
              "vscode.workspace"
            ],
            "assertions": [
              "Settings retrieved correctly",
              "Defaults applied when missing",
              "Type validation works"
            ],
            "priority": "medium",
            "test_code": "import { ConfigurationManager } from '../config/configurationManager';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\nconst mockWorkspace = {\n  getConfiguration: jest.fn().mockReturnValue({\n    get: jest.fn((key: string, defaultValue: any) => {\n      const config: any = {\n        'shadowWatch.provider': 'openai',\n        'shadowWatch.openaiApiKey': 'test-key',\n        'shadowWatch.maxTokens': 4000\n      };\n      return config[key] || defaultValue;\n    })\n  })\n};\n\n(vscode.workspace as any) = mockWorkspace;\n\ndescribe('ConfigurationManager', () => {\n  let configManager: ConfigurationManager;\n\n  beforeEach(() => {\n    configManager = ConfigurationManager.getInstance();\n    jest.clearAllMocks();\n  });\n\n  test('retrieves provider setting correctly', () => {\n    const provider = configManager.getCurrentProvider();\n\n    expect(provider).toBe('openai');\n  });\n\n  test('retrieves API key correctly', () => {\n    const apiKey = configManager.getOpenAIKey();\n\n    expect(apiKey).toBe('test-key');\n  });\n\n  test('returns default when setting not configured', () => {\n    const maxTokens = configManager.getMaxTokens();\n\n    expect(maxTokens).toBeDefined();\n    expect(typeof maxTokens).toBe('number');\n  });\n\n  test('validates configuration values', () => {\n    const isValid = configManager.validateConfiguration();\n\n    expect(typeof isValid).toBe('boolean');\n  });\n});",
            "run_instructions": "npm test -- configurationManager.test.ts -t 'retrieves provider setting correctly'"
          }
        ]
      }
    ],
    "read_write_test_suites": [],
    "user_workflow_test_suites": []
  }
}