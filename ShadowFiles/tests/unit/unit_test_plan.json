{
  "rationale": "This comprehensive unit test plan provides thorough coverage of the Shadow Watch codebase to prevent regressions during refactoring. The tests focus on critical areas identified in the architecture analysis: (1) Core analysis logic (analyzer.ts) that detects god objects, circular dependencies, and complexity issues - these are the foundation of the product and must work correctly. (2) AI integration (llmService.ts, llmIntegration.ts) which are the largest and most complex files requiring refactoring - tests ensure behavior is preserved during splitting. (3) Extension entry point (extension.ts) to verify command registration and lifecycle management remain correct. (4) View components (insightsTreeView, productNavigator) to ensure UI continues working. (5) Support infrastructure (cache, fileWatcher, diagnosticsProvider) to validate that continuous monitoring and performance optimizations function properly. Each test suite includes executable TypeScript code using Jest with proper mocking strategies, covering happy paths, edge cases, and error handling. Tests are prioritized based on impact (high priority for core analysis and AI integration, medium for UI and utilities) to guide testing efforts. The plan addresses the massive file sizes mentioned in architecture insights by ensuring comprehensive test coverage before refactoring, allowing safe splitting of large modules while maintaining functionality.",
  "aggregated_plan": {
    "unit_test_plan": {
      "strategy": "Test the TypeScript codebase using Jest as the primary testing framework. Focus on unit testing core analysis logic (analyzer.ts, insightGenerator.ts), AI integration (llmService.ts, llmIntegration.ts), and view components (insightsTreeView.ts, productNavigator.ts) in isolation. Mock external dependencies like VS Code API, file system operations, and HTTP clients. Use dependency injection where possible to facilitate testing. Test both happy paths and edge cases for all critical functions. Organize tests by source file with one test suite per major module. Prioritize testing functions mentioned in architecture insights and core business logic that would cause regressions if broken.",
      "testing_framework": "jest",
      "mocking_approach": "Use Jest's built-in mocking capabilities (jest.fn(), jest.mock()) to mock external dependencies. Mock the VS Code API using a custom mock implementation since it's not available in Node test environment. Mock file system operations (fs) using jest.mock('fs'). Mock HTTP clients (openai, @anthropic-ai/sdk) for AI service tests. Use jest.spyOn() to spy on internal methods while keeping implementation. Create mock factories for complex objects like AnalysisResult, ArchitectureIssue, and VS Code types.",
      "isolation_strategy": "Test each module in complete isolation by mocking all external dependencies. Core analysis logic (analyzer.ts) can be tested independently by mocking file system. AI integration (llmService.ts) can be tested by mocking HTTP clients. View components can be tested by mocking VS Code API and data sources. Cache module should be tested by mocking file system but using real cache logic. Only integration tests should test multiple modules together."
    },
    "test_suites": [
      {
        "id": "analyzer-tests",
        "name": "Code Analyzer Tests",
        "description": "Tests for core static analysis functionality including god object detection, circular dependency finding, complexity calculation, and health score computation",
        "test_file_path": "src/test/analyzer.test.ts",
        "source_files": [
          "src/analyzer.ts"
        ],
        "test_cases": [
          {
            "id": "test-detect-god-objects",
            "name": "test_detectGodObjects_identifies_large_files",
            "description": "Verifies that detectGodObjects correctly identifies files exceeding the line count threshold as god objects",
            "target_function": "detectGodObjects",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "File with 1000 lines is detected as god object",
              "File with 400 lines is not detected",
              "Multiple files with mixed sizes"
            ],
            "mocks": [
              "fs.readFileSync to return mock file contents",
              "fs.statSync to return mock file stats"
            ],
            "assertions": [
              "God objects array contains files over threshold",
              "Files under threshold are not included",
              "Correct file paths are returned"
            ],
            "priority": "high",
            "test_code": "import { CodeAnalyzer } from '../analyzer';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('CodeAnalyzer.detectGodObjects', () => {\n  let analyzer: CodeAnalyzer;\n  const mockWorkspaceRoot = '/mock/workspace';\n\n  beforeEach(() => {\n    analyzer = new CodeAnalyzer(mockWorkspaceRoot);\n    jest.clearAllMocks();\n  });\n\n  test('identifies files exceeding line count threshold', () => {\n    const mockFiles = [\n      { path: '/mock/workspace/large.ts', lines: 1000 },\n      { path: '/mock/workspace/small.ts', lines: 300 }\n    ];\n    \n    (fs.readFileSync as jest.Mock).mockImplementation((path: string) => {\n      const file = mockFiles.find(f => f.path === path);\n      return file ? 'line\\n'.repeat(file.lines) : '';\n    });\n\n    const godObjects = analyzer.detectGodObjects(mockFiles.map(f => f.path));\n    \n    expect(godObjects.length).toBe(1);\n    expect(godObjects[0]).toContain('large.ts');\n    expect(godObjects[0]).not.toContain('small.ts');\n  });\n\n  test('returns empty array when no god objects exist', () => {\n    const mockFiles = ['/mock/workspace/small1.ts', '/mock/workspace/small2.ts'];\n    \n    (fs.readFileSync as jest.Mock).mockReturnValue('line\\n'.repeat(200));\n\n    const godObjects = analyzer.detectGodObjects(mockFiles);\n    \n    expect(godObjects).toEqual([]);\n  });\n\n  test('handles multiple large files correctly', () => {\n    const mockFiles = ['/file1.ts', '/file2.ts', '/file3.ts'];\n    \n    (fs.readFileSync as jest.Mock).mockReturnValue('line\\n'.repeat(1500));\n\n    const godObjects = analyzer.detectGodObjects(mockFiles);\n    \n    expect(godObjects.length).toBe(3);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'identifies files exceeding line count threshold'"
          },
          {
            "id": "test-circular-dependencies",
            "name": "test_findCircularDependencies_detects_cycles",
            "description": "Verifies that findCircularDependencies correctly identifies circular import chains between modules",
            "target_function": "findCircularDependencies",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "Two files importing each other",
              "Three-file circular chain",
              "No circular dependencies present"
            ],
            "mocks": [
              "File system to return mock import statements"
            ],
            "assertions": [
              "Circular dependency chains are detected",
              "Chain includes all files in cycle",
              "No false positives for non-circular imports"
            ],
            "priority": "high",
            "test_code": "import { CodeAnalyzer } from '../analyzer';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('CodeAnalyzer.findCircularDependencies', () => {\n  let analyzer: CodeAnalyzer;\n  const mockWorkspaceRoot = '/mock/workspace';\n\n  beforeEach(() => {\n    analyzer = new CodeAnalyzer(mockWorkspaceRoot);\n    jest.clearAllMocks();\n  });\n\n  test('detects simple two-file circular dependency', () => {\n    const fileContents: Record<string, string> = {\n      '/mock/workspace/fileA.ts': \"import { funcB } from './fileB';\",\n      '/mock/workspace/fileB.ts': \"import { funcA } from './fileA';\"\n    };\n    \n    (fs.readFileSync as jest.Mock).mockImplementation((path: string) => {\n      return fileContents[path] || '';\n    });\n\n    const cycles = analyzer.findCircularDependencies(['/mock/workspace/fileA.ts', '/mock/workspace/fileB.ts']);\n    \n    expect(cycles.length).toBeGreaterThan(0);\n    expect(cycles[0]).toContain('fileA');\n    expect(cycles[0]).toContain('fileB');\n  });\n\n  test('detects three-file circular chain', () => {\n    const fileContents: Record<string, string> = {\n      '/mock/workspace/a.ts': \"import './b';\",\n      '/mock/workspace/b.ts': \"import './c';\",\n      '/mock/workspace/c.ts': \"import './a';\"\n    };\n    \n    (fs.readFileSync as jest.Mock).mockImplementation((path: string) => fileContents[path] || '');\n\n    const cycles = analyzer.findCircularDependencies(['/mock/workspace/a.ts', '/mock/workspace/b.ts', '/mock/workspace/c.ts']);\n    \n    expect(cycles.length).toBeGreaterThan(0);\n  });\n\n  test('returns empty when no circular dependencies exist', () => {\n    const fileContents: Record<string, string> = {\n      '/mock/workspace/main.ts': \"import './helper';\",\n      '/mock/workspace/helper.ts': \"export function help() {}\"\n    };\n    \n    (fs.readFileSync as jest.Mock).mockImplementation((path: string) => fileContents[path] || '');\n\n    const cycles = analyzer.findCircularDependencies(['/mock/workspace/main.ts', '/mock/workspace/helper.ts']);\n    \n    expect(cycles).toEqual([]);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'detects simple two-file circular dependency'"
          },
          {
            "id": "test-calculate-complexity",
            "name": "test_calculateComplexity_measures_cyclomatic_complexity",
            "description": "Verifies that calculateComplexity correctly computes cyclomatic complexity for functions with various control flow structures",
            "target_function": "calculateComplexity",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "Simple function with no branches",
              "Function with if statements",
              "Function with loops and switches"
            ],
            "mocks": [
              "TypeScript parser or AST traversal"
            ],
            "assertions": [
              "Complexity matches expected value",
              "Each decision point increments complexity",
              "Linear code has complexity of 1"
            ],
            "priority": "high",
            "test_code": "import { CodeAnalyzer } from '../analyzer';\n\ndescribe('CodeAnalyzer.calculateComplexity', () => {\n  let analyzer: CodeAnalyzer;\n\n  beforeEach(() => {\n    analyzer = new CodeAnalyzer('/mock');\n  });\n\n  test('calculates complexity 1 for simple linear function', () => {\n    const code = `\n      function simple() {\n        const x = 1;\n        return x + 1;\n      }\n    `;\n    \n    const complexity = analyzer.calculateComplexity(code);\n    \n    expect(complexity).toBe(1);\n  });\n\n  test('increments complexity for each if statement', () => {\n    const code = `\n      function withIfs(x: number) {\n        if (x > 0) {\n          return 1;\n        }\n        if (x < 0) {\n          return -1;\n        }\n        return 0;\n      }\n    `;\n    \n    const complexity = analyzer.calculateComplexity(code);\n    \n    expect(complexity).toBeGreaterThanOrEqual(3);\n  });\n\n  test('counts loops and switches in complexity', () => {\n    const code = `\n      function complex(arr: number[]) {\n        for (let i = 0; i < arr.length; i++) {\n          switch (arr[i]) {\n            case 1: return 'one';\n            case 2: return 'two';\n            default: return 'other';\n          }\n        }\n      }\n    `;\n    \n    const complexity = analyzer.calculateComplexity(code);\n    \n    expect(complexity).toBeGreaterThanOrEqual(4);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'calculates complexity 1 for simple linear function'"
          },
          {
            "id": "test-calculate-health-score",
            "name": "test_calculateHealthScore_computes_percentage",
            "description": "Verifies that calculateHealthScore correctly computes overall codebase health percentage based on detected issues",
            "target_function": "calculateHealthScore",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "No issues returns 100%",
              "Mix of errors and warnings",
              "All critical errors"
            ],
            "mocks": [
              "None - pure calculation"
            ],
            "assertions": [
              "Score is between 0 and 100",
              "Errors reduce score more than warnings",
              "Score decreases with more issues"
            ],
            "priority": "high",
            "test_code": "import { CodeAnalyzer } from '../analyzer';\n\ndescribe('CodeAnalyzer.calculateHealthScore', () => {\n  let analyzer: CodeAnalyzer;\n\n  beforeEach(() => {\n    analyzer = new CodeAnalyzer('/mock');\n  });\n\n  test('returns 100% for codebase with no issues', () => {\n    const issues: any[] = [];\n    \n    const score = analyzer.calculateHealthScore(issues);\n    \n    expect(score).toBe(100);\n  });\n\n  test('reduces score for errors more than warnings', () => {\n    const issuesWithErrors = [\n      { severity: 'error', category: 'complexity' },\n      { severity: 'error', category: 'complexity' }\n    ];\n    const issuesWithWarnings = [\n      { severity: 'warning', category: 'style' },\n      { severity: 'warning', category: 'style' }\n    ];\n    \n    const scoreErrors = analyzer.calculateHealthScore(issuesWithErrors);\n    const scoreWarnings = analyzer.calculateHealthScore(issuesWithWarnings);\n    \n    expect(scoreErrors).toBeLessThan(scoreWarnings);\n  });\n\n  test('returns score between 0 and 100', () => {\n    const manyIssues = Array(100).fill({ severity: 'error', category: 'complexity' });\n    \n    const score = analyzer.calculateHealthScore(manyIssues);\n    \n    expect(score).toBeGreaterThanOrEqual(0);\n    expect(score).toBeLessThanOrEqual(100);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'returns 100% for codebase with no issues'"
          },
          {
            "id": "test-identify-dead-code",
            "name": "test_identifyDeadCode_finds_unused_functions",
            "description": "Verifies that identifyDeadCode correctly finds unused functions, classes, and imports that can be removed",
            "target_function": "identifyDeadCode",
            "target_file": "src/analyzer.ts",
            "scenarios": [
              "Unused exported function",
              "Function only used in comments",
              "All functions are used"
            ],
            "mocks": [
              "File system to provide code contents"
            ],
            "assertions": [
              "Unused functions are identified",
              "Used functions are not flagged",
              "Correct file and line numbers"
            ],
            "priority": "medium",
            "test_code": "import { CodeAnalyzer } from '../analyzer';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('CodeAnalyzer.identifyDeadCode', () => {\n  let analyzer: CodeAnalyzer;\n\n  beforeEach(() => {\n    analyzer = new CodeAnalyzer('/mock');\n    jest.clearAllMocks();\n  });\n\n  test('identifies unused exported function', () => {\n    const fileContents: Record<string, string> = {\n      '/mock/main.ts': \"import { usedFunc } from './helpers';\\nusedFunc();\",\n      '/mock/helpers.ts': \"export function usedFunc() {}\\nexport function unusedFunc() {}\"\n    };\n    \n    (fs.readFileSync as jest.Mock).mockImplementation((path: string) => fileContents[path] || '');\n\n    const deadCode = analyzer.identifyDeadCode(['/mock/main.ts', '/mock/helpers.ts']);\n    \n    expect(deadCode.length).toBeGreaterThan(0);\n    expect(deadCode.some((dc: any) => dc.includes('unusedFunc'))).toBe(true);\n  });\n\n  test('returns empty array when all code is used', () => {\n    const fileContents: Record<string, string> = {\n      '/mock/main.ts': \"import { func1, func2 } from './helpers';\\nfunc1();\\nfunc2();\",\n      '/mock/helpers.ts': \"export function func1() {}\\nexport function func2() {}\"\n    };\n    \n    (fs.readFileSync as jest.Mock).mockImplementation((path: string) => fileContents[path] || '');\n\n    const deadCode = analyzer.identifyDeadCode(['/mock/main.ts', '/mock/helpers.ts']);\n    \n    expect(deadCode).toEqual([]);\n  });\n});",
            "run_instructions": "npm test -- analyzer.test.ts -t 'identifies unused exported function'"
          }
        ]
      },
      {
        "id": "llmservice-tests",
        "name": "LLM Service Tests",
        "description": "Tests for AI language model service integration including API calls, response parsing, prompt building, and provider management",
        "test_file_path": "src/test/llmService.test.ts",
        "source_files": [
          "src/llmService.ts"
        ],
        "test_cases": [
          {
            "id": "test-generate-product-docs",
            "name": "test_generateProductDocs_calls_openai_api",
            "description": "Verifies that generateProductDocs correctly calls OpenAI API with proper prompt and parses response",
            "target_function": "generateProductDocs",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "Successful API call returns documentation",
              "API error is handled gracefully",
              "Invalid response format is detected"
            ],
            "mocks": [
              "OpenAI client to return mock responses",
              "File system for reading code"
            ],
            "assertions": [
              "API is called with correct parameters",
              "Response is parsed correctly",
              "Errors are caught and reported"
            ],
            "priority": "high",
            "test_code": "import { LLMService } from '../llmService';\nimport { OpenAI } from 'openai';\n\njest.mock('openai');\n\ndescribe('LLMService.generateProductDocs', () => {\n  let service: LLMService;\n  let mockOpenAI: jest.Mocked<OpenAI>;\n\n  beforeEach(() => {\n    mockOpenAI = {\n      chat: {\n        completions: {\n          create: jest.fn()\n        }\n      }\n    } as any;\n    service = new LLMService();\n    (service as any).openai = mockOpenAI;\n  });\n\n  test('calls OpenAI API with correct prompt structure', async () => {\n    const mockResponse = {\n      choices: [{\n        message: {\n          content: JSON.stringify({\n            product_name: 'Test App',\n            product_purpose: 'Testing',\n            key_features: []\n          })\n        }\n      }]\n    };\n    \n    mockOpenAI.chat.completions.create.mockResolvedValue(mockResponse as any);\n\n    const codeContext = { files: [], stats: { totalFiles: 1, totalLines: 100 } };\n    const result = await service.generateProductDocs(codeContext);\n    \n    expect(mockOpenAI.chat.completions.create).toHaveBeenCalled();\n    expect(result).toBeDefined();\n    expect(result.product_name).toBe('Test App');\n  });\n\n  test('handles API errors gracefully', async () => {\n    mockOpenAI.chat.completions.create.mockRejectedValue(new Error('API Error'));\n\n    const codeContext = { files: [], stats: { totalFiles: 1, totalLines: 100 } };\n    \n    await expect(service.generateProductDocs(codeContext)).rejects.toThrow('API Error');\n  });\n\n  test('validates response schema', async () => {\n    const invalidResponse = {\n      choices: [{\n        message: {\n          content: JSON.stringify({ invalid: 'schema' })\n        }\n      }]\n    };\n    \n    mockOpenAI.chat.completions.create.mockResolvedValue(invalidResponse as any);\n\n    const codeContext = { files: [], stats: { totalFiles: 1, totalLines: 100 } };\n    \n    await expect(service.generateProductDocs(codeContext)).rejects.toThrow();\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t 'calls OpenAI API with correct prompt structure'"
          },
          {
            "id": "test-generate-architecture-insights",
            "name": "test_generateArchitectureInsights_analyzes_patterns",
            "description": "Verifies that generateArchitectureInsights correctly analyzes codebase and returns architecture patterns",
            "target_function": "generateArchitectureInsights",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "Identifies design patterns",
              "Detects architecture layers",
              "Finds component relationships"
            ],
            "mocks": [
              "AI provider to return mock insights"
            ],
            "assertions": [
              "Insights contain architecture patterns",
              "Component relationships are mapped",
              "Design patterns are identified"
            ],
            "priority": "high",
            "test_code": "import { LLMService } from '../llmService';\n\njest.mock('openai');\n\ndescribe('LLMService.generateArchitectureInsights', () => {\n  let service: LLMService;\n\n  beforeEach(() => {\n    service = new LLMService();\n    (service as any).openai = {\n      chat: {\n        completions: {\n          create: jest.fn()\n        }\n      }\n    };\n  });\n\n  test('identifies architecture patterns in codebase', async () => {\n    const mockInsights = {\n      architecture_patterns: ['MVC', 'Singleton'],\n      component_relationships: [],\n      data_flow: 'unidirectional'\n    };\n    \n    (service as any).openai.chat.completions.create.mockResolvedValue({\n      choices: [{ message: { content: JSON.stringify(mockInsights) } }]\n    });\n\n    const codeContext = { files: [], stats: { totalFiles: 5, totalLines: 1000 } };\n    const result = await service.generateArchitectureInsights(codeContext);\n    \n    expect(result.architecture_patterns).toContain('MVC');\n    expect(result.architecture_patterns).toContain('Singleton');\n  });\n\n  test('maps component relationships', async () => {\n    const mockInsights = {\n      architecture_patterns: [],\n      component_relationships: [\n        { from: 'Controller', to: 'Service', relationship: 'uses' }\n      ]\n    };\n    \n    (service as any).openai.chat.completions.create.mockResolvedValue({\n      choices: [{ message: { content: JSON.stringify(mockInsights) } }]\n    });\n\n    const codeContext = { files: [], stats: { totalFiles: 3, totalLines: 500 } };\n    const result = await service.generateArchitectureInsights(codeContext);\n    \n    expect(result.component_relationships).toHaveLength(1);\n    expect(result.component_relationships[0].from).toBe('Controller');\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t 'identifies architecture patterns in codebase'"
          },
          {
            "id": "test-switch-provider",
            "name": "test_switchProvider_changes_ai_backend",
            "description": "Verifies that switching between AI providers (OpenAI, Claude, custom) works correctly",
            "target_function": "switchProvider",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "Switch from OpenAI to Claude",
              "Switch to custom endpoint",
              "Invalid provider handled"
            ],
            "mocks": [
              "Provider initialization"
            ],
            "assertions": [
              "Provider is changed",
              "API client is reinitialized",
              "Subsequent calls use new provider"
            ],
            "priority": "medium",
            "test_code": "import { LLMService } from '../llmService';\n\ndescribe('LLMService.switchProvider', () => {\n  let service: LLMService;\n\n  beforeEach(() => {\n    service = new LLMService();\n  });\n\n  test('switches from OpenAI to Claude provider', () => {\n    service.switchProvider('openai', 'sk-test-key');\n    expect((service as any).currentProvider).toBe('openai');\n    \n    service.switchProvider('claude', 'sk-ant-test-key');\n    expect((service as any).currentProvider).toBe('claude');\n  });\n\n  test('switches to custom endpoint', () => {\n    service.switchProvider('custom', 'custom-key', 'https://custom.api/v1');\n    \n    expect((service as any).currentProvider).toBe('custom');\n    expect((service as any).customEndpoint).toBe('https://custom.api/v1');\n  });\n\n  test('throws error for invalid provider', () => {\n    expect(() => {\n      service.switchProvider('invalid' as any, 'key');\n    }).toThrow();\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t 'switches from OpenAI to Claude provider'"
          },
          {
            "id": "test-validate-response-schema",
            "name": "test_validateResponseSchema_checks_json",
            "description": "Verifies that AI responses are validated against expected JSON schemas",
            "target_function": "validateResponseSchema",
            "target_file": "src/llmService.ts",
            "scenarios": [
              "Valid schema passes",
              "Missing required field fails",
              "Invalid type fails"
            ],
            "mocks": [
              "None - pure validation"
            ],
            "assertions": [
              "Valid schemas pass validation",
              "Invalid schemas throw errors",
              "Error messages are descriptive"
            ],
            "priority": "medium",
            "test_code": "import { LLMService } from '../llmService';\n\ndescribe('LLMService.validateResponseSchema', () => {\n  let service: LLMService;\n\n  beforeEach(() => {\n    service = new LLMService();\n  });\n\n  test('validates correct product docs schema', () => {\n    const validSchema = {\n      product_name: 'Test App',\n      product_purpose: 'Testing',\n      key_features: ['feature1'],\n      user_workflows: []\n    };\n    \n    expect(() => {\n      (service as any).validateProductDocsSchema(validSchema);\n    }).not.toThrow();\n  });\n\n  test('throws error for missing required field', () => {\n    const invalidSchema = {\n      product_name: 'Test App'\n    };\n    \n    expect(() => {\n      (service as any).validateProductDocsSchema(invalidSchema);\n    }).toThrow(/required/);\n  });\n\n  test('throws error for invalid field type', () => {\n    const invalidSchema = {\n      product_name: 123,\n      product_purpose: 'Testing',\n      key_features: 'not an array'\n    };\n    \n    expect(() => {\n      (service as any).validateProductDocsSchema(invalidSchema);\n    }).toThrow();\n  });\n});",
            "run_instructions": "npm test -- llmService.test.ts -t 'validates correct product docs schema'"
          }
        ]
      },
      {
        "id": "llmintegration-tests",
        "name": "LLM Integration Tests",
        "description": "Tests for high-level LLM integration orchestration including workflow coordination, result formatting, and state management",
        "test_file_path": "src/test/llmIntegration.test.ts",
        "source_files": [
          "src/llmIntegration.ts"
        ],
        "test_cases": [
          {
            "id": "test-initialize-llm-service",
            "name": "test_initializeLLMService_sets_up_provider",
            "description": "Verifies that LLM service is initialized with correct configuration",
            "target_function": "initializeLLMService",
            "target_file": "src/llmIntegration.ts",
            "scenarios": [
              "Initialize with API key",
              "Initialize without key",
              "Load saved configuration"
            ],
            "mocks": [
              "VS Code configuration API"
            ],
            "assertions": [
              "Service is initialized",
              "Configuration is loaded",
              "Default provider is set"
            ],
            "priority": "high",
            "test_code": "import * as vscode from 'vscode';\nimport { initializeLLMService } from '../llmIntegration';\n\njest.mock('vscode');\n\ndescribe('initializeLLMService', () => {\n  beforeEach(() => {\n    jest.clearAllMocks();\n  });\n\n  test('initializes service with saved API key', async () => {\n    const mockConfig = {\n      get: jest.fn((key: string) => {\n        if (key === 'openaiApiKey') return 'sk-test-key';\n        return null;\n      })\n    };\n    \n    (vscode.workspace.getConfiguration as jest.Mock).mockReturnValue(mockConfig);\n\n    const service = await initializeLLMService();\n    \n    expect(service).toBeDefined();\n    expect(mockConfig.get).toHaveBeenCalledWith('openaiApiKey');\n  });\n\n  test('initializes service without API key', async () => {\n    const mockConfig = {\n      get: jest.fn(() => null)\n    };\n    \n    (vscode.workspace.getConfiguration as jest.Mock).mockReturnValue(mockConfig);\n\n    const service = await initializeLLMService();\n    \n    expect(service).toBeDefined();\n  });\n});",
            "run_instructions": "npm test -- llmIntegration.test.ts -t 'initializes service with saved API key'"
          },
          {
            "id": "test-save-incremental-results",
            "name": "test_saveIncrementalFileSummary_persists_data",
            "description": "Verifies that incremental analysis results are saved correctly during long-running operations",
            "target_function": "saveIncrementalFileSummary",
            "target_file": "src/llmIntegration.ts",
            "scenarios": [
              "Save first summary",
              "Append to existing summaries",
              "Handle save errors"
            ],
            "mocks": [
              "File system write operations"
            ],
            "assertions": [
              "Data is written to correct file",
              "JSON format is valid",
              "Existing data is preserved"
            ],
            "priority": "medium",
            "test_code": "import * as fs from 'fs';\nimport { saveIncrementalFileSummary } from '../llmIntegration';\n\njest.mock('fs');\n\ndescribe('saveIncrementalFileSummary', () => {\n  beforeEach(() => {\n    jest.clearAllMocks();\n  });\n\n  test('saves summary to incremental file', () => {\n    const mockSummary = {\n      file: '/test/file.ts',\n      purpose: 'Test file',\n      key_functions: []\n    };\n    \n    (fs.existsSync as jest.Mock).mockReturnValue(false);\n    (fs.writeFileSync as jest.Mock).mockImplementation(() => {});\n\n    saveIncrementalFileSummary(mockSummary, '/output');\n    \n    expect(fs.writeFileSync).toHaveBeenCalled();\n    const writeCall = (fs.writeFileSync as jest.Mock).mock.calls[0];\n    expect(writeCall[0]).toContain('file_summaries');\n  });\n\n  test('appends to existing summaries file', () => {\n    const existingSummaries = [{ file: '/existing.ts', purpose: 'Existing' }];\n    const newSummary = { file: '/new.ts', purpose: 'New' };\n    \n    (fs.existsSync as jest.Mock).mockReturnValue(true);\n    (fs.readFileSync as jest.Mock).mockReturnValue(JSON.stringify(existingSummaries));\n    (fs.writeFileSync as jest.Mock).mockImplementation(() => {});\n\n    saveIncrementalFileSummary(newSummary, '/output');\n    \n    const writeCall = (fs.writeFileSync as jest.Mock).mock.calls[0];\n    const writtenData = JSON.parse(writeCall[1]);\n    expect(writtenData).toHaveLength(2);\n  });\n});",
            "run_instructions": "npm test -- llmIntegration.test.ts -t 'saves summary to incremental file'"
          },
          {
            "id": "test-format-enhanced-docs",
            "name": "test_formatEnhancedDocsAsMarkdown_creates_readable_output",
            "description": "Verifies that product documentation is formatted as readable markdown",
            "target_function": "formatEnhancedDocsAsMarkdown",
            "target_file": "src/llmIntegration.ts",
            "scenarios": [
              "Format complete documentation",
              "Handle missing sections",
              "Format with special characters"
            ],
            "mocks": [
              "None - pure formatting"
            ],
            "assertions": [
              "Valid markdown is generated",
              "All sections are included",
              "Special characters are escaped"
            ],
            "priority": "medium",
            "test_code": "import { formatEnhancedDocsAsMarkdown } from '../llmIntegration';\n\ndescribe('formatEnhancedDocsAsMarkdown', () => {\n  test('formats complete documentation as markdown', () => {\n    const docs = {\n      product_name: 'Test App',\n      product_purpose: 'Testing application',\n      key_features: ['Feature 1', 'Feature 2'],\n      user_workflows: []\n    };\n    \n    const markdown = formatEnhancedDocsAsMarkdown(docs);\n    \n    expect(markdown).toContain('# Test App');\n    expect(markdown).toContain('Testing application');\n    expect(markdown).toContain('Feature 1');\n    expect(markdown).toContain('Feature 2');\n  });\n\n  test('handles missing optional sections', () => {\n    const docs = {\n      product_name: 'Minimal App',\n      product_purpose: 'Minimal testing',\n      key_features: []\n    };\n    \n    const markdown = formatEnhancedDocsAsMarkdown(docs);\n    \n    expect(markdown).toContain('Minimal App');\n    expect(markdown).not.toContain('undefined');\n  });\n\n  test('escapes special markdown characters', () => {\n    const docs = {\n      product_name: 'App with *special* chars',\n      product_purpose: 'Testing [links]',\n      key_features: ['Feature with `code`']\n    };\n    \n    const markdown = formatEnhancedDocsAsMarkdown(docs);\n    \n    expect(markdown).toBeDefined();\n    expect(markdown.length).toBeGreaterThan(0);\n  });\n});",
            "run_instructions": "npm test -- llmIntegration.test.ts -t 'formats complete documentation as markdown'"
          }
        ]
      },
      {
        "id": "extension-tests",
        "name": "Extension Entry Point Tests",
        "description": "Tests for main extension activation, command registration, and lifecycle management",
        "test_file_path": "src/test/extension.test.ts",
        "source_files": [
          "src/extension.ts"
        ],
        "test_cases": [
          {
            "id": "test-activate-extension",
            "name": "test_activate_registers_commands",
            "description": "Verifies that extension activation registers all required VS Code commands",
            "target_function": "activate",
            "target_file": "src/extension.ts",
            "scenarios": [
              "Activation registers analyze workspace command",
              "All commands are registered",
              "Tree views are initialized"
            ],
            "mocks": [
              "VS Code extension context",
              "Command registration"
            ],
            "assertions": [
              "All commands are registered",
              "Tree providers are set up",
              "Status bar items are created"
            ],
            "priority": "high",
            "test_code": "import * as vscode from 'vscode';\nimport { activate } from '../extension';\n\njest.mock('vscode');\n\ndescribe('Extension activation', () => {\n  let mockContext: vscode.ExtensionContext;\n\n  beforeEach(() => {\n    mockContext = {\n      subscriptions: [],\n      extensionPath: '/mock/path',\n      globalState: { get: jest.fn(), update: jest.fn() },\n      workspaceState: { get: jest.fn(), update: jest.fn() }\n    } as any;\n    \n    (vscode.commands.registerCommand as jest.Mock) = jest.fn((cmd, handler) => {\n      return { dispose: jest.fn() };\n    });\n  });\n\n  test('registers analyze workspace command', async () => {\n    await activate(mockContext);\n    \n    expect(vscode.commands.registerCommand).toHaveBeenCalledWith(\n      'shadowwatch.analyzeWorkspace',\n      expect.any(Function)\n    );\n  });\n\n  test('registers all required commands', async () => {\n    await activate(mockContext);\n    \n    const registeredCommands = (vscode.commands.registerCommand as jest.Mock).mock.calls.map(call => call[0]);\n    \n    expect(registeredCommands).toContain('shadowwatch.analyzeWorkspace');\n    expect(registeredCommands).toContain('shadowwatch.generateProductDocs');\n    expect(registeredCommands).toContain('shadowwatch.generateLLMInsights');\n  });\n\n  test('initializes tree view providers', async () => {\n    (vscode.window.createTreeView as jest.Mock) = jest.fn(() => ({\n      dispose: jest.fn()\n    }));\n\n    await activate(mockContext);\n    \n    expect(vscode.window.createTreeView).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- extension.test.ts -t 'registers analyze workspace command'"
          },
          {
            "id": "test-analyze-workspace-command",
            "name": "test_analyzeWorkspace_triggers_full_scan",
            "description": "Verifies that analyze workspace command triggers comprehensive codebase analysis",
            "target_function": "analyzeWorkspace",
            "target_file": "src/extension.ts",
            "scenarios": [
              "Analyze complete workspace",
              "Handle no workspace open",
              "Show progress indicator"
            ],
            "mocks": [
              "VS Code workspace API",
              "CodeAnalyzer"
            ],
            "assertions": [
              "Analyzer is invoked",
              "Progress is shown",
              "Results are displayed"
            ],
            "priority": "high",
            "test_code": "import * as vscode from 'vscode';\nimport { analyzeWorkspace } from '../extension';\nimport { CodeAnalyzer } from '../analyzer';\n\njest.mock('vscode');\njest.mock('../analyzer');\n\ndescribe('analyzeWorkspace command', () => {\n  beforeEach(() => {\n    jest.clearAllMocks();\n  });\n\n  test('triggers full workspace analysis', async () => {\n    const mockWorkspace = { uri: { fsPath: '/mock/workspace' } };\n    (vscode.workspace.workspaceFolders as any) = [mockWorkspace];\n    \n    const mockAnalyzer = {\n      analyzeWorkspace: jest.fn().mockResolvedValue({\n        healthScore: 85,\n        issues: []\n      })\n    };\n    (CodeAnalyzer as jest.Mock).mockImplementation(() => mockAnalyzer);\n\n    await analyzeWorkspace();\n    \n    expect(mockAnalyzer.analyzeWorkspace).toHaveBeenCalled();\n  });\n\n  test('shows error when no workspace is open', async () => {\n    (vscode.workspace.workspaceFolders as any) = undefined;\n    (vscode.window.showErrorMessage as jest.Mock) = jest.fn();\n\n    await analyzeWorkspace();\n    \n    expect(vscode.window.showErrorMessage).toHaveBeenCalledWith(\n      expect.stringContaining('workspace')\n    );\n  });\n\n  test('shows progress indicator during analysis', async () => {\n    const mockWorkspace = { uri: { fsPath: '/mock/workspace' } };\n    (vscode.workspace.workspaceFolders as any) = [mockWorkspace];\n    \n    const mockProgress = { report: jest.fn() };\n    (vscode.window.withProgress as jest.Mock) = jest.fn((options, task) => {\n      return task(mockProgress);\n    });\n    \n    const mockAnalyzer = {\n      analyzeWorkspace: jest.fn().mockResolvedValue({ healthScore: 85, issues: [] })\n    };\n    (CodeAnalyzer as jest.Mock).mockImplementation(() => mockAnalyzer);\n\n    await analyzeWorkspace();\n    \n    expect(vscode.window.withProgress).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- extension.test.ts -t 'triggers full workspace analysis'"
          },
          {
            "id": "test-copy-insights-command",
            "name": "test_copyAllInsights_formats_for_llm",
            "description": "Verifies that copy insights command formats issues for AI assistants correctly",
            "target_function": "copyAllInsights",
            "target_file": "src/extension.ts",
            "scenarios": [
              "Copy all insights",
              "Format for Cursor",
              "Format for ChatGPT"
            ],
            "mocks": [
              "Clipboard API",
              "Analysis results"
            ],
            "assertions": [
              "Content is copied to clipboard",
              "Format is correct",
              "LLM-specific formatting applied"
            ],
            "priority": "medium",
            "test_code": "import * as vscode from 'vscode';\nimport { copyAllInsights } from '../extension';\n\njest.mock('vscode');\n\ndescribe('copyAllInsights command', () => {\n  beforeEach(() => {\n    jest.clearAllMocks();\n    (vscode.env.clipboard.writeText as jest.Mock) = jest.fn();\n  });\n\n  test('copies all insights to clipboard', async () => {\n    const mockInsights = [\n      { severity: 'error', description: 'Test issue', file: '/test.ts' }\n    ];\n    \n    await copyAllInsights(mockInsights);\n    \n    expect(vscode.env.clipboard.writeText).toHaveBeenCalled();\n  });\n\n  test('formats insights for Cursor AI', async () => {\n    const mockInsights = [\n      { severity: 'error', description: 'Test issue', file: '/test.ts', line: 10 }\n    ];\n    \n    await copyAllInsights(mockInsights, 'cursor');\n    \n    const copiedText = (vscode.env.clipboard.writeText as jest.Mock).mock.calls[0][0];\n    expect(copiedText).toContain('Test issue');\n    expect(copiedText).toContain('/test.ts');\n  });\n\n  test('shows success message after copying', async () => {\n    (vscode.window.showInformationMessage as jest.Mock) = jest.fn();\n    const mockInsights = [{ severity: 'info', description: 'Info' }];\n    \n    await copyAllInsights(mockInsights);\n    \n    expect(vscode.window.showInformationMessage).toHaveBeenCalledWith(\n      expect.stringContaining('copied')\n    );\n  });\n});",
            "run_instructions": "npm test -- extension.test.ts -t 'copies all insights to clipboard'"
          }
        ]
      },
      {
        "id": "insightstreeview-tests",
        "name": "Insights Tree View Tests",
        "description": "Tests for hierarchical insights navigation tree view component",
        "test_file_path": "src/test/insightsTreeView.test.ts",
        "source_files": [
          "src/insightsTreeView.ts"
        ],
        "test_cases": [
          {
            "id": "test-tree-refresh",
            "name": "test_refresh_updates_tree_view",
            "description": "Verifies that tree view refresh correctly updates displayed insights",
            "target_function": "refresh",
            "target_file": "src/insightsTreeView.ts",
            "scenarios": [
              "Refresh with new insights",
              "Refresh with empty insights",
              "Preserve expansion state"
            ],
            "mocks": [
              "VS Code tree view API"
            ],
            "assertions": [
              "Tree is updated",
              "Change event is fired",
              "Children are re-rendered"
            ],
            "priority": "high",
            "test_code": "import { InsightsTreeProvider } from '../insightsTreeView';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('InsightsTreeProvider.refresh', () => {\n  let provider: InsightsTreeProvider;\n\n  beforeEach(() => {\n    provider = new InsightsTreeProvider();\n  });\n\n  test('fires change event on refresh', () => {\n    const fireStub = jest.fn();\n    (provider as any)._onDidChangeTreeData = { fire: fireStub };\n    \n    provider.refresh();\n    \n    expect(fireStub).toHaveBeenCalled();\n  });\n\n  test('updates insights data', () => {\n    const newInsights = [{ severity: 'error', description: 'New issue' }];\n    \n    provider.updateInsights(newInsights);\n    provider.refresh();\n    \n    expect((provider as any).insights).toEqual(newInsights);\n  });\n});",
            "run_instructions": "npm test -- insightsTreeView.test.ts -t 'fires change event on refresh'"
          },
          {
            "id": "test-tree-children",
            "name": "test_getChildren_returns_hierarchical_structure",
            "description": "Verifies that getChildren returns properly structured tree nodes",
            "target_function": "getChildren",
            "target_file": "src/insightsTreeView.ts",
            "scenarios": [
              "Get root level children",
              "Get nested children",
              "Empty insights"
            ],
            "mocks": [
              "None - data transformation"
            ],
            "assertions": [
              "Correct hierarchy is returned",
              "Child counts are accurate",
              "Severity grouping works"
            ],
            "priority": "high",
            "test_code": "import { InsightsTreeProvider } from '../insightsTreeView';\n\ndescribe('InsightsTreeProvider.getChildren', () => {\n  let provider: InsightsTreeProvider;\n\n  beforeEach(() => {\n    provider = new InsightsTreeProvider();\n  });\n\n  test('returns root level categories', async () => {\n    const insights = [\n      { severity: 'error', category: 'complexity', description: 'Issue 1' },\n      { severity: 'warning', category: 'style', description: 'Issue 2' }\n    ];\n    provider.updateInsights(insights);\n    \n    const children = await provider.getChildren();\n    \n    expect(children.length).toBeGreaterThan(0);\n  });\n\n  test('returns children for category node', async () => {\n    const insights = [\n      { severity: 'error', category: 'complexity', description: 'Issue 1', file: '/test.ts' },\n      { severity: 'error', category: 'complexity', description: 'Issue 2', file: '/test.ts' }\n    ];\n    provider.updateInsights(insights);\n    \n    const rootChildren = await provider.getChildren();\n    const categoryNode = rootChildren[0];\n    const categoryChildren = await provider.getChildren(categoryNode);\n    \n    expect(categoryChildren.length).toBe(2);\n  });\n\n  test('returns empty array for leaf nodes', async () => {\n    const leafNode: any = { type: 'insight', children: [] };\n    \n    const children = await provider.getChildren(leafNode);\n    \n    expect(children).toEqual([]);\n  });\n});",
            "run_instructions": "npm test -- insightsTreeView.test.ts -t 'returns root level categories'"
          },
          {
            "id": "test-tree-item-icons",
            "name": "test_getTreeItem_sets_correct_icons",
            "description": "Verifies that tree items have correct icons based on severity",
            "target_function": "getTreeItem",
            "target_file": "src/insightsTreeView.ts",
            "scenarios": [
              "Error icon for errors",
              "Warning icon for warnings",
              "Info icon for info"
            ],
            "mocks": [
              "VS Code ThemeIcon"
            ],
            "assertions": [
              "Correct icon is set",
              "Icon color matches severity",
              "Tooltip is present"
            ],
            "priority": "low",
            "test_code": "import { InsightsTreeProvider } from '../insightsTreeView';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('InsightsTreeProvider.getTreeItem', () => {\n  let provider: InsightsTreeProvider;\n\n  beforeEach(() => {\n    provider = new InsightsTreeProvider();\n  });\n\n  test('sets error icon for error severity', () => {\n    const element: any = {\n      label: 'Error Issue',\n      severity: 'error',\n      type: 'insight'\n    };\n    \n    const treeItem = provider.getTreeItem(element);\n    \n    expect(treeItem.iconPath).toBeDefined();\n  });\n\n  test('sets warning icon for warning severity', () => {\n    const element: any = {\n      label: 'Warning Issue',\n      severity: 'warning',\n      type: 'insight'\n    };\n    \n    const treeItem = provider.getTreeItem(element);\n    \n    expect(treeItem.iconPath).toBeDefined();\n  });\n\n  test('includes tooltip with description', () => {\n    const element: any = {\n      label: 'Test',\n      description: 'Test description',\n      type: 'insight'\n    };\n    \n    const treeItem = provider.getTreeItem(element);\n    \n    expect(treeItem.tooltip).toContain('Test description');\n  });\n});",
            "run_instructions": "npm test -- insightsTreeView.test.ts -t 'sets error icon for error severity'"
          }
        ]
      },
      {
        "id": "productnavigator-tests",
        "name": "Product Navigator Tests",
        "description": "Tests for product documentation navigation tree view",
        "test_file_path": "src/test/productNavigator.test.ts",
        "source_files": [
          "src/productNavigator.ts"
        ],
        "test_cases": [
          {
            "id": "test-load-product-docs",
            "name": "test_loadProductDocs_parses_documentation",
            "description": "Verifies that product documentation is loaded and parsed correctly",
            "target_function": "loadProductDocs",
            "target_file": "src/productNavigator.ts",
            "scenarios": [
              "Load valid documentation",
              "Handle missing file",
              "Parse JSON structure"
            ],
            "mocks": [
              "File system read operations"
            ],
            "assertions": [
              "Documentation is loaded",
              "Structure is parsed",
              "Navigation tree is built"
            ],
            "priority": "high",
            "test_code": "import { ProductNavigator } from '../productNavigator';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('ProductNavigator.loadProductDocs', () => {\n  let navigator: ProductNavigator;\n\n  beforeEach(() => {\n    navigator = new ProductNavigator();\n    jest.clearAllMocks();\n  });\n\n  test('loads and parses valid documentation', () => {\n    const mockDocs = {\n      product_name: 'Test App',\n      key_features: ['Feature 1'],\n      modules: []\n    };\n    \n    (fs.existsSync as jest.Mock).mockReturnValue(true);\n    (fs.readFileSync as jest.Mock).mockReturnValue(JSON.stringify(mockDocs));\n\n    navigator.loadProductDocs('/path/to/docs.json');\n    \n    expect((navigator as any).productDocs).toBeDefined();\n    expect((navigator as any).productDocs.product_name).toBe('Test App');\n  });\n\n  test('handles missing documentation file', () => {\n    (fs.existsSync as jest.Mock).mockReturnValue(false);\n\n    expect(() => {\n      navigator.loadProductDocs('/nonexistent/docs.json');\n    }).toThrow();\n  });\n\n  test('handles invalid JSON', () => {\n    (fs.existsSync as jest.Mock).mockReturnValue(true);\n    (fs.readFileSync as jest.Mock).mockReturnValue('invalid json');\n\n    expect(() => {\n      navigator.loadProductDocs('/path/to/bad.json');\n    }).toThrow();\n  });\n});",
            "run_instructions": "npm test -- productNavigator.test.ts -t 'loads and parses valid documentation'"
          },
          {
            "id": "test-navigate-to-section",
            "name": "test_navigateToSection_opens_file",
            "description": "Verifies that clicking navigation items opens the correct files",
            "target_function": "navigateToSection",
            "target_file": "src/productNavigator.ts",
            "scenarios": [
              "Navigate to feature",
              "Navigate to module",
              "Handle invalid section"
            ],
            "mocks": [
              "VS Code editor API"
            ],
            "assertions": [
              "File is opened",
              "Cursor is positioned",
              "Editor is focused"
            ],
            "priority": "medium",
            "test_code": "import { ProductNavigator } from '../productNavigator';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('ProductNavigator.navigateToSection', () => {\n  let navigator: ProductNavigator;\n\n  beforeEach(() => {\n    navigator = new ProductNavigator();\n    (vscode.window.showTextDocument as jest.Mock) = jest.fn();\n  });\n\n  test('opens file for valid section', async () => {\n    const section = {\n      type: 'feature',\n      file: '/path/to/feature.ts',\n      line: 10\n    };\n    \n    await navigator.navigateToSection(section);\n    \n    expect(vscode.window.showTextDocument).toHaveBeenCalled();\n  });\n\n  test('positions cursor at correct line', async () => {\n    const mockEditor = {\n      selection: null,\n      revealRange: jest.fn()\n    };\n    (vscode.window.showTextDocument as jest.Mock).mockResolvedValue(mockEditor);\n    \n    const section = { file: '/test.ts', line: 25 };\n    await navigator.navigateToSection(section);\n    \n    expect(mockEditor.revealRange).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- productNavigator.test.ts -t 'opens file for valid section'"
          }
        ]
      },
      {
        "id": "insightgenerator-tests",
        "name": "Insight Generator Tests",
        "description": "Tests for generating architectural insights from static analysis",
        "test_file_path": "src/test/insightGenerator.test.ts",
        "source_files": [
          "src/insightGenerator.ts"
        ],
        "test_cases": [
          {
            "id": "test-generate-insights",
            "name": "test_generateInsights_creates_recommendations",
            "description": "Verifies that insights are generated from analysis results",
            "target_function": "generateInsights",
            "target_file": "src/insightGenerator.ts",
            "scenarios": [
              "Generate from issues",
              "Prioritize by severity",
              "Group by category"
            ],
            "mocks": [
              "Analysis results"
            ],
            "assertions": [
              "Insights are created",
              "Priority is assigned",
              "Categories are correct"
            ],
            "priority": "high",
            "test_code": "import { generateInsights } from '../insightGenerator';\n\ndescribe('generateInsights', () => {\n  test('creates insights from analysis issues', () => {\n    const analysisResult = {\n      issues: [\n        { severity: 'error', category: 'complexity', description: 'High complexity', file: '/test.ts' },\n        { severity: 'warning', category: 'style', description: 'Style issue', file: '/style.ts' }\n      ],\n      healthScore: 75\n    };\n    \n    const insights = generateInsights(analysisResult);\n    \n    expect(insights.length).toBeGreaterThan(0);\n    expect(insights.some(i => i.category === 'complexity')).toBe(true);\n  });\n\n  test('prioritizes errors over warnings', () => {\n    const analysisResult = {\n      issues: [\n        { severity: 'error', category: 'complexity', description: 'Error', file: '/e.ts' },\n        { severity: 'warning', category: 'style', description: 'Warning', file: '/w.ts' }\n      ],\n      healthScore: 80\n    };\n    \n    const insights = generateInsights(analysisResult);\n    const priorities = insights.map(i => i.priority);\n    \n    expect(priorities[0]).toBe('high');\n  });\n\n  test('groups insights by category', () => {\n    const analysisResult = {\n      issues: [\n        { severity: 'error', category: 'complexity', description: 'C1', file: '/c1.ts' },\n        { severity: 'error', category: 'complexity', description: 'C2', file: '/c2.ts' },\n        { severity: 'warning', category: 'style', description: 'S1', file: '/s1.ts' }\n      ],\n      healthScore: 70\n    };\n    \n    const insights = generateInsights(analysisResult);\n    const complexityInsights = insights.filter(i => i.category === 'complexity');\n    \n    expect(complexityInsights.length).toBe(2);\n  });\n});",
            "run_instructions": "npm test -- insightGenerator.test.ts -t 'creates insights from analysis issues'"
          },
          {
            "id": "test-detect-patterns",
            "name": "test_detectArchitecturePatterns_identifies_patterns",
            "description": "Verifies that common architecture patterns are detected",
            "target_function": "detectArchitecturePatterns",
            "target_file": "src/insightGenerator.ts",
            "scenarios": [
              "Detect MVC pattern",
              "Detect singleton pattern",
              "Detect factory pattern"
            ],
            "mocks": [
              "Code structure data"
            ],
            "assertions": [
              "Patterns are identified",
              "Pattern metadata included",
              "Confidence scores assigned"
            ],
            "priority": "medium",
            "test_code": "import { detectArchitecturePatterns } from '../insightGenerator';\n\ndescribe('detectArchitecturePatterns', () => {\n  test('identifies MVC pattern in codebase', () => {\n    const codeStructure = {\n      files: [\n        { path: '/controllers/UserController.ts', type: 'controller' },\n        { path: '/models/User.ts', type: 'model' },\n        { path: '/views/UserView.ts', type: 'view' }\n      ]\n    };\n    \n    const patterns = detectArchitecturePatterns(codeStructure);\n    \n    expect(patterns).toContain('MVC');\n  });\n\n  test('identifies singleton pattern', () => {\n    const codeStructure = {\n      files: [\n        { path: '/services/DatabaseService.ts', content: 'private static instance' }\n      ]\n    };\n    \n    const patterns = detectArchitecturePatterns(codeStructure);\n    \n    expect(patterns.some(p => p.includes('Singleton'))).toBe(true);\n  });\n});",
            "run_instructions": "npm test -- insightGenerator.test.ts -t 'identifies MVC pattern in codebase'"
          }
        ]
      },
      {
        "id": "llmformatter-tests",
        "name": "LLM Formatter Tests",
        "description": "Tests for formatting analysis results for different AI assistants",
        "test_file_path": "src/test/llmFormatter.test.ts",
        "source_files": [
          "src/llmFormatter.ts"
        ],
        "test_cases": [
          {
            "id": "test-format-for-cursor",
            "name": "test_formatForCursor_creates_cursor_prompt",
            "description": "Verifies that Cursor AI prompt format is generated correctly",
            "target_function": "formatForCursor",
            "target_file": "src/llmFormatter.ts",
            "scenarios": [
              "Format single issue",
              "Format multiple issues",
              "Include context"
            ],
            "mocks": [
              "None - pure formatting"
            ],
            "assertions": [
              "Cursor format is correct",
              "All issues included",
              "Context is present"
            ],
            "priority": "high",
            "test_code": "import { formatForCursor } from '../llmFormatter';\n\ndescribe('formatForCursor', () => {\n  test('creates Cursor-formatted prompt for single issue', () => {\n    const issues = [\n      { severity: 'error', description: 'Test issue', file: '/test.ts', line: 10, suggestedFix: 'Fix it' }\n    ];\n    \n    const prompt = formatForCursor(issues);\n    \n    expect(prompt).toContain('Test issue');\n    expect(prompt).toContain('/test.ts');\n    expect(prompt).toContain('10');\n    expect(prompt).toContain('Fix it');\n  });\n\n  test('formats multiple issues with clear separation', () => {\n    const issues = [\n      { severity: 'error', description: 'Issue 1', file: '/file1.ts', line: 5 },\n      { severity: 'warning', description: 'Issue 2', file: '/file2.ts', line: 15 }\n    ];\n    \n    const prompt = formatForCursor(issues);\n    \n    expect(prompt).toContain('Issue 1');\n    expect(prompt).toContain('Issue 2');\n    expect(prompt.split('---').length).toBeGreaterThan(1);\n  });\n\n  test('includes file context and suggestions', () => {\n    const issues = [\n      { severity: 'error', description: 'Complex function', file: '/complex.ts', line: 20, suggestedFix: 'Refactor into smaller functions' }\n    ];\n    \n    const prompt = formatForCursor(issues);\n    \n    expect(prompt).toContain('complex.ts');\n    expect(prompt).toContain('Refactor into smaller functions');\n  });\n});",
            "run_instructions": "npm test -- llmFormatter.test.ts -t 'creates Cursor-formatted prompt for single issue'"
          },
          {
            "id": "test-format-for-chatgpt",
            "name": "test_formatForChatGPT_creates_chatgpt_prompt",
            "description": "Verifies that ChatGPT prompt format is generated correctly",
            "target_function": "formatForChatGPT",
            "target_file": "src/llmFormatter.ts",
            "scenarios": [
              "Verbose format",
              "Include examples",
              "Add instructions"
            ],
            "mocks": [
              "None - pure formatting"
            ],
            "assertions": [
              "ChatGPT format is correct",
              "Verbose explanations included",
              "Instructions are clear"
            ],
            "priority": "high",
            "test_code": "import { formatForChatGPT } from '../llmFormatter';\n\ndescribe('formatForChatGPT', () => {\n  test('creates verbose ChatGPT-formatted prompt', () => {\n    const issues = [\n      { severity: 'error', description: 'God object detected', file: '/god.ts', line: 1, suggestedFix: 'Split into modules' }\n    ];\n    \n    const prompt = formatForChatGPT(issues);\n    \n    expect(prompt.length).toBeGreaterThan(100);\n    expect(prompt).toContain('God object detected');\n    expect(prompt).toContain('Split into modules');\n  });\n\n  test('includes detailed context and explanations', () => {\n    const issues = [\n      { severity: 'warning', description: 'Circular dependency', file: '/a.ts', line: 5 }\n    ];\n    \n    const prompt = formatForChatGPT(issues);\n    \n    expect(prompt).toContain('context');\n    expect(prompt).toContain('Circular dependency');\n  });\n\n  test('formats with proper sections and headers', () => {\n    const issues = [\n      { severity: 'error', description: 'Issue 1', file: '/f1.ts' },\n      { severity: 'warning', description: 'Issue 2', file: '/f2.ts' }\n    ];\n    \n    const prompt = formatForChatGPT(issues);\n    \n    expect(prompt).toContain('##');\n    expect(prompt.split('\\n\\n').length).toBeGreaterThan(2);\n  });\n});",
            "run_instructions": "npm test -- llmFormatter.test.ts -t 'creates verbose ChatGPT-formatted prompt'"
          },
          {
            "id": "test-format-generic",
            "name": "test_formatGeneric_creates_standard_markdown",
            "description": "Verifies that generic markdown format is generated for any AI assistant",
            "target_function": "formatGeneric",
            "target_file": "src/llmFormatter.ts",
            "scenarios": [
              "Standard markdown",
              "Simple format",
              "Universal compatibility"
            ],
            "mocks": [
              "None - pure formatting"
            ],
            "assertions": [
              "Valid markdown produced",
              "Standard structure",
              "Compatible with any LLM"
            ],
            "priority": "medium",
            "test_code": "import { formatGeneric } from '../llmFormatter';\n\ndescribe('formatGeneric', () => {\n  test('creates standard markdown format', () => {\n    const issues = [\n      { severity: 'error', description: 'Test issue', file: '/test.ts', line: 10 }\n    ];\n    \n    const markdown = formatGeneric(issues);\n    \n    expect(markdown).toContain('#');\n    expect(markdown).toContain('Test issue');\n    expect(markdown).toContain('/test.ts');\n  });\n\n  test('produces valid markdown structure', () => {\n    const issues = [\n      { severity: 'error', description: 'E1', file: '/e.ts' },\n      { severity: 'warning', description: 'W1', file: '/w.ts' }\n    ];\n    \n    const markdown = formatGeneric(issues);\n    \n    expect(markdown).toMatch(/^#/);\n    expect(markdown).toContain('\\n');\n  });\n\n  test('is compatible with any AI assistant', () => {\n    const issues = [\n      { severity: 'info', description: 'Simple issue', file: '/simple.ts', line: 1 }\n    ];\n    \n    const markdown = formatGeneric(issues);\n    \n    expect(markdown).not.toContain('Cursor');\n    expect(markdown).not.toContain('ChatGPT');\n    expect(markdown.length).toBeGreaterThan(0);\n  });\n});",
            "run_instructions": "npm test -- llmFormatter.test.ts -t 'creates standard markdown format'"
          }
        ]
      },
      {
        "id": "cache-tests",
        "name": "Cache Tests",
        "description": "Tests for caching mechanism that stores and retrieves analysis results",
        "test_file_path": "src/test/cache.test.ts",
        "source_files": [
          "src/cache.ts"
        ],
        "test_cases": [
          {
            "id": "test-cache-get-set",
            "name": "test_cache_stores_and_retrieves_data",
            "description": "Verifies that cache correctly stores and retrieves analysis results",
            "target_function": "set, get",
            "target_file": "src/cache.ts",
            "scenarios": [
              "Store and retrieve result",
              "Cache miss returns undefined",
              "Expired entries are removed"
            ],
            "mocks": [
              "File system for persistence"
            ],
            "assertions": [
              "Stored data is retrieved",
              "Cache hits work",
              "Misses return undefined"
            ],
            "priority": "high",
            "test_code": "import { Cache } from '../cache';\n\ndescribe('Cache operations', () => {\n  let cache: Cache;\n\n  beforeEach(() => {\n    cache = new Cache('/mock/cache');\n  });\n\n  test('stores and retrieves data correctly', () => {\n    const key = 'test-file.ts';\n    const data = { healthScore: 85, issues: [] };\n    \n    cache.set(key, data);\n    const retrieved = cache.get(key);\n    \n    expect(retrieved).toEqual(data);\n  });\n\n  test('returns undefined for cache miss', () => {\n    const result = cache.get('nonexistent-key');\n    \n    expect(result).toBeUndefined();\n  });\n\n  test('overwrites existing key', () => {\n    const key = 'file.ts';\n    cache.set(key, { healthScore: 50 });\n    cache.set(key, { healthScore: 90 });\n    \n    const result = cache.get(key);\n    \n    expect(result.healthScore).toBe(90);\n  });\n});",
            "run_instructions": "npm test -- cache.test.ts -t 'stores and retrieves data correctly'"
          },
          {
            "id": "test-cache-invalidation",
            "name": "test_cache_invalidates_stale_entries",
            "description": "Verifies that cache correctly invalidates entries based on file changes",
            "target_function": "invalidate",
            "target_file": "src/cache.ts",
            "scenarios": [
              "Invalidate specific file",
              "Invalidate all entries",
              "Check file hash changes"
            ],
            "mocks": [
              "File system to check modification times"
            ],
            "assertions": [
              "Stale entries removed",
              "Valid entries preserved",
              "Hash comparison works"
            ],
            "priority": "high",
            "test_code": "import { Cache } from '../cache';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('Cache invalidation', () => {\n  let cache: Cache;\n\n  beforeEach(() => {\n    cache = new Cache('/mock/cache');\n    jest.clearAllMocks();\n  });\n\n  test('invalidates specific file entry', () => {\n    cache.set('file1.ts', { data: 'test1' });\n    cache.set('file2.ts', { data: 'test2' });\n    \n    cache.invalidate('file1.ts');\n    \n    expect(cache.get('file1.ts')).toBeUndefined();\n    expect(cache.get('file2.ts')).toBeDefined();\n  });\n\n  test('clears all cache entries', () => {\n    cache.set('file1.ts', { data: 'test1' });\n    cache.set('file2.ts', { data: 'test2' });\n    \n    cache.clear();\n    \n    expect(cache.get('file1.ts')).toBeUndefined();\n    expect(cache.get('file2.ts')).toBeUndefined();\n  });\n\n  test('detects file changes via hash', () => {\n    const filePath = '/test/file.ts';\n    (fs.readFileSync as jest.Mock).mockReturnValueOnce('content v1');\n    \n    cache.set(filePath, { data: 'cached' });\n    \n    (fs.readFileSync as jest.Mock).mockReturnValueOnce('content v2');\n    const isStale = cache.isStale(filePath);\n    \n    expect(isStale).toBe(true);\n  });\n});",
            "run_instructions": "npm test -- cache.test.ts -t 'invalidates specific file entry'"
          }
        ]
      },
      {
        "id": "filewatcher-tests",
        "name": "File Watcher Tests",
        "description": "Tests for file system watching and triggering incremental analysis",
        "test_file_path": "src/test/fileWatcher.test.ts",
        "source_files": [
          "src/fileWatcher.ts"
        ],
        "test_cases": [
          {
            "id": "test-watch-start",
            "name": "test_startWatching_monitors_files",
            "description": "Verifies that file watcher starts monitoring workspace files",
            "target_function": "startWatching",
            "target_file": "src/fileWatcher.ts",
            "scenarios": [
              "Start watching workspace",
              "Watch specific patterns",
              "Ignore node_modules"
            ],
            "mocks": [
              "VS Code file system watcher"
            ],
            "assertions": [
              "Watcher is created",
              "Patterns are applied",
              "Ignored paths excluded"
            ],
            "priority": "high",
            "test_code": "import { FileWatcher } from '../fileWatcher';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('FileWatcher.startWatching', () => {\n  let watcher: FileWatcher;\n\n  beforeEach(() => {\n    (vscode.workspace.createFileSystemWatcher as jest.Mock) = jest.fn(() => ({\n      onDidChange: jest.fn(),\n      onDidCreate: jest.fn(),\n      onDidDelete: jest.fn(),\n      dispose: jest.fn()\n    }));\n    watcher = new FileWatcher();\n  });\n\n  test('creates file system watcher for workspace', () => {\n    watcher.startWatching('/workspace');\n    \n    expect(vscode.workspace.createFileSystemWatcher).toHaveBeenCalled();\n  });\n\n  test('watches specific file patterns', () => {\n    watcher.startWatching('/workspace', ['**/*.ts', '**/*.js']);\n    \n    const callArgs = (vscode.workspace.createFileSystemWatcher as jest.Mock).mock.calls[0][0];\n    expect(callArgs).toContain('*.ts');\n  });\n\n  test('registers change event handler', () => {\n    const mockWatcher = {\n      onDidChange: jest.fn(),\n      onDidCreate: jest.fn(),\n      onDidDelete: jest.fn()\n    };\n    (vscode.workspace.createFileSystemWatcher as jest.Mock).mockReturnValue(mockWatcher);\n    \n    watcher.startWatching('/workspace');\n    \n    expect(mockWatcher.onDidChange).toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- fileWatcher.test.ts -t 'creates file system watcher for workspace'"
          },
          {
            "id": "test-file-change-trigger",
            "name": "test_onFileChange_triggers_analysis",
            "description": "Verifies that file changes trigger incremental analysis",
            "target_function": "onFileChange",
            "target_file": "src/fileWatcher.ts",
            "scenarios": [
              "File save triggers analysis",
              "Debounce rapid changes",
              "Skip ignored files"
            ],
            "mocks": [
              "Analyzer",
              "Event emitter"
            ],
            "assertions": [
              "Analysis is triggered",
              "Debouncing works",
              "Ignored files skipped"
            ],
            "priority": "high",
            "test_code": "import { FileWatcher } from '../fileWatcher';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('FileWatcher.onFileChange', () => {\n  let watcher: FileWatcher;\n  let mockAnalyzer: any;\n\n  beforeEach(() => {\n    mockAnalyzer = {\n      analyzeFile: jest.fn().mockResolvedValue({ issues: [] })\n    };\n    watcher = new FileWatcher(mockAnalyzer);\n  });\n\n  test('triggers analysis on file change', async () => {\n    const uri = { fsPath: '/workspace/test.ts' } as vscode.Uri;\n    \n    await watcher.onFileChange(uri);\n    \n    expect(mockAnalyzer.analyzeFile).toHaveBeenCalledWith('/workspace/test.ts');\n  });\n\n  test('debounces rapid file changes', async () => {\n    jest.useFakeTimers();\n    const uri = { fsPath: '/workspace/test.ts' } as vscode.Uri;\n    \n    watcher.onFileChange(uri);\n    watcher.onFileChange(uri);\n    watcher.onFileChange(uri);\n    \n    jest.runAllTimers();\n    \n    expect(mockAnalyzer.analyzeFile).toHaveBeenCalledTimes(1);\n    jest.useRealTimers();\n  });\n\n  test('skips ignored file patterns', async () => {\n    const uri = { fsPath: '/workspace/node_modules/lib.js' } as vscode.Uri;\n    \n    await watcher.onFileChange(uri);\n    \n    expect(mockAnalyzer.analyzeFile).not.toHaveBeenCalled();\n  });\n});",
            "run_instructions": "npm test -- fileWatcher.test.ts -t 'triggers analysis on file change'"
          }
        ]
      },
      {
        "id": "diagnosticsprovider-tests",
        "name": "Diagnostics Provider Tests",
        "description": "Tests for VS Code diagnostics integration showing inline warnings",
        "test_file_path": "src/test/diagnosticsProvider.test.ts",
        "source_files": [
          "src/diagnosticsProvider.ts"
        ],
        "test_cases": [
          {
            "id": "test-update-diagnostics",
            "name": "test_updateDiagnostics_creates_inline_markers",
            "description": "Verifies that diagnostics are created and displayed inline in editor",
            "target_function": "updateDiagnostics",
            "target_file": "src/diagnosticsProvider.ts",
            "scenarios": [
              "Create diagnostics for issues",
              "Update existing diagnostics",
              "Clear diagnostics"
            ],
            "mocks": [
              "VS Code diagnostic collection"
            ],
            "assertions": [
              "Diagnostics are created",
              "Correct severity set",
              "Ranges are accurate"
            ],
            "priority": "high",
            "test_code": "import { DiagnosticsProvider } from '../diagnosticsProvider';\nimport * as vscode from 'vscode';\n\njest.mock('vscode');\n\ndescribe('DiagnosticsProvider.updateDiagnostics', () => {\n  let provider: DiagnosticsProvider;\n  let mockCollection: any;\n\n  beforeEach(() => {\n    mockCollection = {\n      set: jest.fn(),\n      clear: jest.fn()\n    };\n    (vscode.languages.createDiagnosticCollection as jest.Mock) = jest.fn(() => mockCollection);\n    provider = new DiagnosticsProvider();\n  });\n\n  test('creates diagnostics for analysis issues', () => {\n    const uri = vscode.Uri.file('/test.ts');\n    const issues = [\n      { severity: 'error', description: 'Test error', file: '/test.ts', line: 10 }\n    ];\n    \n    provider.updateDiagnostics(uri, issues);\n    \n    expect(mockCollection.set).toHaveBeenCalled();\n    const diagnostics = mockCollection.set.mock.calls[0][1];\n    expect(diagnostics.length).toBe(1);\n  });\n\n  test('sets correct severity for diagnostics', () => {\n    const uri = vscode.Uri.file('/test.ts');\n    const issues = [\n      { severity: 'error', description: 'Error', file: '/test.ts', line: 5 },\n      { severity: 'warning', description: 'Warning', file: '/test.ts', line: 10 }\n    ];\n    \n    provider.updateDiagnostics(uri, issues);\n    \n    const diagnostics = mockCollection.set.mock.calls[0][1];\n    expect(diagnostics[0].severity).toBe(vscode.DiagnosticSeverity.Error);\n    expect(diagnostics[1].severity).toBe(vscode.DiagnosticSeverity.Warning);\n  });\n\n  test('clears diagnostics for file', () => {\n    const uri = vscode.Uri.file('/test.ts');\n    \n    provider.clearDiagnostics(uri);\n    \n    expect(mockCollection.set).toHaveBeenCalledWith(uri, []);\n  });\n});",
            "run_instructions": "npm test -- diagnosticsProvider.test.ts -t 'creates diagnostics for analysis issues'"
          }
        ]
      },
      {
        "id": "filedocumentation-tests",
        "name": "File Documentation Tests",
        "description": "Tests for generating file-level documentation and role detection",
        "test_file_path": "src/test/fileDocumentation.test.ts",
        "source_files": [
          "src/fileDocumentation.ts"
        ],
        "test_cases": [
          {
            "id": "test-detect-file-role",
            "name": "test_detectFileRole_identifies_purpose",
            "description": "Verifies that file roles are correctly identified from content and naming",
            "target_function": "detectFileRole",
            "target_file": "src/fileDocumentation.ts",
            "scenarios": [
              "Detect controller file",
              "Detect model file",
              "Detect utility file"
            ],
            "mocks": [
              "File system to read contents"
            ],
            "assertions": [
              "Role is correctly identified",
              "Confidence score provided",
              "Multiple roles handled"
            ],
            "priority": "medium",
            "test_code": "import { detectFileRole } from '../fileDocumentation';\nimport * as fs from 'fs';\n\njest.mock('fs');\n\ndescribe('detectFileRole', () => {\n  beforeEach(() => {\n    jest.clearAllMocks();\n  });\n\n  test('identifies controller file', () => {\n    const fileContent = 'export class UserController { handleRequest() {} }';\n    (fs.readFileSync as jest.Mock).mockReturnValue(fileContent);\n    \n    const role = detectFileRole('/controllers/UserController.ts');\n    \n    expect(role).toContain('controller');\n  });\n\n  test('identifies model file', () => {\n    const fileContent = 'export class User { id: string; name: string; }';\n    (fs.readFileSync as jest.Mock).mockReturnValue(fileContent);\n    \n    const role = detectFileRole('/models/User.ts');\n    \n    expect(role).toContain('model');\n  });\n\n  test('identifies utility file', () => {\n    const fileContent = 'export function formatDate(date: Date) { return date.toISOString(); }';\n    (fs.readFileSync as jest.Mock).mockReturnValue(fileContent);\n    \n    const role = detectFileRole('/utils/dateUtils.ts');\n    \n    expect(role).toContain('utility');\n  });\n});",
            "run_instructions": "npm test -- fileDocumentation.test.ts -t 'identifies controller file'"
          },
          {
            "id": "test-group-files-by-module",
            "name": "test_groupFilesByModule_organizes_structure",
            "description": "Verifies that files are grouped into logical modules",
            "target_function": "groupFilesByModule",
            "target_file": "src/fileDocumentation.ts",
            "scenarios": [
              "Group by directory",
              "Group by functionality",
              "Handle flat structure"
            ],
            "mocks": [
              "File list"
            ],
            "assertions": [
              "Modules are created",
              "Files grouped correctly",
              "Module names meaningful"
            ],
            "priority": "medium",
            "test_code": "import { groupFilesByModule } from '../fileDocumentation';\n\ndescribe('groupFilesByModule', () => {\n  test('groups files by directory structure', () => {\n    const files = [\n      '/src/controllers/UserController.ts',\n      '/src/controllers/ProductController.ts',\n      '/src/models/User.ts',\n      '/src/models/Product.ts'\n    ];\n    \n    const modules = groupFilesByModule(files);\n    \n    expect(modules.controllers).toBeDefined();\n    expect(modules.controllers.length).toBe(2);\n    expect(modules.models).toBeDefined();\n    expect(modules.models.length).toBe(2);\n  });\n\n  test('handles flat file structure', () => {\n    const files = [\n      '/src/app.ts',\n      '/src/config.ts',\n      '/src/main.ts'\n    ];\n    \n    const modules = groupFilesByModule(files);\n    \n    expect(Object.keys(modules).length).toBeGreaterThan(0);\n  });\n});",
            "run_instructions": "npm test -- fileDocumentation.test.ts -t 'groups files by directory structure'"
          }
        ]
      }
    ],
    "read_write_test_suites": [],
    "user_workflow_test_suites": []
  }
}